{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåç Simple Disaster COG Processing\n",
    "\n",
    "This simplified notebook converts disaster satellite imagery to Cloud Optimized GeoTIFFs (COGs) with just a few cells.\n",
    "\n",
    "## ‚ú® Features\n",
    "- **See files first** - List S3 files before configuring\n",
    "- **Smart configuration** - Define filename functions after seeing actual files\n",
    "- **Auto-discovery** - Automatically categorizes your files\n",
    "- **Simple processing** - Just run the cells in order\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a href=\"https://hub.disasters.2i2c.cloud/hub/user-redirect/git-pull?repo=https://github.com/Disasters-Learning-Portal/disasters-docs&urlpath=lab/tree/disasters-docs/Jupyterhub/simple_disaster_template.ipynb&branch=main\">üöÄ Launch in Disasters-Hub JupyterHub (requires access)</a></h3>\n",
    "\n",
    "<h4> To obtain credentials to VEDA Hub, <a href = \"https://docs.openveda.cloud/user-guide/scientific-computing/getting-access.html\"> follow this link for more information.</a></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" style=\"\n",
    "     background-color: #f8d7da;\n",
    "     color: #721c24;\n",
    "     border-left: 4px solid #28a745;\n",
    "  \">\n",
    "Disclaimer: it is highly recommended to run a tutorial within NASA VEDA JupyterHub, which already includes functions for processing and visualizing data specific to VEDA stories. Running the tutorial outside of the VEDA JupyterHub may lead to errors, specifically related to EarthData authentication. Additionally, it is recommended to use the Pangeo workspace within the VEDA JupyterHub, since certain packages relevant to this tutorial are already installed. </div>\n",
    "\n",
    "<h4> If you <strong>do not</strong> have a VEDA Jupyterhub Account you can launch this notebook on your local environment using MyBinder by clicking the icon below.</h4>\n",
    "<br/>\n",
    "<a href=\"https://binder.openveda.cloud/v2/gh/NASA-IMPACT/veda-docs/9c8cdbae92906fb7062b8a0c759dad90e223a4f9?urlpath=lab%2Ftree%2Fuser-guide%2Fnotebooks%2Fstories%2Fderechos.ipynb\">\n",
    "<img src=\"https://binder.openveda.cloud/badge_logo.svg\" alt=\"Binder\" title=\"A cute binder\" width=\"150\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 0: Setup - Clone Required Repository\n",
    "\n",
    "**Run this cell first!** This notebook requires the `disasters-aws-conversion` repository for processing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Check if disasters-aws-conversion exists, if not clone it\n",
    "repo_name = \"disasters-aws-conversion\"\n",
    "repo_url = \"https://github.com/Disasters-Learning-Portal/disasters-aws-conversion.git\"\n",
    "\n",
    "if not os.path.exists(f\"../{repo_name}\"):\n",
    "    print(f\"üì• Cloning {repo_name} repository...\")\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"clone\", repo_url, f\"../{repo_name}\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        print(f\"‚úÖ Successfully cloned {repo_name}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error cloning repository: {e.stderr}\")\n",
    "else:\n",
    "    print(f\"‚úÖ {repo_name} repository already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 1: Basic Configuration\n",
    "\n",
    "Set your event details and S3 paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: drcs_activations\n",
      "Source: s3://nasa-disasters/drcs_activations/202510_Flood_AK/aria\n",
      "Destination: s3://nasa-disasters/drcs_activations_new/\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# INPUTS\n",
    "# ========================================\n",
    "\n",
    "# S3 Paths\n",
    "BUCKET = 'nasa-disasters'    # S3 bucket (DO NOT CHANGE)\n",
    "DESTINATION_BASE = 'drcs_activations_new'  # Where to save COGs in S3 bucket (DO NOT CHANGE)\n",
    "GEOTIFF_DIR = 'drcs_activations' # This is where all non-converted files should be placed\n",
    "\n",
    "\n",
    "# Event Details\n",
    "EVENT_NAME = '202510_Flood_AK'  # Your sensor or product name (e.g, Sentinel-1, Planet, Landsat)\n",
    "SUB_PRODUCT_NAME = 'aria'         # Sub-directories within PRODUCT_NAME (RGB, trueColor, SWIR, etc.). Can leave blank and it will read from PRODUCT_NAME.\n",
    "SOURCE_PATH = f'{GEOTIFF_DIR}/{EVENT_NAME}/{SUB_PRODUCT_NAME}'      # Where your files are\n",
    "\n",
    "\n",
    "# Processing Options\n",
    "OVERWRITE = False      # Set to True to replace existing files\n",
    "VERIFY = True          # Set to True to verify results after processing\n",
    "SAVE_RESULTS = True    # Set to False to skip saving results CSV to /output directory\n",
    "\n",
    "print(f\"Event: {GEOTIFF_DIR}\")\n",
    "print(f\"Source: s3://{BUCKET}/{SOURCE_PATH}\")\n",
    "print(f\"Destination: s3://{BUCKET}/{DESTINATION_BASE}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 2: Connect to S3 and List Files\n",
    "\n",
    "Let's see what files are available before configuring filename transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Connecting to S3...\n",
      "‚úÖ Connected to S3\n",
      "\n",
      "üìÇ Files in s3://nasa-disasters/drcs_activations/202510_Flood_AK/aria:\n",
      "============================================================\n",
      "Found 6 .tif files:\n",
      "\n",
      " 1. OPERA_L3_DSWX-S1_V1_WTR_2025-10-08_mosaic.tif                (0.01 GB)\n",
      " 2. OPERA_L3_DSWX-S1_V1_WTR_2025-10-10_mosaic.tif                (0.03 GB)\n",
      " 3. OPERA_L3_DSWX-S1_V1_WTR_2025-10-12_mosaic.tif                (0.03 GB)\n",
      " 4. OPERA_L3_DSWX-S1_V1_WTR_2025-10-15_mosaic.tif                (0.02 GB)\n",
      " 5. OPERA_L3_DSWX-HLS_V1_WTR_2025-10-08_mosaic.tif               (0.01 GB)\n",
      " 6. OPERA_L3_DSWX-HLS_V1_WTR_2025-10-13_mosaic.tif               (0.01 GB)\n",
      "\n",
      "============================================================\n",
      "\n",
      "üí° Use this information to create filename functions in Step 3\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for importing functions\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "\n",
    "# Import S3 operations\n",
    "from core.s3_operations import (\n",
    "    initialize_s3_client,\n",
    "    list_s3_files,\n",
    "    get_file_size_from_s3\n",
    ")\n",
    "\n",
    "# Initialize S3 client\n",
    "print(\"üåê Connecting to S3...\")\n",
    "s3_client, _ = initialize_s3_client(bucket_name=BUCKET, verbose=False)\n",
    "\n",
    "if s3_client:\n",
    "    print(\"‚úÖ Connected to S3\\n\")\n",
    "    \n",
    "    # List all TIF files\n",
    "    print(f\"üìÇ Files in s3://{BUCKET}/{SOURCE_PATH}:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    files = list_s3_files(s3_client, BUCKET, SOURCE_PATH, suffix='.tif')\n",
    "    \n",
    "    if files:\n",
    "        print(f\"Found {len(files)} .tif files:\\n\")\n",
    "        for i, file_path in enumerate(files[:10], 1):  # Show first 10\n",
    "            filename = os.path.basename(file_path)\n",
    "            try:\n",
    "                size_gb = get_file_size_from_s3(s3_client, BUCKET, file_path)\n",
    "                print(f\"{i:2}. {filename:<60} ({size_gb:.2f} GB)\")\n",
    "            except:\n",
    "                print(f\"{i:2}. {filename}\")\n",
    "        \n",
    "        if len(files) > 10:\n",
    "            print(f\"\\n... and {len(files) - 10} more files\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"\\nüí° Use this information to create filename functions in Step 3\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No .tif files found in the specified path.\")\n",
    "        print(\"   Check your SOURCE_PATH configuration.\")\n",
    "else:\n",
    "    print(\"‚ùå Could not connect to S3. Check your AWS credentials.\")\n",
    "    files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 3a: Define Categorization and Filename Transformations\n",
    "\n",
    "Based on the files you see above, configure:\n",
    "1. **Categorization patterns** - Regex patterns to identify file types\n",
    "2. **Filename functions** - How to transform filenames\n",
    "3. **Output directories** - Where each category should be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CATEGORIZATION AND OUTPUT CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "import re\n",
    "\n",
    "# STEP 1: Define how to extract dates from filenames\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract date from filename in YYYYMMDD format.\"\"\"\n",
    "    dates = re.findall(r'\\d{8}', filename)\n",
    "    if dates:\n",
    "        date_str = dates[0]\n",
    "        return f\"{date_str[0:4]}-{date_str[4:6]}-{date_str[6:8]}\"\n",
    "    return None\n",
    "\n",
    "# STEP 2: Define filename transformation functions for each category\n",
    "def create_truecolor_filename(original_path, event_name):\n",
    "    \"\"\"Create filename for trueColor products.\"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    return f\"{event_name}_{stem}_day.tif\"\n",
    "\n",
    "def create_colorinfrared_filename(original_path, event_name):\n",
    "    \"\"\"Create filename for colorInfrared products.\"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    return f\"{event_name}_{stem}_day.tif\"\n",
    "\n",
    "def create_naturalcolor_filename(original_path, event_name):\n",
    "    \"\"\"Create filename for naturalColor products.\"\"\"\n",
    "    filename = os.path.basename(original_path)\n",
    "    stem = os.path.splitext(filename)[0]\n",
    "    date = extract_date_from_filename(stem)\n",
    "    \n",
    "    if date:\n",
    "        stem_clean = re.sub(r'_\\d{8}', '', stem)\n",
    "        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n",
    "    return f\"{event_name}_{stem}_day.tif\"\n",
    "\n",
    "# STEP 3: Configure categorization patterns (REQUIRED)\n",
    "# These regex patterns determine which files belong to which category\n",
    "CATEGORIZATION_PATTERNS = {\n",
    "    'trueColor': r'trueColor|truecolor|true_color',\n",
    "    'colorInfrared': r'colorInfrared|colorIR|color_infrared',\n",
    "    'naturalColor': r'naturalColor|natural_color',\n",
    "    # Add patterns for ALL file types you want to process\n",
    "    # Files not matching any pattern will be skipped with a warning\n",
    "}\n",
    "\n",
    "# STEP 4: Map categories to filename transformation functions\n",
    "FILENAME_CREATORS = {\n",
    "    'trueColor': create_truecolor_filename,\n",
    "    'colorInfrared': create_colorinfrared_filename,\n",
    "    'naturalColor': create_naturalcolor_filename,\n",
    "    # Must have an entry for each category in CATEGORIZATION_PATTERNS\n",
    "}\n",
    "\n",
    "# STEP 5: Specify output directories for each category\n",
    "OUTPUT_DIRS = {\n",
    "    'trueColor': 'Landsat/trueColor',\n",
    "    'colorInfrared': 'Landsat/colorIR',\n",
    "    'naturalColor': 'Landsat/naturalColor',\n",
    "    # Must have an entry for each category in CATEGORIZATION_PATTERNS\n",
    "}\n",
    "\n",
    "# OPTIONAL: Specify no-data values (None = auto-detect)\n",
    "NODATA_VALUES = {\n",
    "    'trueColor': 0,\n",
    "    'colorInfrared': 0,\n",
    "    'naturalColor': 0\n",
    "    # Leave empty or set to None for auto-detection\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 3b: Test the new functions to verify what the inputs and outputs will be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration defined\n",
      "\n",
      "üìÇ Categories and output paths:\n",
      "   ‚Ä¢ trueColor:\n",
      "     Pattern: trueColor|truecolor|true_color\n",
      "     Output:  drcs_activations_new/Landsat/trueColor\n",
      "   ‚Ä¢ colorInfrared:\n",
      "     Pattern: colorInfrared|colorIR|color_infrared\n",
      "     Output:  drcs_activations_new/Landsat/colorIR\n",
      "   ‚Ä¢ naturalColor:\n",
      "     Pattern: naturalColor|natural_color\n",
      "     Output:  drcs_activations_new/Landsat/naturalColor\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Configuration defined\")\n",
    "print(f\"\\nüìÇ Categories and output paths:\")\n",
    "for category, path in OUTPUT_DIRS.items():\n",
    "    pattern = CATEGORIZATION_PATTERNS.get(category, 'No pattern defined')\n",
    "    print(f\"   ‚Ä¢ {category}:\")\n",
    "    print(f\"     Pattern: {pattern}\")\n",
    "    print(f\"     Output:  {DESTINATION_BASE}/{path}\")\n",
    "\n",
    "# Test with sample filename if files exist\n",
    "if files:\n",
    "    sample_file = files[0]\n",
    "    sample_name = os.path.basename(sample_file)\n",
    "    \n",
    "    # Check which category it would match\n",
    "    matched_category = None\n",
    "    for cat, pattern in CATEGORIZATION_PATTERNS.items():\n",
    "        if re.search(pattern, sample_name, re.IGNORECASE):\n",
    "            matched_category = cat\n",
    "            break\n",
    "    \n",
    "    if matched_category:\n",
    "        new_name = FILENAME_CREATORS[matched_category](sample_file, EVENT_NAME)\n",
    "        print(f\"\\nüìù Example transformation:\")\n",
    "        print(f\"   Original: {sample_name}\")\n",
    "        print(f\"   Category: {matched_category}\")\n",
    "        print(f\"   ‚Üí New:    {new_name}\")\n",
    "        print(f\"   ‚Üí Output: {DESTINATION_BASE}/{OUTPUT_DIRS[matched_category]}/{new_name}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: Sample file doesn't match any category pattern:\")\n",
    "        print(f\"   File: {sample_name}\")\n",
    "        print(f\"   Add a pattern to CATEGORIZATION_PATTERNS to process this file type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 4: Initialize Processor and Preview\n",
    "\n",
    "Now let's set up the processor and preview all transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All modules loaded successfully\n",
      "\n",
      "üåê Connecting to S3...\n",
      "‚úÖ Connected to S3 successfully\n",
      "‚úÖ Processor ready\n",
      "\n",
      "\n",
      "üîç Searching for files in: geotiffs_to_convert/202408_TropicalStorm_Debby/landsat8\n",
      "‚ö†Ô∏è No .tif files found\n",
      "‚ö†Ô∏è No files found to process.\n"
     ]
    }
   ],
   "source": [
    "# Import our simplified helper\n",
    "from notebooks.notebook_helpers import SimpleProcessor\n",
    "\n",
    "# Create full configuration with categorization patterns\n",
    "config = {\n",
    "    'event_name': EVENT_NAME,\n",
    "    'bucket': BUCKET,\n",
    "    'source_path': SOURCE_PATH,\n",
    "    'destination_base': DESTINATION_BASE,\n",
    "    'overwrite': OVERWRITE,\n",
    "    'verify': VERIFY,\n",
    "    'save_results': SAVE_RESULTS,  # Add save results flag\n",
    "    'categorization_patterns': CATEGORIZATION_PATTERNS,  # IMPORTANT: Include patterns\n",
    "    'filename_creators': FILENAME_CREATORS,\n",
    "    'output_dirs': OUTPUT_DIRS,\n",
    "    'nodata_values': NODATA_VALUES\n",
    "}\n",
    "\n",
    "# Initialize processor\n",
    "processor = SimpleProcessor(config)\n",
    "\n",
    "# Connect to S3 (already connected, but needed for processor)\n",
    "if processor.connect_to_s3():\n",
    "    print(\"‚úÖ Processor ready\\n\")\n",
    "    \n",
    "    # Discover and categorize files\n",
    "    num_files = processor.discover_files()\n",
    "    \n",
    "    if num_files > 0:\n",
    "        # Show preview of transformations\n",
    "        processor.preview_processing()\n",
    "        \n",
    "        print(\"\\nüìå Review the transformations above.\")\n",
    "        print(\"   ‚Ä¢ Files will be saved to the directories specified in OUTPUT_DIRS\")\n",
    "        print(\"   ‚Ä¢ If files appear as 'uncategorized', add patterns to CATEGORIZATION_PATTERNS\")\n",
    "        print(\"   ‚Ä¢ When ready, proceed to Step 5 to process the files.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No files found to process.\")\n",
    "else:\n",
    "    print(\"‚ùå Could not initialize processor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 5: Process Files\n",
    "\n",
    "Run this cell to start processing all files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting processing...\n",
      "This may take several minutes depending on file sizes.\n",
      "\n",
      "\n",
      "üöÄ Starting processing...\n",
      "\n",
      "üì¶ Processing colorInfrared (3 files)\n",
      "  ‚è≠Ô∏è Skipped: LC08_colorInfrared_20240715_155319_016036.tif (exists)\n",
      "  ‚è≠Ô∏è Skipped: LC08_colorInfrared_20240715_155343_016037.tif (exists)\n",
      "  ‚è≠Ô∏è Skipped: LC08_colorInfrared_20240715_15547_016038.tif (exists)\n",
      "\n",
      "üì¶ Processing naturalColor (3 files)\n",
      "  ‚è≠Ô∏è Skipped: LC08_naturalColor_20240715_155319_016036.tif (exists)\n",
      "  ‚è≠Ô∏è Skipped: LC08_naturalColor_20240715_155343_016037.tif (exists)\n",
      "  ‚è≠Ô∏è Skipped: LC08_naturalColor_20240715_15547_016038.tif (exists)\n",
      "\n",
      "üì¶ Processing trueColor (3 files)\n",
      "  ‚è≠Ô∏è Skipped: LC08_trueColor_20240715_155319_016036.tif (exists)\n",
      "  ‚è≠Ô∏è Skipped: LC08_trueColor_20240715_155343_016037.tif (exists)\n",
      "  ‚è≠Ô∏è Skipped: LC08_trueColor_20240715_15547_016038.tif (exists)\n",
      "\n",
      "============================================================\n",
      "‚úÖ PROCESSING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Results:\n",
      "  ‚è≠Ô∏è Skipped: 9\n",
      "\n",
      "Processing time: 0.0 minutes\n",
      "\n",
      "üìÅ Results saved to: output/202408_TropicalStorm_Debby/results_20250929_191143.csv\n",
      "============================================================\n",
      "\n",
      "üìä Processing Complete!\n",
      "                                     source_file       category   status  \\\n",
      "0  LC08_colorInfrared_20240715_155319_016036.tif  colorInfrared  skipped   \n",
      "1  LC08_colorInfrared_20240715_155343_016037.tif  colorInfrared  skipped   \n",
      "2   LC08_colorInfrared_20240715_15547_016038.tif  colorInfrared  skipped   \n",
      "3   LC08_naturalColor_20240715_155319_016036.tif   naturalColor  skipped   \n",
      "4   LC08_naturalColor_20240715_155343_016037.tif   naturalColor  skipped   \n",
      "5    LC08_naturalColor_20240715_15547_016038.tif   naturalColor  skipped   \n",
      "6      LC08_trueColor_20240715_155319_016036.tif      trueColor  skipped   \n",
      "7      LC08_trueColor_20240715_155343_016037.tif      trueColor  skipped   \n",
      "8       LC08_trueColor_20240715_15547_016038.tif      trueColor  skipped   \n",
      "\n",
      "           reason                                        output_path  \\\n",
      "0  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n",
      "1  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n",
      "2  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n",
      "3  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n",
      "4  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n",
      "5  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n",
      "6  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n",
      "7  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n",
      "8  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n",
      "\n",
      "   time_seconds  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "5             0  \n",
      "6             0  \n",
      "7             0  \n",
      "8             0  \n"
     ]
    }
   ],
   "source": [
    "# Process all files\n",
    "if 'num_files' in locals() and num_files > 0:\n",
    "    print(\"üöÄ Starting processing...\")\n",
    "    print(\"This may take several minutes depending on file sizes.\\n\")\n",
    "    \n",
    "    # Process everything\n",
    "    results = processor.process_all()\n",
    "    \n",
    "    # Display results\n",
    "    if not results.empty:\n",
    "        print(\"\\nüìä Processing Complete!\")\n",
    "        display(results) if 'display' in dir() else print(results)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No files to process. Complete Steps 1-4 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "if 'results' in locals() and not results.empty:\n",
    "    print(\"üìä PROCESSING STATISTICS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Success rate\n",
    "    total = len(results)\n",
    "    success = len(results[results['status'] == 'success'])\n",
    "    failed = len(results[results['status'] == 'failed'])\n",
    "    skipped = len(results[results['status'] == 'skipped'])\n",
    "    \n",
    "    print(f\"Total files: {total}\")\n",
    "    print(f\"‚úÖ Success: {success}\")\n",
    "    print(f\"‚ùå Failed: {failed}\")\n",
    "    print(f\"‚è≠Ô∏è Skipped: {skipped}\")\n",
    "    print(f\"\\nSuccess rate: {(success/total*100):.1f}%\")\n",
    "    \n",
    "    # Failed files\n",
    "    if failed > 0:\n",
    "        print(\"\\n‚ùå Failed files:\")\n",
    "        failed_df = results[results['status'] == 'failed']\n",
    "        for idx, row in failed_df.iterrows():\n",
    "            print(f\"  - {row['source_file']}: {row.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Processing times\n",
    "    if 'time_seconds' in results.columns:\n",
    "        success_df = results[results['status'] == 'success']\n",
    "        if not success_df.empty:\n",
    "            avg_time = success_df['time_seconds'].mean()\n",
    "            max_time = success_df['time_seconds'].max()\n",
    "            print(f\"\\n‚è±Ô∏è Timing:\")\n",
    "            print(f\"Average: {avg_time:.1f} seconds per file\")\n",
    "            print(f\"Slowest: {max_time:.1f} seconds\")\n",
    "else:\n",
    "    print(\"No results to analyze. Run Step 5 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Tips & Troubleshooting\n",
    "\n",
    "### Workflow Summary:\n",
    "1. **Setup** - Clone disasters-aws-conversion repository (Step 0)\n",
    "2. **Configure** basic settings (Step 1)\n",
    "3. **List files** from S3 to see naming patterns (Step 2)\n",
    "4. **Define functions** to transform filenames (Step 3)\n",
    "5. **Preview** transformations (Step 4)\n",
    "6. **Process** all files (Step 5)\n",
    "7. **Review** results (Step 6)\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **\"ModuleNotFoundError: No module named 'core'\" or import errors**\n",
    "   - Run Step 0 first to clone the disasters-aws-conversion repository\n",
    "   - Restart kernel and run all cells from the beginning\n",
    "\n",
    "2. **\"No files found\"**\n",
    "   - Check `SOURCE_PATH` in Step 1\n",
    "   - Verify bucket permissions\n",
    "   - Ensure files have `.tif` extension\n",
    "\n",
    "3. **Wrong filename transformations**\n",
    "   - Review actual filenames in Step 2\n",
    "   - Adjust functions in Step 3\n",
    "   - Re-run Step 4 to preview\n",
    "\n",
    "4. **Files being skipped**\n",
    "   - Files already exist in destination\n",
    "   - Set `OVERWRITE = True` in Step 1\n",
    "\n",
    "5. **Processing errors**\n",
    "   - Check AWS credentials\n",
    "   - Verify S3 write permissions\n",
    "   - Check available disk space for temp files\n",
    "\n",
    "### Need More Control?\n",
    "\n",
    "Use the full template at `disaster_processing_template.ipynb` for:\n",
    "- Manual chunk configuration\n",
    "- Custom compression settings\n",
    "- Detailed memory management\n",
    "- Advanced processing options"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

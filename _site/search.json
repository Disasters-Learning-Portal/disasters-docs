[
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "",
    "text": "Welcome to the homepage for U.S. Greenhouse Gas (GHG) Center data flow diagrams. These diagrams summarize the process a dataset goes through from acquisition to integration in the U.S. GHG Center.\nClick on a dataset name to view the data flow diagram for that dataset.\nView the US GHG Center Data Catalog",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "workflow.html#contact",
    "href": "workflow.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Flow Diagrams",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Flow Diagrams"
    ]
  },
  {
    "objectID": "utility.html",
    "href": "utility.html",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks Utility Functions",
    "section": "",
    "text": "Welcome to the U.S. Greenhouse Gas (GHG) Center data usage notebooks utility functions, your gateway to exploring and analyzing curated datasets on greenhouse gas emissions. Our cloud-based system offers seamless access to GHG curated datasets. Dive into the data with our utility functions, which demonstrate how to explore, access, visualize, and conduct basic data analysis for each GHG Center dataset in a code notebook environment.\nJoin us in our mission to make data-driven environmental solutions. Explore, analyze, and make a difference with the US GHG Center.\nView the US GHG Center Data Catalog"
  },
  {
    "objectID": "utility.html#utilities",
    "href": "utility.html#utilities",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks Utility Functions",
    "section": "Utilities",
    "text": "Utilities\nSection contains multiple utility functions\n\nimport requests\nimport pandas as pd\nimport datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import rgb2hex\nimport numpy as np\nimport sys\n\nRASTER_API_URL = \"https://earth.gov/ghgcenter/api/raster\"\n\n# Functions mentioned below are defined in stats_module.py\n\ndef generate_stats(item, geojson, asset_name):\n    \"\"\"\n    Retrieve statistics for a specific granule (item) within a GeoJSON-defined polygon.\n\n    Args:\n        item (dict): The granule containing item details (including assets and metadata).\n        geojson (dict): A GeoJSON Feature or FeatureCollection specifying the bounding box.\n        asset_name (str): The asset name or raster identifier to be used.\n\n    Returns:\n        dict: A dictionary with computed statistics and the item's datetime information.\n    \"\"\"\n    result = requests.post(\n        f\"{RASTER_API_URL}/cog/statistics\",\n        params={\"url\": item[\"assets\"][asset_name][\"href\"]},\n        json=geojson,\n    ).json()\n\n    print(result)\n\n    # Handle cases where either \"start_datetime\" or \"datetime\" is present\n    datetime_value = item[\"properties\"].get(\"start_datetime\", item[\"properties\"].get(\"datetime\"))\n\n    return {\n        **result[\"properties\"],\n        \"datetime\": datetime_value,\n    }\n\n\n\ndef clean_stats(stats_json):\n    \"\"\"\n    Clean and normalize the statistics JSON data and convert it into a pandas DataFrame.\n\n    Args:\n        stats_json (list of dict): List of statistics dictionaries for each granule.\n\n    Returns:\n        pd.DataFrame: A DataFrame with flattened and cleaned statistics.\n    \"\"\"\n    df = pd.json_normalize(stats_json)\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n    df[\"date\"] = pd.to_datetime(df[\"datetime\"])\n    return df\n\n\ndef display_stats(df, num_rows=5):\n    \"\"\"\n    Display the top rows of the cleaned statistics DataFrame.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the cleaned statistics.\n        num_rows (int): Number of rows to display (default is 5).\n    \"\"\"\n    print(df.head(num_rows))\n\n# Functions mentioned below are defined in ghgc_utlis.py\n\ndef raster_stats(item, geojson,**kwargs):\n    \"\"\"\n    Returns Raster API statistics for an item. Inputs: item, geojson, url = Raster API url, asset = asset name within item. Outputs: dictionary containing statistics over the bounding box and item's datetime information.\n    \"\"\"\n\n    try:\n        url = item[\"assets\"][kwargs[\"asset\"]][\"href\"]\n    except TypeError as err:\n        url = item.assets[kwargs[\"asset\"]].href\n    except KeyError as err:\n        print('KeyError in raster_stats: Make sure you include \\'url\\' and \\'asset\\' as keyword arguments!')\n        sys.exit()      \n    \n    # A POST request is made to submit the data associated with the item of interest (specific observation) within the boundaries of the polygon to compute its statistics\n    result = requests.post(\n\n        # Raster API Endpoint for computing statistics\n        f\"{kwargs['url']}/cog/statistics\",\n\n        # Pass the URL to the item, asset name, and raster identifier as parameters\n        params={\"url\": url},\n\n        # Send the GeoJSON object (polygon) along with the request\n        json=geojson,\n\n    # Return the response in JSON format\n    ).json()\n\n\n    # Print the result\n    ##print(result)\n\n    # Return a dictionary containing the computed statistics along with the item's datetime information.\n    try:\n        return {\n            **result[\"properties\"],\n            \"datetime\": item[\"properties\"][\"start_datetime\"],\n        }\n    except KeyError as err:\n        try:\n            return {\n                **result[\"features\"][0][\"properties\"],\n                'datetime': item[\"properties\"][\"start_datetime\"],\n            }\n        except TypeError as err:\n            return {\n                **result[\"features\"][0][\"properties\"],\n                \"datetime\": item.properties[\"start_datetime\"]\n            }\n    except TypeError as err:\n        return {\n            **result[\"properties\"],\n            \"datetime\": item.properties[\"start_datetime\"]\n        }\n\ndef clean_stats(stats_json) -&gt; pd.DataFrame:\n    \"\"\"\n    Takes dictionary output from generate_stats() and returns a neater, more intuitively-titled pandas DataFrame.\n    \"\"\"\n    pd.set_option('display.float_format', '{:.20f}'.format)\n    stats_json_ = [stats_json[datetime] for datetime in stats_json] \n    # Normalize the JSON data \n    df = pd.json_normalize(stats_json_)\n\n    # Replace the naming \"statistics.b1\" in the columns\n    df.columns = [col.replace(\"statistics.b1.\", \"\") for col in df.columns]\n\n    # Set the datetime format\n    df[\"date\"] = pd.to_datetime(df[\"datetime\"])\n\n    # Return the cleaned format\n    return df\n\ndef generate_stats(items,geojson,**kwargs):\n    \"\"\"\n    Runs raster_stats() and clean-stats() on all items. Inputs: List containing multiple items; geojson; url = URL for Raster API, asset = asset name for item field. Outputs: Pandas DataFrame of cleaned statistics for all items in list.\n    \"\"\"\n    stats = {}\n    print('Generating stats...')\n    for item in items:\n        try:\n            date = item[\"properties\"][\"start_datetime\"]  # Get the associated date\n        except TypeError:\n            date = item.properties[\"start_datetime\"]\n        year_month = date[:7].replace('-', '')  # Convert datetime to year-month\n        stats[year_month] = raster_stats(item, geojson,**kwargs)\n    df = clean_stats(stats)\n    print('Done!')\n    return df\n\ndef generate_html_colorbar(color_map,rescale_values,label=None,dark=False):\n    \"\"\"\n    Creates html-formatted string which can be added to Folium maps to display a colorbar. Required inputs: colormap (matplotlib-accepted string), rescale_values in the form of a dictionary containing keys 'max' and 'min' which specify the desired colorbar range. Optional inputs: label, which will display above the colorbar. Output: html-formatted string detailing construction of the colorbar.\n    \"\"\"\n    # Pull out colors from our chosen colormap\n    cmap = plt.get_cmap(color_map)\n    colors = cmap(np.linspace(0,1,11))\n    colors = [rgb2hex(c) for c in colors]\n    # Define custom tick values for the legend bar\n    tick_val = np.round(np.linspace(rescale_values['min'],rescale_values['max'],5),decimals=6)\n    # Create a HTML representation\n    legend_html = cmap._repr_html_()\n\n    # Create a customized HTML structure for the legend\n#    legend_html = f'''\n#    &lt;div style=\"position: fixed; bottom: 50px; left: 175px; z-index: 1000; width: 400px; height: auto; #background-color: rgba(255, 255, 255, 0.8);\n#             border-radius: 5px; border: 1px solid grey; padding: 10px; font-size: 12px; color: black;\"&gt;\n#        &lt;b&gt;{label}&lt;/b&gt;&lt;br&gt;\n#        &lt;div style=\"display: flex; justify-content: space-between;\"&gt;\n#            &lt;div&gt;{tick_val[0]}&lt;/div&gt; \n#            &lt;div&gt;{tick_val[1]}&lt;/div&gt; \n#            &lt;div&gt;{tick_val[2]}&lt;/div&gt; \n#            &lt;div&gt;{tick_val[3]}&lt;/div&gt; \n#            &lt;div&gt;{tick_val[4]}&lt;/div&gt; \n#        &lt;/div&gt;\n#        &lt;div style=\"background: linear-gradient(to right,\n#                {colors[0]}, {colors[1]} {20}%,\n#                {colors[1]} {20}%, {colors[2]} {40}%,\n#                {colors[2]} {40}%, {colors[3]} {50}%,\n#                {colors[3]} {50}%, {colors[4]} {80}%,\n#                {colors[4]} {80}%, {colors[5]}); height: 10px;\"&gt;&lt;/div&gt;\n#    &lt;/div&gt;\n#    '''\n    if dark:\n        bg_color = \"rgba(0, 0, 0, 0.8)\"\n        font_color=\"white\"\n    else:\n        bg_color = \"rgba(255, 255, 255, 0.8)\"\n        font_color=\"black\"\n    \n    legend_html = f'''\n    &lt;div style=\"position: fixed; bottom: 50px; left: 175px; z-index: 1000; width: 400px; height: auto; background-color: {bg_color};\n             border-radius: 5px; border: 1px solid grey; padding: 10px; font-size: 12px; color: {font_color};\"&gt;\n        &lt;b&gt;{label}&lt;/b&gt;&lt;br&gt;\n        &lt;div style=\"display: flex; justify-content: space-between;\"&gt;\n            &lt;div&gt;{tick_val[0]}&lt;/div&gt; \n            &lt;div&gt;{tick_val[1]}&lt;/div&gt; \n            &lt;div&gt;{tick_val[2]}&lt;/div&gt; \n            &lt;div&gt;{tick_val[3]}&lt;/div&gt; \n            &lt;div&gt;{tick_val[4]}&lt;/div&gt;\n        &lt;/div&gt;\n        &lt;div style=\"background: linear-gradient(to right,\n                {colors[0]}, {colors[1]} {10}%,\n                {colors[1]} {10}%, {colors[2]} {20}%,\n                {colors[2]} {20}%, {colors[3]} {30}%,\n                {colors[3]} {30}%, {colors[4]} {40}%,\n                {colors[4]} {40}%, {colors[5]} {50}%,\n                {colors[5]} {50}%, {colors[6]} {60}%,\n                {colors[6]} {60}%, {colors[7]} {70}%,\n                {colors[7]} {70}%, {colors[8]} {80}%,\n                {colors[8]} {80}%, {colors[9]} {90}%,\n                {colors[9]} {90}%, {colors[10]}); height: 10px;\"&gt;&lt;/div&gt;\n    &lt;/div&gt;\n    '''\n    return legend_html"
  },
  {
    "objectID": "utility.html#contact",
    "href": "utility.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks Utility Functions",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NASA Disasters: Documentation",
    "section": "",
    "text": "The NASA Disasters Program advances science and builds tools to help communities make informed decisions for disaster planning. We develop free and accessible resources that use Earth observations to reveal how natural hazards interact with vulnerability, exposure, and coping capacity in a changing climate.\nOn this site, you can find the technical documentation for the services used to visualize data, how to connect to these services, how to load datasets, how the datasets were transformed into cloud-optimized formats that enable efficient cloud data access, and how to visualize datasets.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "NASA Disasters: Documentation",
    "section": "",
    "text": "The NASA Disasters Program advances science and builds tools to help communities make informed decisions for disaster planning. We develop free and accessible resources that use Earth observations to reveal how natural hazards interact with vulnerability, exposure, and coping capacity in a changing climate.\nOn this site, you can find the technical documentation for the services used to visualize data, how to connect to these services, how to load datasets, how the datasets were transformed into cloud-optimized formats that enable efficient cloud data access, and how to visualize datasets.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "NASA Disasters: Documentation",
    "section": "Contents",
    "text": "Contents\n\nA guide to access AWS resources, with guidelines to utilize AWS commands and operations.\nDirections on how to setup GitHub, access a repository, use commands, and understand the GitHub workflow in Disasters.\nHow to use JupyterHub to work collaboratively on notebooks within a repositroy and the Disasters framework.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "NASA Disasters: Documentation",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "datausage.html",
    "href": "datausage.html",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "",
    "text": "Welcome to the homepage for the U.S. Greenhouse Gas (GHG) Center data usage notebooks. Each dataset available in the GHG Center Data Catalog has an associated “Introductory notebook” created by the GHG Center team which demonstrates how to access, visualize, and conduct basic data analysis in a Jupyter Notebook environment. Additional notebooks may be provided with each dataset. Click on a dataset under “GHG Center Dataset Tutorials” to learn more about the dataset and to view the associated code notebooks.\nAlong with GHG Center-curated notebooks, science users can also contribute notebooks that use GHG Center datasets. Notebooks submitted by the scientific community can be found under the “Community-Contributed Tutorials” section.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#ghg-center-dataset-tutorials",
    "href": "datausage.html#ghg-center-dataset-tutorials",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "GHG Center Dataset Tutorials",
    "text": "GHG Center Dataset Tutorials\nTutorial notebooks created by the GHG Center team. Introductory notebooks demonstrate how to access, visualize, and conduct basic data analysis for each dataset available in the GHG Center Data Catalog. Notebooks demonstrating how to conduct additional or more specialized analyses may also be listed under the relevant dataset.\n\nAir-Sea CO₂ Flux, ECCO-Darwin Model v5\n\nIntroductory notebook\n\nAtmospheric Carbon Dioxide Concentrations from the NOAA Global Monitoring Laboratory\n\nIntroductory notebook\n\nCarbon Dioxide and Methane Concentrations from the Indianapolis Flux Experiment (INFLUX)\n\nIntroductory notebook\n\nCarbon Dioxide and Methane Concentrations from the Los Angeles Megacity Carbon Project\n\nIntroductory notebook\n\nCarbon Dioxide and Methane Concentrations from the Northeast Corridor (NEC) Urban Test Bed\n\nIntroductory notebook\n\nCarbonTracker-CH₄ Isotopic Methane Inverse Fluxes\n\nIntroductory notebook\n\nEMIT Methane Point Source Plume Complexes\n\nIntroductory notebook\n\nGeostationary Satellite Observations of Extreme and Transient Methane Emissions from Oil and Gas Infrastructure\n\nIntroductory notebook\n\nGOSAT-based Top-down Total and Natural Methane Emissions\n\nIntroductory notebook\n\nGRA²PES Greenhouse Gas and Air Quality Species\n\nIntroductory notebook\n\nMiCASA Land Carbon Flux\n\nIntroductory notebook\n\nOCO-2 GEOS Column CO₂ Concentrations\n\nIntroductory notebook\n\nOCO-2 MIP Top-Down CO₂ Budgets\n\nIntroductory notebook\nIntermediate level notebook to read and visualize National CO₂ Budgets using OCO-2 MIP Top-Down CO₂ Budget country total data. This notebook utilizes the country totals available at https://ceos.org/gst/carbon-dioxide.html, which compliment the global 1° x 1° gridded CO₂ Budget data featured in the US GHG Center.\n\nODIAC Fossil Fuel CO₂ Emissions\n\nIntroductory notebook\n\nSEDAC Gridded World Population Density\n\nIntroductory notebook\n\nU.S. Gridded Anthropogenic Methane Emissions Inventory\n\nIntroductory notebook\n\nVulcan Fossil Fuel CO₂ Emissions\n\nIntroductory notebook\n\nWetland Methane Emissions, LPJ-EOSIM Model\n\nIntroductory notebook",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#community-contributed-tutorials",
    "href": "datausage.html#community-contributed-tutorials",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Community-Contributed Tutorials",
    "text": "Community-Contributed Tutorials\nNotebooks provided by the scientific community that use GHG Center datasets. The GHG Center is in the process of establishing a workflow for community notebook contribution. Once available, the contribution process will be published here. In the meantime, please submit any notebook contribution inquiries through the US GHG Center Contact Form.\nNote: these notebooks are contributed by the scientific community, and are not actively maintained by the GHG Center team.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "datausage.html#contact",
    "href": "datausage.html#contact",
    "title": "U.S. Greenhouse Gas Center: Data Usage Notebooks",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Data Usage Notebooks"
    ]
  },
  {
    "objectID": "AWS/AWS_SSO_Setup_Guide.html",
    "href": "AWS/AWS_SSO_Setup_Guide.html",
    "title": "🚀 AWS SSO Configuration Guide",
    "section": "",
    "text": "This guide walks you through setting up AWS Single Sign-On (SSO) to securely manage your AWS credentials without storing them in plain text.\n\n\n\n\n\n✅ AWS CLI v2 installed (version 2.x or higher)\n✅ Access to AWS Identity Center (formerly AWS SSO)\n✅ Your organization’s SSO portal URL\n\n\n\n\n\n\n\naws configure sso\n\n\n\nWhen prompted for SSO session name, enter a descriptive name for your profile:\nSSO session name (Recommended): disasters\n💡 Tip: Use meaningful names like prod-admin, dev-poweruser, etc.\n\n\n\nFind your SSO URL in the AWS Identity Center portal and enter it:\nSSO start URL [None]: https://d-9067c5bbc5.awsapps.com/start/#\n📍 Where to find: Navigate to your AWS SSO portal → Look for the URL in your browser\n\n\n\nEnter the region where your Identity Center is configured:\nSSO region [None]: us-east-1\n\n\n\nPress Enter to accept the default:\nSSO registration scopes [sso:account:access]: \n✨ The default scope is sufficient for most use cases\n\n\n\n🌐 A browser window will open automatically: 1. Log in with your corporate credentials 2. Click “Allow” to grant access to AWS CLI (botocore) 3. Return to your terminal\n\n\n\nEnter your AWS account ID (12 digits):\nAWS account ID: 867530900000\n📝 Find this in your AWS SSO portal under the accounts tab\n\n\n\nSelect from available roles:\nThere are 2 roles available to you.\n&gt; Project-Power-User\n  ReadOnlyAccess\nUse arrow keys to select, then press Enter\n\n\n\nConfirm or change the AWS region:\nCLI default region [us-east-1]: us-east-1\n\n\n\nChoose your preferred output format:\nCLI default output format [None]: json\nOptions: json, yaml, text, table\n\n\n\n\n\nTest your configuration:\naws s3 ls --profile disasters-sso\nExpected output:\n                           PRE browseui/\n                           PRE california_wildfires_202501/\n                           PRE disasters/\n                           ...\n\n\n\n\n\n\naws sso login --profile disasters-sso\n\n\n\n# List S3 buckets\naws s3 ls --profile disasters-sso\n\n# Get caller identity\naws sts get-caller-identity --profile disasters-sso\n\n\n\nTo avoid passing --profile disasters-sso with every command, you can set the AWS_PROFILE environment variable:\n# Set the environment variable\nexport AWS_PROFILE=disasters-sso\n\n# Now you can run commands without --profile\naws s3 ls\naws sts get-caller-identity\nThis is especially helpful when running many AWS commands in a session.\n\n\n\naws sso logout\n\n\n\n\n\nYour SSO configuration is stored in ~/.aws/config:\n[profile disasters-sso]\nsso_session = disasters\nsso_account_id = 867530900000\nsso_role_name = Project-Power-User\nregion = us-east-1\noutput = json\n\n[sso-session disasters]\nsso_start_url = https://d-9067c5bbc5.awsapps.com/start/#\nsso_region = us-east-1\nsso_registration_scopes = sso:account:access\n\n\n\n\n\n\n\nAWS SSO provides temporary credentials that expire after 1-12 hours\nCredentials are automatically refreshed when you run commands\nNo permanent credentials are stored on your machine\n\n\n\n\nAWS-Vault expects permanent credentials to generate temporary ones. Since AWS SSO already provides temporary credentials: - Adding SSO temporary credentials to aws-vault causes authentication errors - SSO handles credential refresh automatically, making aws-vault redundant - Use AWS SSO for temporary credential profiles, aws-vault for permanent ones\n\n\n\n\n\n\n\nCause: Trying to use expired temporary credentials Solution: Run aws sso login --profile your-profile to refresh\n\n\n\nSolution: Add --use-device-code flag:\naws sso login --profile disasters-sso --use-device-code\n\n\n\nCreate separate profiles for each account/role combination:\n[profile prod-admin]\nsso_session = mycompany\nsso_account_id = 111111111111\nsso_role_name = Administrator\n\n[profile dev-readonly]\nsso_session = mycompany\nsso_account_id = 222222222222\nsso_role_name = ReadOnlyAccess\n\n\n\n\n\n\nNever store credentials in plain text ❌\nUse SSO for all AWS access ✅\nLogout when finished working 🔒\nUse descriptive profile names 📝\nSet up MFA on your SSO account 🔐\n\n\n\n\n\n\nRemove plain text credentials from ~/.aws/credentials\nUpdate scripts to use --profile flag\nSet default profile: export AWS_PROFILE=disasters-sso\nConsider using aws-sso-util for enhanced SSO features\n\n\n\n\n\n\nAWS CLI SSO Documentation\nIAM Identity Center User Guide\nAWS CLI Command Reference\n\n\n🔐 Remember: Security is everyone’s responsibility. Keep your credentials safe!",
    "crumbs": [
      "AWS",
      "🚀 AWS SSO Configuration Guide"
    ]
  },
  {
    "objectID": "AWS/AWS_SSO_Setup_Guide.html#secure-your-aws-credentials-with-aws-identity-center-sso",
    "href": "AWS/AWS_SSO_Setup_Guide.html#secure-your-aws-credentials-with-aws-identity-center-sso",
    "title": "🚀 AWS SSO Configuration Guide",
    "section": "",
    "text": "This guide walks you through setting up AWS Single Sign-On (SSO) to securely manage your AWS credentials without storing them in plain text.",
    "crumbs": [
      "AWS",
      "🚀 AWS SSO Configuration Guide"
    ]
  },
  {
    "objectID": "AWS/AWS_SSO_Setup_Guide.html#prerequisites",
    "href": "AWS/AWS_SSO_Setup_Guide.html#prerequisites",
    "title": "🚀 AWS SSO Configuration Guide",
    "section": "",
    "text": "✅ AWS CLI v2 installed (version 2.x or higher)\n✅ Access to AWS Identity Center (formerly AWS SSO)\n✅ Your organization’s SSO portal URL",
    "crumbs": [
      "AWS",
      "🚀 AWS SSO Configuration Guide"
    ]
  },
  {
    "objectID": "AWS/AWS_SSO_Setup_Guide.html#step-by-step-setup-instructions",
    "href": "AWS/AWS_SSO_Setup_Guide.html#step-by-step-setup-instructions",
    "title": "🚀 AWS SSO Configuration Guide",
    "section": "",
    "text": "aws configure sso\n\n\n\nWhen prompted for SSO session name, enter a descriptive name for your profile:\nSSO session name (Recommended): disasters\n💡 Tip: Use meaningful names like prod-admin, dev-poweruser, etc.\n\n\n\nFind your SSO URL in the AWS Identity Center portal and enter it:\nSSO start URL [None]: https://d-9067c5bbc5.awsapps.com/start/#\n📍 Where to find: Navigate to your AWS SSO portal → Look for the URL in your browser\n\n\n\nEnter the region where your Identity Center is configured:\nSSO region [None]: us-east-1\n\n\n\nPress Enter to accept the default:\nSSO registration scopes [sso:account:access]: \n✨ The default scope is sufficient for most use cases\n\n\n\n🌐 A browser window will open automatically: 1. Log in with your corporate credentials 2. Click “Allow” to grant access to AWS CLI (botocore) 3. Return to your terminal\n\n\n\nEnter your AWS account ID (12 digits):\nAWS account ID: 867530900000\n📝 Find this in your AWS SSO portal under the accounts tab\n\n\n\nSelect from available roles:\nThere are 2 roles available to you.\n&gt; Project-Power-User\n  ReadOnlyAccess\nUse arrow keys to select, then press Enter\n\n\n\nConfirm or change the AWS region:\nCLI default region [us-east-1]: us-east-1\n\n\n\nChoose your preferred output format:\nCLI default output format [None]: json\nOptions: json, yaml, text, table",
    "crumbs": [
      "AWS",
      "🚀 AWS SSO Configuration Guide"
    ]
  },
  {
    "objectID": "AWS/AWS_SSO_Setup_Guide.html#verification",
    "href": "AWS/AWS_SSO_Setup_Guide.html#verification",
    "title": "🚀 AWS SSO Configuration Guide",
    "section": "",
    "text": "Test your configuration:\naws s3 ls --profile disasters-sso\nExpected output:\n                           PRE browseui/\n                           PRE california_wildfires_202501/\n                           PRE disasters/\n                           ...",
    "crumbs": [
      "AWS",
      "🚀 AWS SSO Configuration Guide"
    ]
  },
  {
    "objectID": "AWS/AWS_SSO_Setup_Guide.html#daily-usage",
    "href": "AWS/AWS_SSO_Setup_Guide.html#daily-usage",
    "title": "🚀 AWS SSO Configuration Guide",
    "section": "",
    "text": "aws sso login --profile disasters-sso\n\n\n\n# List S3 buckets\naws s3 ls --profile disasters-sso\n\n# Get caller identity\naws sts get-caller-identity --profile disasters-sso\n\n\n\nTo avoid passing --profile disasters-sso with every command, you can set the AWS_PROFILE environment variable:\n# Set the environment variable\nexport AWS_PROFILE=disasters-sso\n\n# Now you can run commands without --profile\naws s3 ls\naws sts get-caller-identity\nThis is especially helpful when running many AWS commands in a session.\n\n\n\naws sso logout",
    "crumbs": [
      "AWS",
      "🚀 AWS SSO Configuration Guide"
    ]
  },
  {
    "objectID": "AWS/AWS_SSO_Setup_Guide.html#configuration-files",
    "href": "AWS/AWS_SSO_Setup_Guide.html#configuration-files",
    "title": "🚀 AWS SSO Configuration Guide",
    "section": "",
    "text": "Your SSO configuration is stored in ~/.aws/config:\n[profile disasters-sso]\nsso_session = disasters\nsso_account_id = 867530900000\nsso_role_name = Project-Power-User\nregion = us-east-1\noutput = json\n\n[sso-session disasters]\nsso_start_url = https://d-9067c5bbc5.awsapps.com/start/#\nsso_region = us-east-1\nsso_registration_scopes = sso:account:access",
    "crumbs": [
      "AWS",
      "🚀 AWS SSO Configuration Guide"
    ]
  },
  {
    "objectID": "AWS/AWS_SSO_Setup_Guide.html#important-notes",
    "href": "AWS/AWS_SSO_Setup_Guide.html#important-notes",
    "title": "🚀 AWS SSO Configuration Guide",
    "section": "",
    "text": "AWS SSO provides temporary credentials that expire after 1-12 hours\nCredentials are automatically refreshed when you run commands\nNo permanent credentials are stored on your machine\n\n\n\n\nAWS-Vault expects permanent credentials to generate temporary ones. Since AWS SSO already provides temporary credentials: - Adding SSO temporary credentials to aws-vault causes authentication errors - SSO handles credential refresh automatically, making aws-vault redundant - Use AWS SSO for temporary credential profiles, aws-vault for permanent ones",
    "crumbs": [
      "AWS",
      "🚀 AWS SSO Configuration Guide"
    ]
  },
  {
    "objectID": "AWS/AWS_SSO_Setup_Guide.html#troubleshooting",
    "href": "AWS/AWS_SSO_Setup_Guide.html#troubleshooting",
    "title": "🚀 AWS SSO Configuration Guide",
    "section": "",
    "text": "Cause: Trying to use expired temporary credentials Solution: Run aws sso login --profile your-profile to refresh\n\n\n\nSolution: Add --use-device-code flag:\naws sso login --profile disasters-sso --use-device-code\n\n\n\nCreate separate profiles for each account/role combination:\n[profile prod-admin]\nsso_session = mycompany\nsso_account_id = 111111111111\nsso_role_name = Administrator\n\n[profile dev-readonly]\nsso_session = mycompany\nsso_account_id = 222222222222\nsso_role_name = ReadOnlyAccess",
    "crumbs": [
      "AWS",
      "🚀 AWS SSO Configuration Guide"
    ]
  },
  {
    "objectID": "AWS/AWS_SSO_Setup_Guide.html#best-practices",
    "href": "AWS/AWS_SSO_Setup_Guide.html#best-practices",
    "title": "🚀 AWS SSO Configuration Guide",
    "section": "",
    "text": "Never store credentials in plain text ❌\nUse SSO for all AWS access ✅\nLogout when finished working 🔒\nUse descriptive profile names 📝\nSet up MFA on your SSO account 🔐",
    "crumbs": [
      "AWS",
      "🚀 AWS SSO Configuration Guide"
    ]
  },
  {
    "objectID": "AWS/AWS_SSO_Setup_Guide.html#next-steps",
    "href": "AWS/AWS_SSO_Setup_Guide.html#next-steps",
    "title": "🚀 AWS SSO Configuration Guide",
    "section": "",
    "text": "Remove plain text credentials from ~/.aws/credentials\nUpdate scripts to use --profile flag\nSet default profile: export AWS_PROFILE=disasters-sso\nConsider using aws-sso-util for enhanced SSO features",
    "crumbs": [
      "AWS",
      "🚀 AWS SSO Configuration Guide"
    ]
  },
  {
    "objectID": "AWS/AWS_SSO_Setup_Guide.html#resources",
    "href": "AWS/AWS_SSO_Setup_Guide.html#resources",
    "title": "🚀 AWS SSO Configuration Guide",
    "section": "",
    "text": "AWS CLI SSO Documentation\nIAM Identity Center User Guide\nAWS CLI Command Reference\n\n\n🔐 Remember: Security is everyone’s responsibility. Keep your credentials safe!",
    "crumbs": [
      "AWS",
      "🚀 AWS SSO Configuration Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html",
    "href": "AWS/aws-s3-commands-guide.html",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "AWS CLI Installation and Configuration\nBasic S3 Commands\nS3 API Commands\nRecursive Operations\nData Upload and Sync Operations\nBucket Management\nObject Management\nAccess Control and Permissions\nPerformance Optimization\nCost Management\nTroubleshooting\nOfficial AWS Resources\n\n\n\n\n\n\n# macOS using Homebrew\nbrew install awscli\n\n# Using pip\npip install awscli --upgrade --user\n\n# Verify installation\naws --version\n\n\n\n# Configure AWS CLI with credentials\naws configure\n\n# Configure specific profile\naws configure --profile myprofile\n\n# List configuration\naws configure list\n\n# Set region for current session\nexport AWS_DEFAULT_REGION=us-east-1\n\n\n\n\n\n\n# List all buckets\naws s3 ls\n\n# List objects in a bucket\naws s3 ls s3://my-bucket/\n\n# List objects with human-readable sizes\naws s3 ls s3://my-bucket/ --human-readable\n\n# List objects with summary\naws s3 ls s3://my-bucket/ --summarize\n\n# List objects recursively\naws s3 ls s3://my-bucket/ --recursive\n\n# List objects with specific prefix\naws s3 ls s3://my-bucket/prefix/ --recursive\n\n\n\n# Copy file to S3\naws s3 cp file.txt s3://my-bucket/\n\n# Copy from S3 to local\naws s3 cp s3://my-bucket/file.txt ./\n\n# Copy between S3 buckets\naws s3 cp s3://source-bucket/file.txt s3://dest-bucket/\n\n# Copy with specific storage class\naws s3 cp file.txt s3://my-bucket/ --storage-class GLACIER\n\n# Copy with server-side encryption\naws s3 cp file.txt s3://my-bucket/ --sse AES256\n\n\n\n# Move file to S3\naws s3 mv file.txt s3://my-bucket/\n\n# Move from S3 to local\naws s3 mv s3://my-bucket/file.txt ./\n\n# Move between S3 locations\naws s3 mv s3://my-bucket/old-path/ s3://my-bucket/new-path/ --recursive\n\n\n\n# Delete single object\naws s3 rm s3://my-bucket/file.txt\n\n# Delete all objects with prefix\naws s3 rm s3://my-bucket/prefix/ --recursive\n\n# Delete bucket (must be empty)\naws s3 rb s3://my-bucket/\n\n# Force delete bucket with contents\naws s3 rb s3://my-bucket/ --force\n\n\n\n\n\n\n# Create bucket (us-east-1)\naws s3api create-bucket --bucket my-bucket\n\n# Create bucket in specific region\naws s3api create-bucket --bucket my-bucket \\\n  --region us-west-2 \\\n  --create-bucket-configuration LocationConstraint=us-west-2\n\n# Enable versioning\naws s3api put-bucket-versioning --bucket my-bucket \\\n  --versioning-configuration Status=Enabled\n\n# Enable server-side encryption by default\naws s3api put-bucket-encryption --bucket my-bucket \\\n  --server-side-encryption-configuration '{\n    \"Rules\": [{\n      \"ApplyServerSideEncryptionByDefault\": {\n        \"SSEAlgorithm\": \"AES256\"\n      }\n    }]\n  }'\n\n\n\n# Put object\naws s3api put-object --bucket my-bucket --key file.txt --body ./file.txt\n\n# Put object with metadata\naws s3api put-object --bucket my-bucket --key file.txt \\\n  --body ./file.txt \\\n  --metadata '{\"author\":\"John Doe\",\"version\":\"1.0\"}'\n\n# Put object with content type\naws s3api put-object --bucket my-bucket --key image.jpg \\\n  --body ./image.jpg \\\n  --content-type image/jpeg\n\n# Put object with tags\naws s3api put-object --bucket my-bucket --key file.txt \\\n  --body ./file.txt \\\n  --tagging 'environment=production&team=data'\n\n\n\n# Initiate multipart upload\naws s3api create-multipart-upload --bucket my-bucket --key large-file.zip\n\n# Upload part\naws s3api upload-part --bucket my-bucket \\\n  --key large-file.zip \\\n  --part-number 1 \\\n  --body part1.dat \\\n  --upload-id \"upload-id-here\"\n\n# Complete multipart upload\naws s3api complete-multipart-upload --bucket my-bucket \\\n  --key large-file.zip \\\n  --upload-id \"upload-id-here\" \\\n  --multipart-upload file://parts.json\n\n# Abort multipart upload\naws s3api abort-multipart-upload --bucket my-bucket \\\n  --key large-file.zip \\\n  --upload-id \"upload-id-here\"\n\n\n\n# Get object metadata\naws s3api head-object --bucket my-bucket --key file.txt\n\n# Get object ACL\naws s3api get-object-acl --bucket my-bucket --key file.txt\n\n# Get object tags\naws s3api get-object-tagging --bucket my-bucket --key file.txt\n\n# List object versions\naws s3api list-object-versions --bucket my-bucket --prefix folder/\n\n\n\n\n\n\n# Sync local directory to S3\naws s3 sync ./local-folder s3://my-bucket/folder/\n\n# Sync S3 to local\naws s3 sync s3://my-bucket/folder/ ./local-folder\n\n# Sync with delete (remove files not in source)\naws s3 sync ./local-folder s3://my-bucket/folder/ --delete\n\n# Sync only specific file types\naws s3 sync ./local-folder s3://my-bucket/folder/ \\\n  --exclude \"*\" --include \"*.jpg\"\n\n# Sync with size-only comparison (faster)\naws s3 sync ./local-folder s3://my-bucket/folder/ --size-only\n\n# Dry run to preview changes\naws s3 sync ./local-folder s3://my-bucket/folder/ --dryrun\n\n\n\n# Copy entire directory\naws s3 cp ./local-folder s3://my-bucket/folder/ --recursive\n\n# Copy with exclude patterns\naws s3 cp s3://my-bucket/ s3://backup-bucket/ \\\n  --recursive \\\n  --exclude \"*.tmp\" \\\n  --exclude \"logs/*\"\n\n# Copy with include patterns\naws s3 cp s3://my-bucket/ s3://backup-bucket/ \\\n  --recursive \\\n  --exclude \"*\" \\\n  --include \"*.pdf\" \\\n  --include \"*.docx\"\n\n# Copy files modified after specific date\naws s3 cp s3://my-bucket/ ./local-folder/ \\\n  --recursive \\\n  --exclude \"*\" \\\n  --include \"*\" \\\n  --metadata-directive COPY\n\n\n\n\n\n\n# Upload multiple files with parallel transfers\naws s3 cp ./data-folder s3://my-bucket/data/ \\\n  --recursive \\\n  --cli-write-timeout 0 \\\n  --cli-read-timeout 0\n\n# Upload with progress bar\naws s3 cp large-file.zip s3://my-bucket/ \\\n  --no-guess-mime-type \\\n  --cli-progress-bar on\n\n# Upload with bandwidth limit (KB/s)\naws configure set s3.max_bandwidth 5000KB/s\naws s3 cp ./large-folder s3://my-bucket/ --recursive\n\n\n\n# Sync with exact timestamps\naws s3 sync ./folder s3://my-bucket/ --exact-timestamps\n\n# Sync with follow symlinks\naws s3 sync ./folder s3://my-bucket/ --follow-symlinks\n\n# Sync with no follow symlinks\naws s3 sync ./folder s3://my-bucket/ --no-follow-symlinks\n\n# Sync with ACL settings\naws s3 sync ./folder s3://my-bucket/ --acl public-read\n\n# Sync with storage class\naws s3 sync ./folder s3://my-bucket/ \\\n  --storage-class INTELLIGENT_TIERING\n\n\n\n\n\n\n# Get bucket policy\naws s3api get-bucket-policy --bucket my-bucket\n\n# Put bucket policy\naws s3api put-bucket-policy --bucket my-bucket \\\n  --policy file://bucket-policy.json\n\n# Delete bucket policy\naws selman get-bucket-policy --bucket my-bucket\n\n# Example bucket policy (bucket-policy.json)\ncat &gt; bucket-policy.json &lt;&lt; 'EOF'\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"PublicReadGetObject\",\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::my-bucket/*\"\n    }\n  ]\n}\nEOF\n\n\n\n# Put lifecycle configuration\naws s3api put-bucket-lifecycle-configuration \\\n  --bucket my-bucket \\\n  --lifecycle-configuration file://lifecycle.json\n\n# Get lifecycle configuration\naws s3api get-bucket-lifecycle-configuration --bucket my-bucket\n\n# Example lifecycle configuration\ncat &gt; lifecycle.json &lt;&lt; 'EOF'\n{\n  \"Rules\": [\n    {\n      \"ID\": \"Archive old files\",\n      \"Status\": \"Enabled\",\n      \"Transitions\": [\n        {\n          \"Days\": 30,\n          \"StorageClass\": \"INTELLIGENT_TIERING\"\n        },\n        {\n          \"Days\": 90,\n          \"StorageClass\": \"GLACIER\"\n        }\n      ]\n    }\n  ]\n}\nEOF\n\n\n\n# Put CORS configuration\naws s3api put-bucket-cors --bucket my-bucket \\\n  --cors-configuration file://cors.json\n\n# Get CORS configuration\naws s3api get-bucket-cors --bucket my-bucket\n\n# Example CORS configuration\ncat &gt; cors.json &lt;&lt; 'EOF'\n{\n  \"CORSRules\": [\n    {\n      \"AllowedOrigins\": [\"*\"],\n      \"AllowedMethods\": [\"GET\", \"PUT\", \"POST\"],\n      \"AllowedHeaders\": [\"*\"],\n      \"MaxAgeSeconds\": 3000\n    }\n  ]\n}\nEOF\n\n\n\n\n\n\n# Copy object within same bucket\naws s3api copy-object \\\n  --bucket my-bucket \\\n  --copy-source my-bucket/old-key \\\n  --key new-key\n\n# Restore object from Glacier\naws s3api restore-object \\\n  --bucket my-bucket \\\n  --key archived-file.txt \\\n  --restore-request Days=7\n\n# Generate presigned URL (expires in 1 hour)\naws s3 presign s3://my-bucket/file.txt --expires-in 3600\n\n# Batch delete objects\naws s3api delete-objects --bucket my-bucket \\\n  --delete file://delete.json\n\n# Example delete.json\ncat &gt; delete.json &lt;&lt; 'EOF'\n{\n  \"Objects\": [\n    {\"Key\": \"file1.txt\"},\n    {\"Key\": \"file2.txt\"},\n    {\"Key\": \"folder/file3.txt\"}\n  ]\n}\nEOF\n\n\n\n# Put object tags\naws s3api put-object-tagging \\\n  --bucket my-bucket \\\n  --key file.txt \\\n  --tagging 'TagSet=[{Key=environment,Value=prod},{Key=owner,Value=teamA}]'\n\n# Get object tags\naws s3api get-object-tagging --bucket my-bucket --key file.txt\n\n# Delete object tags\naws s3api delete-object-tagging --bucket my-bucket --key file.txt\n\n\n\n\n\n\n# Put bucket ACL\naws s3api put-bucket-acl --bucket my-bucket --acl private\n\n# Put object ACL\naws s3api put-object-acl --bucket my-bucket --key file.txt --acl public-read\n\n# Grant specific permissions\naws s3api put-object-acl --bucket my-bucket --key file.txt \\\n  --grant-read emailaddress=user@example.com \\\n  --grant-write emailaddress=admin@example.com\n\n# Put bucket public access block\naws s3api put-public-access-block --bucket my-bucket \\\n  --public-access-block-configuration \\\n  BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=false,RestrictPublicBuckets=false\n\n\n\n# Example IAM policy for S3 access\ncat &gt; s3-policy.json &lt;&lt; 'EOF'\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::my-bucket/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:ListBucket\",\n      \"Resource\": \"arn:aws:s3:::my-bucket\"\n    }\n  ]\n}\nEOF\n\n# Attach policy to user\naws iam put-user-policy --user-name myuser \\\n  --policy-name S3Access \\\n  --policy-document file://s3-policy.json\n\n\n\n\n\n\n# Enable transfer acceleration\naws s3api put-bucket-accelerate-configuration \\\n  --bucket my-bucket \\\n  --accelerate-configuration Status=Enabled\n\n# Use accelerated endpoint\naws s3 cp file.txt s3://my-bucket/ \\\n  --endpoint-url https://my-bucket.s3-accelerate.amazonaws.com\n\n\n\n# Configure concurrent requests\naws configure set s3.max_concurrent_requests 20\naws configure set s3.max_queue_size 10000\n\n# Use multipart threshold for large files\naws configure set s3.multipart_threshold 64MB\naws configure set s3.multipart_chunksize 16MB\n\n# Set max bandwidth\naws configure set s3.max_bandwidth 100MB/s\n\n\n\n# Enable requester pays\naws s3api put-bucket-request-payment \\\n  --bucket my-bucket \\\n  --request-payment-configuration Payer=Requester\n\n# Access requester-pays bucket\naws s3 cp s3://requester-pays-bucket/file.txt ./ --request-payer requester\n\n\n\n\n\n\n# Put analytics configuration\naws s3api put-bucket-analytics-configuration \\\n  --bucket my-bucket \\\n  --id analysis-1 \\\n  --analytics-configuration file://analytics.json\n\n# List analytics configurations\naws s3api list-bucket-analytics-configurations --bucket my-bucket\n\n\n\n# Put intelligent tiering configuration\naws s3api put-bucket-intelligent-tiering-configuration \\\n  --bucket my-bucket \\\n  --id config-1 \\\n  --intelligent-tiering-configuration file://tiering.json\n\n\n\n# Get bucket metrics configuration\naws s3api get-bucket-metrics-configuration \\\n  --bucket my-bucket \\\n  --id metrics-1\n\n# List bucket metrics\naws s3api list-bucket-metrics-configurations --bucket my-bucket\n\n\n\n\n\n\n\n\n# Test if bucket exists\naws s3api head-bucket --bucket my-bucket\n\n# Check bucket location\naws s3api get-bucket-location --bucket my-bucket\n\n# List bucket with debug output\naws s3 ls s3://my-bucket/ --debug\n\n\n\n# Check IAM permissions\naws iam simulate-principal-policy \\\n  --policy-source-arn arn:aws:iam::123456789012:user/username \\\n  --action-names s3:GetObject s3:PutObject \\\n  --resource-arns arn:aws:s3:::my-bucket/*\n\n\n\n# Test S3 endpoint connectivity\naws s3 ls --debug 2&gt;&1 | grep \"endpoint\"\n\n# Use specific endpoint\naws s3 ls --endpoint-url https://s3.us-west-2.amazonaws.com\n\n# Check DNS resolution\nnslookup s3.amazonaws.com\n\n\n\n\n\n\n\n# Query CSV file with S3 Select\naws s3api select-object-content \\\n  --bucket my-bucket \\\n  --key data.csv \\\n  --expression \"SELECT * FROM S3Object WHERE age &gt; 25\" \\\n  --expression-type SQL \\\n  --input-serialization '{\"CSV\": {\"FileHeaderInfo\": \"USE\"}}' \\\n  --output-serialization '{\"CSV\": {}}' \\\n  output.csv\n\n\n\n# Put inventory configuration\naws s3api put-bucket-inventory-configuration \\\n  --bucket my-bucket \\\n  --id inventory-1 \\\n  --inventory-configuration file://inventory.json\n\n\n\n# Create batch job\naws s3control create-job \\\n  --account-id 123456789012 \\\n  --manifest file://manifest.json \\\n  --operation file://operation.json \\\n  --priority 10 \\\n  --role-arn arn:aws:iam::123456789012:role/batch-operations-role\n\n\n\n\n\n\n# Enable default encryption\naws s3api put-bucket-encryption --bucket my-bucket \\\n  --server-side-encryption-configuration '{\n    \"Rules\": [{\n      \"ApplyServerSideEncryptionByDefault\": {\n        \"SSEAlgorithm\": \"aws:kms\",\n        \"KMSMasterKeyID\": \"arn:aws:kms:us-east-1:123456789012:key/12345678\"\n      }\n    }]\n  }'\n\n# Enable bucket logging\naws s3api put-bucket-logging --bucket my-bucket \\\n  --bucket-logging-status file://logging.json\n\n# Enable MFA delete\naws s3api put-bucket-versioning --bucket my-bucket \\\n  --versioning-configuration Status=Enabled,MFADelete=Enabled \\\n  --mfa \"arn:aws:iam::123456789012:mfa/root-account-mfa-device 123456\"\n\n\n\n# Upload with checksum\naws s3api put-object --bucket my-bucket --key file.txt \\\n  --body ./file.txt \\\n  --content-md5 $(openssl dgst -md5 -binary file.txt | base64)\n\n# Verify object integrity\naws s3api head-object --bucket my-bucket --key file.txt \\\n  --checksum-mode ENABLED\n\n\n\n\n\n\n\nAWS CLI Command Reference\nAWS S3 API Reference\nS3 User Guide\nS3 Best Practices\nS3 Security Best Practices\n\n\n\n\n\nGetting Started with S3\nS3 Storage Classes\nS3 Transfer Acceleration\nS3 Lifecycle Policies\nS3 Replication\n\n\n\n\n\nAWS SDK for Python (Boto3)\nAWS SDK for JavaScript\nS3 API Examples\nS3 Select Examples\n\n\n\n\n\nAWS CLI Installation\nAWS CLI Configuration\nS3 Browser Tools\nCloudFormation S3 Templates\n\n\n\n\n\nS3 CloudWatch Metrics\nS3 Access Logging\nS3 Troubleshooting\nS3 Error Responses\n\n\n\n\n\nS3 Pricing\nS3 Cost Optimization\nS3 Storage Lens\nAWS Cost Explorer\n\n\n\n\n\nS3 Compliance\nS3 Object Lock\nAWS Config Rules for S3\nS3 Access Points\n\n\n\n\n\n\n\n# Upload file\naws s3 cp file.txt s3://bucket/\n\n# Download file\naws s3 cp s3://bucket/file.txt ./\n\n# Sync directory\naws s3 sync ./folder s3://bucket/folder/\n\n# List contents\naws s3 ls s3://bucket/ --recursive\n\n# Delete file\naws s3 rm s3://bucket/file.txt\n\n# Create bucket\naws s3 mb s3://new-bucket/\n\n# Remove bucket\naws s3 rb s3://bucket/ --force\n\n# Get object info\naws s3api head-object --bucket bucket --key file.txt\n\n# Generate presigned URL\naws s3 presign s3://bucket/file.txt\n\n# Check bucket access\naws s3api head-bucket --bucket bucket\n\n\n\n\n# AWS Credentials\nexport AWS_ACCESS_KEY_ID=your-access-key\nexport AWS_SECRET_ACCESS_KEY=your-secret-key\nexport AWS_SESSION_TOKEN=your-session-token\n\n# AWS Configuration\nexport AWS_DEFAULT_REGION=us-east-1\nexport AWS_DEFAULT_OUTPUT=json\nexport AWS_PROFILE=myprofile\n\n# S3 Specific\nexport AWS_S3_ENDPOINT=https://s3.amazonaws.com\nexport S3_USE_ACCELERATE_ENDPOINT=true\n\n\n\nThis guide covers the essential AWS S3 commands and operations for data management. Always refer to the official AWS documentation for the most up-to-date information and additional features. Remember to follow security best practices and implement proper access controls when working with S3 buckets and objects.",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#table-of-contents",
    "href": "AWS/aws-s3-commands-guide.html#table-of-contents",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "AWS CLI Installation and Configuration\nBasic S3 Commands\nS3 API Commands\nRecursive Operations\nData Upload and Sync Operations\nBucket Management\nObject Management\nAccess Control and Permissions\nPerformance Optimization\nCost Management\nTroubleshooting\nOfficial AWS Resources",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#aws-cli-installation-and-configuration",
    "href": "AWS/aws-s3-commands-guide.html#aws-cli-installation-and-configuration",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# macOS using Homebrew\nbrew install awscli\n\n# Using pip\npip install awscli --upgrade --user\n\n# Verify installation\naws --version\n\n\n\n# Configure AWS CLI with credentials\naws configure\n\n# Configure specific profile\naws configure --profile myprofile\n\n# List configuration\naws configure list\n\n# Set region for current session\nexport AWS_DEFAULT_REGION=us-east-1",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#basic-s3-commands",
    "href": "AWS/aws-s3-commands-guide.html#basic-s3-commands",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# List all buckets\naws s3 ls\n\n# List objects in a bucket\naws s3 ls s3://my-bucket/\n\n# List objects with human-readable sizes\naws s3 ls s3://my-bucket/ --human-readable\n\n# List objects with summary\naws s3 ls s3://my-bucket/ --summarize\n\n# List objects recursively\naws s3 ls s3://my-bucket/ --recursive\n\n# List objects with specific prefix\naws s3 ls s3://my-bucket/prefix/ --recursive\n\n\n\n# Copy file to S3\naws s3 cp file.txt s3://my-bucket/\n\n# Copy from S3 to local\naws s3 cp s3://my-bucket/file.txt ./\n\n# Copy between S3 buckets\naws s3 cp s3://source-bucket/file.txt s3://dest-bucket/\n\n# Copy with specific storage class\naws s3 cp file.txt s3://my-bucket/ --storage-class GLACIER\n\n# Copy with server-side encryption\naws s3 cp file.txt s3://my-bucket/ --sse AES256\n\n\n\n# Move file to S3\naws s3 mv file.txt s3://my-bucket/\n\n# Move from S3 to local\naws s3 mv s3://my-bucket/file.txt ./\n\n# Move between S3 locations\naws s3 mv s3://my-bucket/old-path/ s3://my-bucket/new-path/ --recursive\n\n\n\n# Delete single object\naws s3 rm s3://my-bucket/file.txt\n\n# Delete all objects with prefix\naws s3 rm s3://my-bucket/prefix/ --recursive\n\n# Delete bucket (must be empty)\naws s3 rb s3://my-bucket/\n\n# Force delete bucket with contents\naws s3 rb s3://my-bucket/ --force",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#s3-api-commands",
    "href": "AWS/aws-s3-commands-guide.html#s3-api-commands",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# Create bucket (us-east-1)\naws s3api create-bucket --bucket my-bucket\n\n# Create bucket in specific region\naws s3api create-bucket --bucket my-bucket \\\n  --region us-west-2 \\\n  --create-bucket-configuration LocationConstraint=us-west-2\n\n# Enable versioning\naws s3api put-bucket-versioning --bucket my-bucket \\\n  --versioning-configuration Status=Enabled\n\n# Enable server-side encryption by default\naws s3api put-bucket-encryption --bucket my-bucket \\\n  --server-side-encryption-configuration '{\n    \"Rules\": [{\n      \"ApplyServerSideEncryptionByDefault\": {\n        \"SSEAlgorithm\": \"AES256\"\n      }\n    }]\n  }'\n\n\n\n# Put object\naws s3api put-object --bucket my-bucket --key file.txt --body ./file.txt\n\n# Put object with metadata\naws s3api put-object --bucket my-bucket --key file.txt \\\n  --body ./file.txt \\\n  --metadata '{\"author\":\"John Doe\",\"version\":\"1.0\"}'\n\n# Put object with content type\naws s3api put-object --bucket my-bucket --key image.jpg \\\n  --body ./image.jpg \\\n  --content-type image/jpeg\n\n# Put object with tags\naws s3api put-object --bucket my-bucket --key file.txt \\\n  --body ./file.txt \\\n  --tagging 'environment=production&team=data'\n\n\n\n# Initiate multipart upload\naws s3api create-multipart-upload --bucket my-bucket --key large-file.zip\n\n# Upload part\naws s3api upload-part --bucket my-bucket \\\n  --key large-file.zip \\\n  --part-number 1 \\\n  --body part1.dat \\\n  --upload-id \"upload-id-here\"\n\n# Complete multipart upload\naws s3api complete-multipart-upload --bucket my-bucket \\\n  --key large-file.zip \\\n  --upload-id \"upload-id-here\" \\\n  --multipart-upload file://parts.json\n\n# Abort multipart upload\naws s3api abort-multipart-upload --bucket my-bucket \\\n  --key large-file.zip \\\n  --upload-id \"upload-id-here\"\n\n\n\n# Get object metadata\naws s3api head-object --bucket my-bucket --key file.txt\n\n# Get object ACL\naws s3api get-object-acl --bucket my-bucket --key file.txt\n\n# Get object tags\naws s3api get-object-tagging --bucket my-bucket --key file.txt\n\n# List object versions\naws s3api list-object-versions --bucket my-bucket --prefix folder/",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#recursive-operations",
    "href": "AWS/aws-s3-commands-guide.html#recursive-operations",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# Sync local directory to S3\naws s3 sync ./local-folder s3://my-bucket/folder/\n\n# Sync S3 to local\naws s3 sync s3://my-bucket/folder/ ./local-folder\n\n# Sync with delete (remove files not in source)\naws s3 sync ./local-folder s3://my-bucket/folder/ --delete\n\n# Sync only specific file types\naws s3 sync ./local-folder s3://my-bucket/folder/ \\\n  --exclude \"*\" --include \"*.jpg\"\n\n# Sync with size-only comparison (faster)\naws s3 sync ./local-folder s3://my-bucket/folder/ --size-only\n\n# Dry run to preview changes\naws s3 sync ./local-folder s3://my-bucket/folder/ --dryrun\n\n\n\n# Copy entire directory\naws s3 cp ./local-folder s3://my-bucket/folder/ --recursive\n\n# Copy with exclude patterns\naws s3 cp s3://my-bucket/ s3://backup-bucket/ \\\n  --recursive \\\n  --exclude \"*.tmp\" \\\n  --exclude \"logs/*\"\n\n# Copy with include patterns\naws s3 cp s3://my-bucket/ s3://backup-bucket/ \\\n  --recursive \\\n  --exclude \"*\" \\\n  --include \"*.pdf\" \\\n  --include \"*.docx\"\n\n# Copy files modified after specific date\naws s3 cp s3://my-bucket/ ./local-folder/ \\\n  --recursive \\\n  --exclude \"*\" \\\n  --include \"*\" \\\n  --metadata-directive COPY",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#data-upload-and-sync-operations",
    "href": "AWS/aws-s3-commands-guide.html#data-upload-and-sync-operations",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# Upload multiple files with parallel transfers\naws s3 cp ./data-folder s3://my-bucket/data/ \\\n  --recursive \\\n  --cli-write-timeout 0 \\\n  --cli-read-timeout 0\n\n# Upload with progress bar\naws s3 cp large-file.zip s3://my-bucket/ \\\n  --no-guess-mime-type \\\n  --cli-progress-bar on\n\n# Upload with bandwidth limit (KB/s)\naws configure set s3.max_bandwidth 5000KB/s\naws s3 cp ./large-folder s3://my-bucket/ --recursive\n\n\n\n# Sync with exact timestamps\naws s3 sync ./folder s3://my-bucket/ --exact-timestamps\n\n# Sync with follow symlinks\naws s3 sync ./folder s3://my-bucket/ --follow-symlinks\n\n# Sync with no follow symlinks\naws s3 sync ./folder s3://my-bucket/ --no-follow-symlinks\n\n# Sync with ACL settings\naws s3 sync ./folder s3://my-bucket/ --acl public-read\n\n# Sync with storage class\naws s3 sync ./folder s3://my-bucket/ \\\n  --storage-class INTELLIGENT_TIERING",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#bucket-management",
    "href": "AWS/aws-s3-commands-guide.html#bucket-management",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# Get bucket policy\naws s3api get-bucket-policy --bucket my-bucket\n\n# Put bucket policy\naws s3api put-bucket-policy --bucket my-bucket \\\n  --policy file://bucket-policy.json\n\n# Delete bucket policy\naws selman get-bucket-policy --bucket my-bucket\n\n# Example bucket policy (bucket-policy.json)\ncat &gt; bucket-policy.json &lt;&lt; 'EOF'\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"PublicReadGetObject\",\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::my-bucket/*\"\n    }\n  ]\n}\nEOF\n\n\n\n# Put lifecycle configuration\naws s3api put-bucket-lifecycle-configuration \\\n  --bucket my-bucket \\\n  --lifecycle-configuration file://lifecycle.json\n\n# Get lifecycle configuration\naws s3api get-bucket-lifecycle-configuration --bucket my-bucket\n\n# Example lifecycle configuration\ncat &gt; lifecycle.json &lt;&lt; 'EOF'\n{\n  \"Rules\": [\n    {\n      \"ID\": \"Archive old files\",\n      \"Status\": \"Enabled\",\n      \"Transitions\": [\n        {\n          \"Days\": 30,\n          \"StorageClass\": \"INTELLIGENT_TIERING\"\n        },\n        {\n          \"Days\": 90,\n          \"StorageClass\": \"GLACIER\"\n        }\n      ]\n    }\n  ]\n}\nEOF\n\n\n\n# Put CORS configuration\naws s3api put-bucket-cors --bucket my-bucket \\\n  --cors-configuration file://cors.json\n\n# Get CORS configuration\naws s3api get-bucket-cors --bucket my-bucket\n\n# Example CORS configuration\ncat &gt; cors.json &lt;&lt; 'EOF'\n{\n  \"CORSRules\": [\n    {\n      \"AllowedOrigins\": [\"*\"],\n      \"AllowedMethods\": [\"GET\", \"PUT\", \"POST\"],\n      \"AllowedHeaders\": [\"*\"],\n      \"MaxAgeSeconds\": 3000\n    }\n  ]\n}\nEOF",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#object-management",
    "href": "AWS/aws-s3-commands-guide.html#object-management",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# Copy object within same bucket\naws s3api copy-object \\\n  --bucket my-bucket \\\n  --copy-source my-bucket/old-key \\\n  --key new-key\n\n# Restore object from Glacier\naws s3api restore-object \\\n  --bucket my-bucket \\\n  --key archived-file.txt \\\n  --restore-request Days=7\n\n# Generate presigned URL (expires in 1 hour)\naws s3 presign s3://my-bucket/file.txt --expires-in 3600\n\n# Batch delete objects\naws s3api delete-objects --bucket my-bucket \\\n  --delete file://delete.json\n\n# Example delete.json\ncat &gt; delete.json &lt;&lt; 'EOF'\n{\n  \"Objects\": [\n    {\"Key\": \"file1.txt\"},\n    {\"Key\": \"file2.txt\"},\n    {\"Key\": \"folder/file3.txt\"}\n  ]\n}\nEOF\n\n\n\n# Put object tags\naws s3api put-object-tagging \\\n  --bucket my-bucket \\\n  --key file.txt \\\n  --tagging 'TagSet=[{Key=environment,Value=prod},{Key=owner,Value=teamA}]'\n\n# Get object tags\naws s3api get-object-tagging --bucket my-bucket --key file.txt\n\n# Delete object tags\naws s3api delete-object-tagging --bucket my-bucket --key file.txt",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#access-control-and-permissions",
    "href": "AWS/aws-s3-commands-guide.html#access-control-and-permissions",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# Put bucket ACL\naws s3api put-bucket-acl --bucket my-bucket --acl private\n\n# Put object ACL\naws s3api put-object-acl --bucket my-bucket --key file.txt --acl public-read\n\n# Grant specific permissions\naws s3api put-object-acl --bucket my-bucket --key file.txt \\\n  --grant-read emailaddress=user@example.com \\\n  --grant-write emailaddress=admin@example.com\n\n# Put bucket public access block\naws s3api put-public-access-block --bucket my-bucket \\\n  --public-access-block-configuration \\\n  BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=false,RestrictPublicBuckets=false\n\n\n\n# Example IAM policy for S3 access\ncat &gt; s3-policy.json &lt;&lt; 'EOF'\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::my-bucket/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:ListBucket\",\n      \"Resource\": \"arn:aws:s3:::my-bucket\"\n    }\n  ]\n}\nEOF\n\n# Attach policy to user\naws iam put-user-policy --user-name myuser \\\n  --policy-name S3Access \\\n  --policy-document file://s3-policy.json",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#performance-optimization",
    "href": "AWS/aws-s3-commands-guide.html#performance-optimization",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# Enable transfer acceleration\naws s3api put-bucket-accelerate-configuration \\\n  --bucket my-bucket \\\n  --accelerate-configuration Status=Enabled\n\n# Use accelerated endpoint\naws s3 cp file.txt s3://my-bucket/ \\\n  --endpoint-url https://my-bucket.s3-accelerate.amazonaws.com\n\n\n\n# Configure concurrent requests\naws configure set s3.max_concurrent_requests 20\naws configure set s3.max_queue_size 10000\n\n# Use multipart threshold for large files\naws configure set s3.multipart_threshold 64MB\naws configure set s3.multipart_chunksize 16MB\n\n# Set max bandwidth\naws configure set s3.max_bandwidth 100MB/s\n\n\n\n# Enable requester pays\naws s3api put-bucket-request-payment \\\n  --bucket my-bucket \\\n  --request-payment-configuration Payer=Requester\n\n# Access requester-pays bucket\naws s3 cp s3://requester-pays-bucket/file.txt ./ --request-payer requester",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#cost-management",
    "href": "AWS/aws-s3-commands-guide.html#cost-management",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# Put analytics configuration\naws s3api put-bucket-analytics-configuration \\\n  --bucket my-bucket \\\n  --id analysis-1 \\\n  --analytics-configuration file://analytics.json\n\n# List analytics configurations\naws s3api list-bucket-analytics-configurations --bucket my-bucket\n\n\n\n# Put intelligent tiering configuration\naws s3api put-bucket-intelligent-tiering-configuration \\\n  --bucket my-bucket \\\n  --id config-1 \\\n  --intelligent-tiering-configuration file://tiering.json\n\n\n\n# Get bucket metrics configuration\naws s3api get-bucket-metrics-configuration \\\n  --bucket my-bucket \\\n  --id metrics-1\n\n# List bucket metrics\naws s3api list-bucket-metrics-configurations --bucket my-bucket",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#troubleshooting",
    "href": "AWS/aws-s3-commands-guide.html#troubleshooting",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# Test if bucket exists\naws s3api head-bucket --bucket my-bucket\n\n# Check bucket location\naws s3api get-bucket-location --bucket my-bucket\n\n# List bucket with debug output\naws s3 ls s3://my-bucket/ --debug\n\n\n\n# Check IAM permissions\naws iam simulate-principal-policy \\\n  --policy-source-arn arn:aws:iam::123456789012:user/username \\\n  --action-names s3:GetObject s3:PutObject \\\n  --resource-arns arn:aws:s3:::my-bucket/*\n\n\n\n# Test S3 endpoint connectivity\naws s3 ls --debug 2&gt;&1 | grep \"endpoint\"\n\n# Use specific endpoint\naws s3 ls --endpoint-url https://s3.us-west-2.amazonaws.com\n\n# Check DNS resolution\nnslookup s3.amazonaws.com",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#advanced-operations",
    "href": "AWS/aws-s3-commands-guide.html#advanced-operations",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# Query CSV file with S3 Select\naws s3api select-object-content \\\n  --bucket my-bucket \\\n  --key data.csv \\\n  --expression \"SELECT * FROM S3Object WHERE age &gt; 25\" \\\n  --expression-type SQL \\\n  --input-serialization '{\"CSV\": {\"FileHeaderInfo\": \"USE\"}}' \\\n  --output-serialization '{\"CSV\": {}}' \\\n  output.csv\n\n\n\n# Put inventory configuration\naws s3api put-bucket-inventory-configuration \\\n  --bucket my-bucket \\\n  --id inventory-1 \\\n  --inventory-configuration file://inventory.json\n\n\n\n# Create batch job\naws s3control create-job \\\n  --account-id 123456789012 \\\n  --manifest file://manifest.json \\\n  --operation file://operation.json \\\n  --priority 10 \\\n  --role-arn arn:aws:iam::123456789012:role/batch-operations-role",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#best-practices",
    "href": "AWS/aws-s3-commands-guide.html#best-practices",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# Enable default encryption\naws s3api put-bucket-encryption --bucket my-bucket \\\n  --server-side-encryption-configuration '{\n    \"Rules\": [{\n      \"ApplyServerSideEncryptionByDefault\": {\n        \"SSEAlgorithm\": \"aws:kms\",\n        \"KMSMasterKeyID\": \"arn:aws:kms:us-east-1:123456789012:key/12345678\"\n      }\n    }]\n  }'\n\n# Enable bucket logging\naws s3api put-bucket-logging --bucket my-bucket \\\n  --bucket-logging-status file://logging.json\n\n# Enable MFA delete\naws s3api put-bucket-versioning --bucket my-bucket \\\n  --versioning-configuration Status=Enabled,MFADelete=Enabled \\\n  --mfa \"arn:aws:iam::123456789012:mfa/root-account-mfa-device 123456\"\n\n\n\n# Upload with checksum\naws s3api put-object --bucket my-bucket --key file.txt \\\n  --body ./file.txt \\\n  --content-md5 $(openssl dgst -md5 -binary file.txt | base64)\n\n# Verify object integrity\naws s3api head-object --bucket my-bucket --key file.txt \\\n  --checksum-mode ENABLED",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#official-aws-resources",
    "href": "AWS/aws-s3-commands-guide.html#official-aws-resources",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "AWS CLI Command Reference\nAWS S3 API Reference\nS3 User Guide\nS3 Best Practices\nS3 Security Best Practices\n\n\n\n\n\nGetting Started with S3\nS3 Storage Classes\nS3 Transfer Acceleration\nS3 Lifecycle Policies\nS3 Replication\n\n\n\n\n\nAWS SDK for Python (Boto3)\nAWS SDK for JavaScript\nS3 API Examples\nS3 Select Examples\n\n\n\n\n\nAWS CLI Installation\nAWS CLI Configuration\nS3 Browser Tools\nCloudFormation S3 Templates\n\n\n\n\n\nS3 CloudWatch Metrics\nS3 Access Logging\nS3 Troubleshooting\nS3 Error Responses\n\n\n\n\n\nS3 Pricing\nS3 Cost Optimization\nS3 Storage Lens\nAWS Cost Explorer\n\n\n\n\n\nS3 Compliance\nS3 Object Lock\nAWS Config Rules for S3\nS3 Access Points",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#quick-reference-card",
    "href": "AWS/aws-s3-commands-guide.html#quick-reference-card",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# Upload file\naws s3 cp file.txt s3://bucket/\n\n# Download file\naws s3 cp s3://bucket/file.txt ./\n\n# Sync directory\naws s3 sync ./folder s3://bucket/folder/\n\n# List contents\naws s3 ls s3://bucket/ --recursive\n\n# Delete file\naws s3 rm s3://bucket/file.txt\n\n# Create bucket\naws s3 mb s3://new-bucket/\n\n# Remove bucket\naws s3 rb s3://bucket/ --force\n\n# Get object info\naws s3api head-object --bucket bucket --key file.txt\n\n# Generate presigned URL\naws s3 presign s3://bucket/file.txt\n\n# Check bucket access\naws s3api head-bucket --bucket bucket",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#environment-variables",
    "href": "AWS/aws-s3-commands-guide.html#environment-variables",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "# AWS Credentials\nexport AWS_ACCESS_KEY_ID=your-access-key\nexport AWS_SECRET_ACCESS_KEY=your-secret-key\nexport AWS_SESSION_TOKEN=your-session-token\n\n# AWS Configuration\nexport AWS_DEFAULT_REGION=us-east-1\nexport AWS_DEFAULT_OUTPUT=json\nexport AWS_PROFILE=myprofile\n\n# S3 Specific\nexport AWS_S3_ENDPOINT=https://s3.amazonaws.com\nexport S3_USE_ACCELERATE_ENDPOINT=true",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-s3-commands-guide.html#conclusion",
    "href": "AWS/aws-s3-commands-guide.html#conclusion",
    "title": "AWS S3 Commands and Operations Guide",
    "section": "",
    "text": "This guide covers the essential AWS S3 commands and operations for data management. Always refer to the official AWS documentation for the most up-to-date information and additional features. Remember to follow security best practices and implement proper access controls when working with S3 buckets and objects.",
    "crumbs": [
      "AWS",
      "AWS S3 Commands and Operations Guide"
    ]
  },
  {
    "objectID": "AWS/aws-mfa-setup.html",
    "href": "AWS/aws-mfa-setup.html",
    "title": "AWS MFA Authentication Setup and Usage Guide",
    "section": "",
    "text": "Overview\nPrerequisites\nInitial Setup\nScript Installation\nUsage\nTroubleshooting\nBest Practices\nSecurity Considerations\n\n\n\n\n\n\n\nMulti-Factor Authentication (MFA) adds an extra layer of security to your AWS account by requiring two forms of identification: 1. Something you know (your password/credentials) 2. Something you have (a time-based token from your MFA device)\n\n\n\n\nEnhanced Security: Temporary credentials automatically expire, reducing the risk if they’re compromised\nCompliance: Many organizations require MFA for production AWS access\nBest Practice: AWS recommends using temporary credentials instead of long-lived access keys\nSession Management: Temporary credentials can be scoped with specific permissions\n\n\n\n\nThe ~/aws_mfa.sh script is a lightweight utility that you call with aws_env to: - Automatically detect your current AWS user - Find your configured MFA device - Generate temporary 12-hour session credentials - Export them to your current shell environment - Optionally save them for use in Jupyter notebooks\n\n\n\n\n\n\n\n\nAWS CLI (version 2.x recommended)\n# Check if AWS CLI is installed\naws --version\n\n# Install AWS CLI on macOS\nbrew install awscli\n\n# Install AWS CLI on Linux\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\nBash Shell (standard on macOS and Linux)\n# Check bash version\nbash --version\nPython (optional, for Jupyter notebook integration)\n# Check Python installation\npython --version\n\n\n\n\n\nIAM User Account with:\n\nProgrammatic access enabled\nMFA device configured\nAppropriate permissions for your tasks\n\nMFA Device Setup\n\nVirtual MFA device (Google Authenticator, Authy, etc.)\nHardware MFA token (YubiKey, RSA SecurID, etc.)\n\n\n\n\n\n\n\n\n\nCreate or edit ~/.aws/credentials:\n[smce-veda]\naws_access_key_id = YOUR_ACCESS_KEY_ID\naws_secret_access_key = YOUR_SECRET_ACCESS_KEY\nNOTE: These credentials are obtained within the AWS console when you register for an AWS account. You must get approval for this account.\n\n\n\nCreate or edit ~/.aws/config:\n[smce-veda]\nregion = us-west-2\n\n\n\n\nLog into AWS Console\nNavigate to IAM → Users → Your Username\nSelect “Security credentials” tab\nClick “Manage” next to MFA devices\nFollow the setup wizard for your MFA device type\n\nNOTE: I’ve found that having ONLY the MFA through phone app (e.g., Google Authenticator) is the only way to make this work. If you select Passkey and have an MFA through phone, they seem to conflict and the steps outlined will not work.\n\n\n\n# Replace YOUR_USERNAME with your actual IAM username\naws iam list-mfa-devices --user-name YOUR_USERNAME\n\n# Output should show:\n# \"SerialNumber\": \"arn:aws:iam::123456789012:mfa/YOUR_USERNAME\"\n\n\n\n\n\n\n\nCreate the file ~/aws_mfa.sh:\n#!/bin/bash\n\n# 🔒 AWS MFA Credential Generator\n# This script creates temporary (12-hour) credentials using your MFA device\n# Requires terminal access for secure input\n# License: GPL 2 or higher\n\n# Check for terminal access\nif [ ! -t 0 ]; then\n  echo \"❌ Error: This script requires terminal access for secure input\" &gt;&2\n  return\nfi\n\n# Prevent token conflicts\nif [ -n \"$AWS_SESSION_TOKEN\" ]; then\n  echo \"⚠️  Active session detected! \n   To generate new credentials, clear your current session:\n   unset AWS_SESSION_TOKEN AWS_SECRET_ACCESS_KEY AWS_ACCESS_KEY_ID\n   Then ensure you have valid profile credentials configured.\" &gt;&2\n  return\nfi\n\n# Identify current user\nidentity=$(aws sts get-caller-identity --output json)\nusername=$(echo -- \"$identity\" | sed -n 's!.*\"arn:aws:iam::.*:user/\\(.*\\)\".*!\\1!p')\n\nif [ -z \"$username\" ]; then\n  echo \"❌ Unable to identify user. Expected format:\n    arn:aws:iam::.....:user/YOUR_USERNAME\n  \nCurrent identity output:\n$identity\" &gt;&2\n  return\nfi\n\necho \"👤 Authenticated as: $username\" &gt;&2\n\n# Find MFA device\nmfa=$(aws iam list-mfa-devices --user-name \"$username\" --output json)\ndevice=$(echo -- \"$mfa\" | sed -n 's!.*\"SerialNumber\": \"\\(.*\\)\".*!\\1!p')\n\nif [ -z \"$device\" ]; then\n  echo \"❌ No MFA device found for user: $username\n  \nMFA device output:\n$mfa\" &gt;&2\n  return\nfi\n\necho \"📱 MFA device found: $device\" &gt;&2\n\n# Request MFA code\necho -n \"🔢 Enter your MFA code: \" &gt;&2\nread code\n\n# Generate temporary credentials\ntokens=$(aws sts get-session-token --serial-number \"$device\" --token-code $code --output json)\n\necho $tokens\n\n# Extract credentials\nsecret=$(echo -- \"$tokens\" | sed -n 's!.*\"SecretAccessKey\": \"\\(.*\\)\".*!\\1!p')\nsession=$(echo -- \"$tokens\" | sed -n 's!.*\"SessionToken\": \"\\(.*\\)\".*!\\1!p')\naccess=$(echo -- \"$tokens\" | sed -n 's!.*\"AccessKeyId\": \"\\(.*\\)\".*!\\1!p')\nexpire=$(echo -- \"$tokens\" | sed -n 's!.*\"Expiration\": \"\\(.*\\)\".*!\\1!p')\n\nif [ -z \"$secret\" -o -z \"$session\" -o -z \"$access\" ]; then\n  echo \"❌ Failed to generate temporary credentials\n  \nToken response:\n$tokens\" &gt;&2\n  return\nfi\n\n# Export credentials to environment\nexport AWS_SESSION_TOKEN=$session\nexport AWS_SECRET_ACCESS_KEY=$secret\nexport AWS_ACCESS_KEY_ID=$access\n\necho \"✅ Temporary credentials activated! Expires: $expire\" &gt;&2\n\n# Save credentials to .env file for Jupyter notebooks (optional)\n# This will save the new credentials to a .env file in the current directory where you called \"aws_env\"\npython ~/set_aws_creds.py\n\n\n\nchmod +x ~/aws_mfa.sh\n\n\n\nAdd to your ~/.zshrc (or ~/.bashrc):\n##################### CREATE AWS temporary credentials\n\n# 🎯 AWS Environment Switcher\n# Seamlessly switch between AWS accounts with MFA support\naws_env() {\n  # Clear any existing session\n  unset AWS_SESSION_TOKEN\n\n  # Load credentials for the specified profile\n  export AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id --profile $1)\n  export AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key --profile $1)\n  export AWS_DEFAULT_REGION=$(aws configure get region --profile $1)\n\n  # Profiles requiring MFA authentication\n  MFA_PROFILES=(\"veda-smce\" \"smce-veda\" \"aq\" \"uah-veda\")\n\n  # Check if MFA is required for this profile\n  if [[ \" ${MFA_PROFILES[@]} \" =~ \" ${1} \" ]]; then\n    echo \"🔐 MFA required for profile: $1\"\n    source ~/aws_mfa.sh\n  fi\n\n  echo \"✅ Successfully switched to $1 environment!\"\n}\nAlternatively, if you organize your shell configuration, you can add this to ~/.zshrc.d/functions or similar.\nReload your shell configuration:\nsource ~/.zshrc  # or source ~/.bashrc\n\n\n\nIf you use Jupyter notebooks, create ~/set_aws_creds.py:\n#!/usr/bin/env python\nimport os\n\n# Get credentials from environment\naccess_key = os.environ.get('AWS_ACCESS_KEY_ID')\nsecret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\nsession_token = os.environ.get('AWS_SESSION_TOKEN')\n\nif access_key and secret_key and session_token:\n    # Save to .env file for Jupyter notebooks\n    with open(os.path.expanduser('~/.env'), 'w') as f:\n        f.write(f'AWS_ACCESS_KEY_ID={access_key}\\n')\n        f.write(f'AWS_SECRET_ACCESS_KEY={secret_key}\\n')\n        f.write(f'AWS_SESSION_TOKEN={session_token}\\n')\n    print(\"✅ Credentials saved to ~/.env for Jupyter notebooks\")\nelse:\n    print(\"❌ No credentials found in environment\")\n\n\n\n\n\n\n\n\n\n# Switch to a profile with MFA support\naws_env smce-veda\n\n# For profiles without MFA requirement\naws_env other-profile\n\n# Or source the MFA script directly\nsource ~/aws_mfa.sh\nThe aws_env function will: 1. Load credentials for the specified profile 2. Check if MFA is required for that profile 3. If MFA is needed, automatically call the MFA script 4. The MFA script will detect your user and prompt for MFA code 5. Generate and export temporary credentials\n\n\n\n$ aws_env smce-veda\n🔐 MFA required for profile: smce-veda\n👤 Authenticated as: john.doe\n📱 MFA device found: arn:aws:iam::123456789012:mfa/john.doe\n🔢 Enter your MFA code: 123456\n✅ Temporary credentials activated! Expires: 2024-01-01T12:00:00Z\n✅ Successfully switched to smce-veda environment!\n\n\n\n\nOnce authenticated, you can use AWS CLI commands normally:\n# List S3 buckets\naws s3 ls s3://nasa-disasters/\n\n# Get current identity\naws sts get-caller-identity\n\n\n\nTo clear your current MFA session:\nunset AWS_SESSION_TOKEN AWS_SECRET_ACCESS_KEY AWS_ACCESS_KEY_ID\n\n\n\n\n\n\n\n\n\nSolution: Clear your current session:\nunset AWS_SESSION_TOKEN AWS_SECRET_ACCESS_KEY AWS_ACCESS_KEY_ID\n\n\n\nSolutions: - Verify your AWS credentials are configured correctly - Check that your credentials have permission to call sts:GetCallerIdentity - Ensure you’re using the correct AWS profile (if using multiple profiles)\n\n\n\nSolutions: - Verify MFA is enabled on your IAM user - Check that your user has permission to call iam:ListMFADevices - Ensure the MFA device is properly attached to your user\n\n\n\nSolutions: - Verify the MFA code is correct and hasn’t expired - Check that your user has permission to call sts:GetSessionToken - Ensure your system clock is synchronized (MFA codes are time-based)\n\n\n\nSolution: Make sure to use source or the function:\n# Correct - runs in current shell\nsource ~/aws_mfa.sh\n\n# Wrong - runs in subshell\n~/aws_mfa.sh\n\n\n\n\n\n\n# View current identity\naws sts get-caller-identity\n\n# Check environment variables\nenv | grep AWS\n\n\n\n# Replace USERNAME with your IAM username\naws iam list-mfa-devices --user-name USERNAME\n\n\n\n# Test basic permissions\naws iam get-user\naws sts get-session-token --serial-number YOUR_MFA_ARN --token-code 123456\n\n\n\n\n\n\n\n\n\nNever Share MFA Tokens\n\nMFA tokens are time-sensitive but should still be kept private\nDon’t log or store MFA tokens in scripts\n\nRegular Credential Rotation\n# Create new access key\naws iam create-access-key --user-name YOUR_USERNAME\n\n# Delete old access key\naws iam delete-access-key --access-key-id OLD_ACCESS_KEY_ID --user-name YOUR_USERNAME\nSecure Credential Storage\n# Set restrictive permissions on AWS files\nchmod 600 ~/.aws/credentials\nchmod 600 ~/.aws/config\nchmod 700 ~/.aws\nUse Least Privilege\n\nOnly grant permissions necessary for your tasks\nConsider using AWS IAM roles when possible\n\n\n\n\n\n\nMultiple AWS Profiles If you use multiple AWS accounts, configure profiles in ~/.aws/credentials:\n[smce-veda]\naws_access_key_id = KEY1\naws_secret_access_key = SECRET1\n\n[veda-smce]\naws_access_key_id = KEY2\naws_secret_access_key = SECRET2\n\n[aq]\naws_access_key_id = KEY3\naws_secret_access_key = SECRET3\nFUN FACT: You can have different credentials opened within each terminal. This can alleviate having to re-authenticate for different accounts.\nSwitch profiles using the aws_env function:\n# Switch to a profile with automatic MFA handling\naws_env smce-veda\n\n# Switch to another profile\naws_env aq\nAutomate Common Tasks Create helper functions in your shell configuration:\n# Quick S3 listing\ns3ls() {\n    aws s3 ls \"s3://$1\"\n}\nSession Management Check if your session is still valid:\naws sts get-caller-identity &&gt;/dev/null && echo \"✅ Session valid\" || echo \"❌ Session expired\"\n\n\n\n\n\n\n\n\n\nNever Commit Credentials Add to .gitignore:\n# AWS credentials\n.aws/credentials\n.aws/config\n.env\n*.pem\nEnvironment Variables\n\nTemporary credentials are stored in environment variables\nThey’re only available in the current shell session\nClosing the terminal clears the credentials\n\nSession Duration\n\nDefault session duration is 12 hours (43200 seconds)\nSessions automatically expire and cannot be renewed\nMust generate new credentials after expiration\n\n\n\n\n\n\nVirtual MFA Best Practices\n\nUse reputable authenticator apps\nEnable cloud backup for MFA seeds\nKeep backup codes in secure location\n\nHardware MFA Best Practices\n\nStore device in secure location\nConsider having a backup MFA device\nTest device regularly\n\n\n\n\n\n\nUse VPN for Sensitive Operations\n\nConsider using VPN when accessing AWS from public networks\nBe aware of IP-based IAM policies\n\nAudit Trail\n\nEnable CloudTrail for API call logging\nRegularly review access patterns\nMonitor for unusual activity\n\n\n\n\n\n\n\n\nAWS MFA Documentation\nAWS CLI Configuration\nSTS GetSessionToken API\nIAM Best Practices\nAWS Security Best Practices\n\n\nLast Updated: 2024 Version: 2.0",
    "crumbs": [
      "AWS",
      "AWS MFA Authentication Setup and Usage Guide"
    ]
  },
  {
    "objectID": "AWS/aws-mfa-setup.html#table-of-contents",
    "href": "AWS/aws-mfa-setup.html#table-of-contents",
    "title": "AWS MFA Authentication Setup and Usage Guide",
    "section": "",
    "text": "Overview\nPrerequisites\nInitial Setup\nScript Installation\nUsage\nTroubleshooting\nBest Practices\nSecurity Considerations",
    "crumbs": [
      "AWS",
      "AWS MFA Authentication Setup and Usage Guide"
    ]
  },
  {
    "objectID": "AWS/aws-mfa-setup.html#overview",
    "href": "AWS/aws-mfa-setup.html#overview",
    "title": "AWS MFA Authentication Setup and Usage Guide",
    "section": "",
    "text": "Multi-Factor Authentication (MFA) adds an extra layer of security to your AWS account by requiring two forms of identification: 1. Something you know (your password/credentials) 2. Something you have (a time-based token from your MFA device)\n\n\n\n\nEnhanced Security: Temporary credentials automatically expire, reducing the risk if they’re compromised\nCompliance: Many organizations require MFA for production AWS access\nBest Practice: AWS recommends using temporary credentials instead of long-lived access keys\nSession Management: Temporary credentials can be scoped with specific permissions\n\n\n\n\nThe ~/aws_mfa.sh script is a lightweight utility that you call with aws_env to: - Automatically detect your current AWS user - Find your configured MFA device - Generate temporary 12-hour session credentials - Export them to your current shell environment - Optionally save them for use in Jupyter notebooks",
    "crumbs": [
      "AWS",
      "AWS MFA Authentication Setup and Usage Guide"
    ]
  },
  {
    "objectID": "AWS/aws-mfa-setup.html#prerequisites",
    "href": "AWS/aws-mfa-setup.html#prerequisites",
    "title": "AWS MFA Authentication Setup and Usage Guide",
    "section": "",
    "text": "AWS CLI (version 2.x recommended)\n# Check if AWS CLI is installed\naws --version\n\n# Install AWS CLI on macOS\nbrew install awscli\n\n# Install AWS CLI on Linux\ncurl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\nBash Shell (standard on macOS and Linux)\n# Check bash version\nbash --version\nPython (optional, for Jupyter notebook integration)\n# Check Python installation\npython --version\n\n\n\n\n\nIAM User Account with:\n\nProgrammatic access enabled\nMFA device configured\nAppropriate permissions for your tasks\n\nMFA Device Setup\n\nVirtual MFA device (Google Authenticator, Authy, etc.)\nHardware MFA token (YubiKey, RSA SecurID, etc.)",
    "crumbs": [
      "AWS",
      "AWS MFA Authentication Setup and Usage Guide"
    ]
  },
  {
    "objectID": "AWS/aws-mfa-setup.html#initial-setup",
    "href": "AWS/aws-mfa-setup.html#initial-setup",
    "title": "AWS MFA Authentication Setup and Usage Guide",
    "section": "",
    "text": "Create or edit ~/.aws/credentials:\n[smce-veda]\naws_access_key_id = YOUR_ACCESS_KEY_ID\naws_secret_access_key = YOUR_SECRET_ACCESS_KEY\nNOTE: These credentials are obtained within the AWS console when you register for an AWS account. You must get approval for this account.\n\n\n\nCreate or edit ~/.aws/config:\n[smce-veda]\nregion = us-west-2\n\n\n\n\nLog into AWS Console\nNavigate to IAM → Users → Your Username\nSelect “Security credentials” tab\nClick “Manage” next to MFA devices\nFollow the setup wizard for your MFA device type\n\nNOTE: I’ve found that having ONLY the MFA through phone app (e.g., Google Authenticator) is the only way to make this work. If you select Passkey and have an MFA through phone, they seem to conflict and the steps outlined will not work.\n\n\n\n# Replace YOUR_USERNAME with your actual IAM username\naws iam list-mfa-devices --user-name YOUR_USERNAME\n\n# Output should show:\n# \"SerialNumber\": \"arn:aws:iam::123456789012:mfa/YOUR_USERNAME\"",
    "crumbs": [
      "AWS",
      "AWS MFA Authentication Setup and Usage Guide"
    ]
  },
  {
    "objectID": "AWS/aws-mfa-setup.html#script-installation",
    "href": "AWS/aws-mfa-setup.html#script-installation",
    "title": "AWS MFA Authentication Setup and Usage Guide",
    "section": "",
    "text": "Create the file ~/aws_mfa.sh:\n#!/bin/bash\n\n# 🔒 AWS MFA Credential Generator\n# This script creates temporary (12-hour) credentials using your MFA device\n# Requires terminal access for secure input\n# License: GPL 2 or higher\n\n# Check for terminal access\nif [ ! -t 0 ]; then\n  echo \"❌ Error: This script requires terminal access for secure input\" &gt;&2\n  return\nfi\n\n# Prevent token conflicts\nif [ -n \"$AWS_SESSION_TOKEN\" ]; then\n  echo \"⚠️  Active session detected! \n   To generate new credentials, clear your current session:\n   unset AWS_SESSION_TOKEN AWS_SECRET_ACCESS_KEY AWS_ACCESS_KEY_ID\n   Then ensure you have valid profile credentials configured.\" &gt;&2\n  return\nfi\n\n# Identify current user\nidentity=$(aws sts get-caller-identity --output json)\nusername=$(echo -- \"$identity\" | sed -n 's!.*\"arn:aws:iam::.*:user/\\(.*\\)\".*!\\1!p')\n\nif [ -z \"$username\" ]; then\n  echo \"❌ Unable to identify user. Expected format:\n    arn:aws:iam::.....:user/YOUR_USERNAME\n  \nCurrent identity output:\n$identity\" &gt;&2\n  return\nfi\n\necho \"👤 Authenticated as: $username\" &gt;&2\n\n# Find MFA device\nmfa=$(aws iam list-mfa-devices --user-name \"$username\" --output json)\ndevice=$(echo -- \"$mfa\" | sed -n 's!.*\"SerialNumber\": \"\\(.*\\)\".*!\\1!p')\n\nif [ -z \"$device\" ]; then\n  echo \"❌ No MFA device found for user: $username\n  \nMFA device output:\n$mfa\" &gt;&2\n  return\nfi\n\necho \"📱 MFA device found: $device\" &gt;&2\n\n# Request MFA code\necho -n \"🔢 Enter your MFA code: \" &gt;&2\nread code\n\n# Generate temporary credentials\ntokens=$(aws sts get-session-token --serial-number \"$device\" --token-code $code --output json)\n\necho $tokens\n\n# Extract credentials\nsecret=$(echo -- \"$tokens\" | sed -n 's!.*\"SecretAccessKey\": \"\\(.*\\)\".*!\\1!p')\nsession=$(echo -- \"$tokens\" | sed -n 's!.*\"SessionToken\": \"\\(.*\\)\".*!\\1!p')\naccess=$(echo -- \"$tokens\" | sed -n 's!.*\"AccessKeyId\": \"\\(.*\\)\".*!\\1!p')\nexpire=$(echo -- \"$tokens\" | sed -n 's!.*\"Expiration\": \"\\(.*\\)\".*!\\1!p')\n\nif [ -z \"$secret\" -o -z \"$session\" -o -z \"$access\" ]; then\n  echo \"❌ Failed to generate temporary credentials\n  \nToken response:\n$tokens\" &gt;&2\n  return\nfi\n\n# Export credentials to environment\nexport AWS_SESSION_TOKEN=$session\nexport AWS_SECRET_ACCESS_KEY=$secret\nexport AWS_ACCESS_KEY_ID=$access\n\necho \"✅ Temporary credentials activated! Expires: $expire\" &gt;&2\n\n# Save credentials to .env file for Jupyter notebooks (optional)\n# This will save the new credentials to a .env file in the current directory where you called \"aws_env\"\npython ~/set_aws_creds.py\n\n\n\nchmod +x ~/aws_mfa.sh\n\n\n\nAdd to your ~/.zshrc (or ~/.bashrc):\n##################### CREATE AWS temporary credentials\n\n# 🎯 AWS Environment Switcher\n# Seamlessly switch between AWS accounts with MFA support\naws_env() {\n  # Clear any existing session\n  unset AWS_SESSION_TOKEN\n\n  # Load credentials for the specified profile\n  export AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id --profile $1)\n  export AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key --profile $1)\n  export AWS_DEFAULT_REGION=$(aws configure get region --profile $1)\n\n  # Profiles requiring MFA authentication\n  MFA_PROFILES=(\"veda-smce\" \"smce-veda\" \"aq\" \"uah-veda\")\n\n  # Check if MFA is required for this profile\n  if [[ \" ${MFA_PROFILES[@]} \" =~ \" ${1} \" ]]; then\n    echo \"🔐 MFA required for profile: $1\"\n    source ~/aws_mfa.sh\n  fi\n\n  echo \"✅ Successfully switched to $1 environment!\"\n}\nAlternatively, if you organize your shell configuration, you can add this to ~/.zshrc.d/functions or similar.\nReload your shell configuration:\nsource ~/.zshrc  # or source ~/.bashrc\n\n\n\nIf you use Jupyter notebooks, create ~/set_aws_creds.py:\n#!/usr/bin/env python\nimport os\n\n# Get credentials from environment\naccess_key = os.environ.get('AWS_ACCESS_KEY_ID')\nsecret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\nsession_token = os.environ.get('AWS_SESSION_TOKEN')\n\nif access_key and secret_key and session_token:\n    # Save to .env file for Jupyter notebooks\n    with open(os.path.expanduser('~/.env'), 'w') as f:\n        f.write(f'AWS_ACCESS_KEY_ID={access_key}\\n')\n        f.write(f'AWS_SECRET_ACCESS_KEY={secret_key}\\n')\n        f.write(f'AWS_SESSION_TOKEN={session_token}\\n')\n    print(\"✅ Credentials saved to ~/.env for Jupyter notebooks\")\nelse:\n    print(\"❌ No credentials found in environment\")",
    "crumbs": [
      "AWS",
      "AWS MFA Authentication Setup and Usage Guide"
    ]
  },
  {
    "objectID": "AWS/aws-mfa-setup.html#usage",
    "href": "AWS/aws-mfa-setup.html#usage",
    "title": "AWS MFA Authentication Setup and Usage Guide",
    "section": "",
    "text": "# Switch to a profile with MFA support\naws_env smce-veda\n\n# For profiles without MFA requirement\naws_env other-profile\n\n# Or source the MFA script directly\nsource ~/aws_mfa.sh\nThe aws_env function will: 1. Load credentials for the specified profile 2. Check if MFA is required for that profile 3. If MFA is needed, automatically call the MFA script 4. The MFA script will detect your user and prompt for MFA code 5. Generate and export temporary credentials\n\n\n\n$ aws_env smce-veda\n🔐 MFA required for profile: smce-veda\n👤 Authenticated as: john.doe\n📱 MFA device found: arn:aws:iam::123456789012:mfa/john.doe\n🔢 Enter your MFA code: 123456\n✅ Temporary credentials activated! Expires: 2024-01-01T12:00:00Z\n✅ Successfully switched to smce-veda environment!\n\n\n\n\nOnce authenticated, you can use AWS CLI commands normally:\n# List S3 buckets\naws s3 ls s3://nasa-disasters/\n\n# Get current identity\naws sts get-caller-identity\n\n\n\nTo clear your current MFA session:\nunset AWS_SESSION_TOKEN AWS_SECRET_ACCESS_KEY AWS_ACCESS_KEY_ID",
    "crumbs": [
      "AWS",
      "AWS MFA Authentication Setup and Usage Guide"
    ]
  },
  {
    "objectID": "AWS/aws-mfa-setup.html#troubleshooting",
    "href": "AWS/aws-mfa-setup.html#troubleshooting",
    "title": "AWS MFA Authentication Setup and Usage Guide",
    "section": "",
    "text": "Solution: Clear your current session:\nunset AWS_SESSION_TOKEN AWS_SECRET_ACCESS_KEY AWS_ACCESS_KEY_ID\n\n\n\nSolutions: - Verify your AWS credentials are configured correctly - Check that your credentials have permission to call sts:GetCallerIdentity - Ensure you’re using the correct AWS profile (if using multiple profiles)\n\n\n\nSolutions: - Verify MFA is enabled on your IAM user - Check that your user has permission to call iam:ListMFADevices - Ensure the MFA device is properly attached to your user\n\n\n\nSolutions: - Verify the MFA code is correct and hasn’t expired - Check that your user has permission to call sts:GetSessionToken - Ensure your system clock is synchronized (MFA codes are time-based)\n\n\n\nSolution: Make sure to use source or the function:\n# Correct - runs in current shell\nsource ~/aws_mfa.sh\n\n# Wrong - runs in subshell\n~/aws_mfa.sh\n\n\n\n\n\n\n# View current identity\naws sts get-caller-identity\n\n# Check environment variables\nenv | grep AWS\n\n\n\n# Replace USERNAME with your IAM username\naws iam list-mfa-devices --user-name USERNAME\n\n\n\n# Test basic permissions\naws iam get-user\naws sts get-session-token --serial-number YOUR_MFA_ARN --token-code 123456",
    "crumbs": [
      "AWS",
      "AWS MFA Authentication Setup and Usage Guide"
    ]
  },
  {
    "objectID": "AWS/aws-mfa-setup.html#best-practices",
    "href": "AWS/aws-mfa-setup.html#best-practices",
    "title": "AWS MFA Authentication Setup and Usage Guide",
    "section": "",
    "text": "Never Share MFA Tokens\n\nMFA tokens are time-sensitive but should still be kept private\nDon’t log or store MFA tokens in scripts\n\nRegular Credential Rotation\n# Create new access key\naws iam create-access-key --user-name YOUR_USERNAME\n\n# Delete old access key\naws iam delete-access-key --access-key-id OLD_ACCESS_KEY_ID --user-name YOUR_USERNAME\nSecure Credential Storage\n# Set restrictive permissions on AWS files\nchmod 600 ~/.aws/credentials\nchmod 600 ~/.aws/config\nchmod 700 ~/.aws\nUse Least Privilege\n\nOnly grant permissions necessary for your tasks\nConsider using AWS IAM roles when possible\n\n\n\n\n\n\nMultiple AWS Profiles If you use multiple AWS accounts, configure profiles in ~/.aws/credentials:\n[smce-veda]\naws_access_key_id = KEY1\naws_secret_access_key = SECRET1\n\n[veda-smce]\naws_access_key_id = KEY2\naws_secret_access_key = SECRET2\n\n[aq]\naws_access_key_id = KEY3\naws_secret_access_key = SECRET3\nFUN FACT: You can have different credentials opened within each terminal. This can alleviate having to re-authenticate for different accounts.\nSwitch profiles using the aws_env function:\n# Switch to a profile with automatic MFA handling\naws_env smce-veda\n\n# Switch to another profile\naws_env aq\nAutomate Common Tasks Create helper functions in your shell configuration:\n# Quick S3 listing\ns3ls() {\n    aws s3 ls \"s3://$1\"\n}\nSession Management Check if your session is still valid:\naws sts get-caller-identity &&gt;/dev/null && echo \"✅ Session valid\" || echo \"❌ Session expired\"",
    "crumbs": [
      "AWS",
      "AWS MFA Authentication Setup and Usage Guide"
    ]
  },
  {
    "objectID": "AWS/aws-mfa-setup.html#security-considerations",
    "href": "AWS/aws-mfa-setup.html#security-considerations",
    "title": "AWS MFA Authentication Setup and Usage Guide",
    "section": "",
    "text": "Never Commit Credentials Add to .gitignore:\n# AWS credentials\n.aws/credentials\n.aws/config\n.env\n*.pem\nEnvironment Variables\n\nTemporary credentials are stored in environment variables\nThey’re only available in the current shell session\nClosing the terminal clears the credentials\n\nSession Duration\n\nDefault session duration is 12 hours (43200 seconds)\nSessions automatically expire and cannot be renewed\nMust generate new credentials after expiration\n\n\n\n\n\n\nVirtual MFA Best Practices\n\nUse reputable authenticator apps\nEnable cloud backup for MFA seeds\nKeep backup codes in secure location\n\nHardware MFA Best Practices\n\nStore device in secure location\nConsider having a backup MFA device\nTest device regularly\n\n\n\n\n\n\nUse VPN for Sensitive Operations\n\nConsider using VPN when accessing AWS from public networks\nBe aware of IP-based IAM policies\n\nAudit Trail\n\nEnable CloudTrail for API call logging\nRegularly review access patterns\nMonitor for unusual activity",
    "crumbs": [
      "AWS",
      "AWS MFA Authentication Setup and Usage Guide"
    ]
  },
  {
    "objectID": "AWS/aws-mfa-setup.html#additional-resources",
    "href": "AWS/aws-mfa-setup.html#additional-resources",
    "title": "AWS MFA Authentication Setup and Usage Guide",
    "section": "",
    "text": "AWS MFA Documentation\nAWS CLI Configuration\nSTS GetSessionToken API\nIAM Best Practices\nAWS Security Best Practices\n\n\nLast Updated: 2024 Version: 2.0",
    "crumbs": [
      "AWS",
      "AWS MFA Authentication Setup and Usage Guide"
    ]
  },
  {
    "objectID": "aws.html",
    "href": "aws.html",
    "title": "AWS Authentication and S3 Operations Guides",
    "section": "",
    "text": "Welcome to the homepage for AWS access configuration and data management resources.\nThis collection of guides provides step-by-step instructions for securely managing AWS authentication (using MFA or SSO) and for operating with Amazon S3 using the AWS Command Line Interface (CLI).\nEach guide is designed to help users configure secure AWS environments, automate access through scripts or profiles, and perform efficient data operations across AWS services.",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "aws.html#aws-access-and-security-configuration-guides",
    "href": "aws.html#aws-access-and-security-configuration-guides",
    "title": "AWS Authentication and S3 Operations Guides",
    "section": "AWS Access and Security Configuration Guides",
    "text": "AWS Access and Security Configuration Guides\nTutorial-style guides demonstrating secure authentication methods and credential management for AWS users.\n\nAWS Multi-Factor Authentication (MFA) Setup and Usage\n\nGuide\nLearn how to enable and use AWS MFA for secure, temporary session credentials.\nThis guide explains how to:\nConfigure MFA devices (virtual or hardware)\n\nCreate and use the aws_mfa.sh script for automatic token-based authentication\n\nIntegrate temporary credentials into your shell or Jupyter notebooks\n\nImplement best practices for session management and credential security\n\n\nAWS Single Sign-On (SSO) Configuration with AWS Identity Center\n\nGuide\nStep-by-step instructions for setting up and using AWS Identity Center (formerly AWS SSO) for passwordless, secure access.\nThis guide includes:\nAWS CLI SSO configuration walkthrough\n\nUsing browser-based authentication\n\nManaging multiple AWS accounts and IAM roles\n\nDaily login and logout workflow for SSO sessions\n\nBest practices for avoiding plain-text credentials and ensuring secure session management",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "aws.html#aws-data-operations-and-management",
    "href": "aws.html#aws-data-operations-and-management",
    "title": "AWS Authentication and S3 Operations Guides",
    "section": "AWS Data Operations and Management",
    "text": "AWS Data Operations and Management\nPractical reference for working with Amazon S3 using the AWS CLI, including data upload, access control, and cost optimization.\n\nAWS S3 Commands and Operations Guide\n\nGuide\nA complete reference of commonly used AWS S3 CLI commands for bucket and object management.\nKey sections include:\nCLI installation and configuration\n\nFile operations (cp, mv, rm, sync)\n\nRecursive uploads and downloads\n\nAccess control, encryption, and bucket policies\n\nPerformance and cost management techniques\n\nSecurity best practices and troubleshooting methods",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "aws.html#best-practices-summary",
    "href": "aws.html#best-practices-summary",
    "title": "AWS Authentication and S3 Operations Guides",
    "section": "Best Practices Summary",
    "text": "Best Practices Summary\nWhen working with AWS authentication and S3: - Use temporary credentials through MFA or SSO — avoid storing permanent keys.\n- Enable encryption and versioning for critical S3 data.\n- Regularly rotate access keys and restrict IAM permissions by least privilege.\n- Test access with aws sts get-caller-identity and validate your session before running commands.\n- Review CloudTrail and CloudWatch logs to monitor activity.",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "aws.html#community-contributed-scripts-and-tools",
    "href": "aws.html#community-contributed-scripts-and-tools",
    "title": "AWS Authentication and S3 Operations Guides",
    "section": "Community-Contributed Scripts and Tools",
    "text": "Community-Contributed Scripts and Tools\nCommunity utilities and automation scripts for enhanced AWS usability may be added here in the future.\nThese will include: - MFA and SSO integration helpers\n- Automated credential refreshers\n- Advanced S3 sync and cost-analysis scripts",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "aws.html#contact",
    "href": "aws.html#contact",
    "title": "AWS Authentication and S3 Operations Guides",
    "section": "Contact",
    "text": "Contact\nFor questions, feedback, or contribution inquiries, please contact your AWS system administrator or submit requests through your organization’s AWS support channel.\nAlways follow internal security policies and AWS best practices when managing credentials or data access.",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "index2.html",
    "href": "index2.html",
    "title": "NASA Disasters: Documentation",
    "section": "",
    "text": "The NASA Disasters Program advances science and builds tools to help communities make informed decisions for disaster planning. We develop free and accessible resources that use Earth observations to reveal how natural hazards interact with vulnerability, exposure, and coping capacity in a changing climate.\nOn this site, you can find the technical documentation for the services used to visualize data, how to connect to these services, how to load datasets, how the datasets were transformed into cloud-optimized formats that enable efficient cloud data access, and how to visualize datasets.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index2.html#welcome",
    "href": "index2.html#welcome",
    "title": "NASA Disasters: Documentation",
    "section": "",
    "text": "The NASA Disasters Program advances science and builds tools to help communities make informed decisions for disaster planning. We develop free and accessible resources that use Earth observations to reveal how natural hazards interact with vulnerability, exposure, and coping capacity in a changing climate.\nOn this site, you can find the technical documentation for the services used to visualize data, how to connect to these services, how to load datasets, how the datasets were transformed into cloud-optimized formats that enable efficient cloud data access, and how to visualize datasets.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index2.html#contents",
    "href": "index2.html#contents",
    "title": "NASA Disasters: Documentation",
    "section": "Contents",
    "text": "Contents\n\nA guide to access AWS resources, with guidelines to utilize AWS commands and operations.\nDirections on how to setup GitHub, access a repository, use commands, and understand the GitHub workflow in Disasters.\nHow to use JupyterHub to work collaboratively on notebooks within a repositroy and the Disasters framework.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index2.html#contact",
    "href": "index2.html#contact",
    "title": "NASA Disasters: Documentation",
    "section": "Contact",
    "text": "Contact\nFor technical help or general questions, please contact the support team using the feedback form.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html",
    "href": "git-github-comprehensive-guide.html",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "Git is a distributed version control system that tracks changes in your code over time. GitHub is a cloud-based hosting service that lets you manage Git repositories with additional collaboration features. This guide will walk you through everything you need to know to get started with Git and GitHub on macOS.\n\n\n\nVersion Control: Track every change made to your code\nCollaboration: Work with others without conflicts\nBackup: Your code is safely stored in the cloud\nDocumentation: Built-in wiki and issue tracking\nPortfolio: Showcase your work to potential employers",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#table-of-contents",
    "href": "git-github-comprehensive-guide.html#table-of-contents",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "Introduction\nPrerequisites & System Setup\nGit Installation\nGitHub Account Setup\nGitHub CLI Installation & Authentication\nSetting Up Your First Repository\nEssential Git Commands\nGitHub CLI Essentials\nCommon Workflows\nBest Practices\nTroubleshooting\nQuick Reference\nResources & Links",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#introduction",
    "href": "git-github-comprehensive-guide.html#introduction",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "Git is a distributed version control system that tracks changes in your code over time. GitHub is a cloud-based hosting service that lets you manage Git repositories with additional collaboration features. This guide will walk you through everything you need to know to get started with Git and GitHub on macOS.\n\n\n\nVersion Control: Track every change made to your code\nCollaboration: Work with others without conflicts\nBackup: Your code is safely stored in the cloud\nDocumentation: Built-in wiki and issue tracking\nPortfolio: Showcase your work to potential employers",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#prerequisites-system-setup",
    "href": "git-github-comprehensive-guide.html#prerequisites-system-setup",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "macOS 10.15 (Catalina) or later\nAdministrator access to install software\nInternet connection\nTerminal application (built into macOS)\n\n\n\n\n\nText Editor: VS Code, Sublime Text, or vim\nTerminal: iTerm2 or built-in Terminal app\nGit GUI (optional): SourceTree, GitHub Desktop, or GitKraken",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#git-installation",
    "href": "git-github-comprehensive-guide.html#git-installation",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "# Install Homebrew if not already installed\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install Git\nbrew install git\n\n# Verify installation\ngit --version\n\n\n\n# This will prompt to install Xcode Command Line Tools\ngit --version\n\n# Follow the prompts to complete installation\n\n\n\n\nVisit https://git-scm.com/download/mac\nDownload the installer\nRun the installer package\nVerify: git --version\n\n\n\n\n# Set your name (visible in commits)\ngit config --global user.name \"Your Name\"\n\n# Set your email (should match GitHub account)\ngit config --global user.email \"your.email@example.com\"\n\n# Set default branch name to 'main'\ngit config --global init.defaultBranch main\n\n# Set default editor (optional)\ngit config --global core.editor \"code --wait\"  # For VS Code\n# git config --global core.editor \"vim\"        # For vim\n# git config --global core.editor \"nano\"       # For nano\n\n# Enable color output\ngit config --global color.ui auto\n\n# View all settings\ngit config --list",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#github-account-setup",
    "href": "git-github-comprehensive-guide.html#github-account-setup",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "Visit https://github.com\nClick “Sign up” in the top right\nEnter your details:\n\nUsername: Choose wisely - this is permanent and public\nEmail: Use a professional email address\nPassword: Use a strong, unique password\n\nVerify your email address\nComplete the profile setup\n\n\n\n\n\nEnable Two-Factor Authentication (2FA):\n\nGo to Settings → Security\nClick “Enable two-factor authentication”\nUse an authenticator app (Google Authenticator, Authy)\nSave backup codes securely\n\nAdd SSH Key (recommended for secure authentication):\n\n# Generate SSH key\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Press Enter for default location\n# Set a passphrase (recommended)\n\n# Start SSH agent\neval \"$(ssh-agent -s)\"\n\n# Add SSH key to agent\nssh-add ~/.ssh/id_ed25519\n\n# Copy public key to clipboard\npbcopy &lt; ~/.ssh/id_ed25519.pub\n\nAdd SSH Key to GitHub:\n\nGo to Settings → SSH and GPG keys\nClick “New SSH key”\nPaste your key and give it a descriptive title\nClick “Add SSH key”\n\nTest SSH connection:\n\nssh -T git@github.com\n# You should see: \"Hi username! You've successfully authenticated...\"\n\n\n\n\nGo to Settings → Developer settings → Personal access tokens → Tokens (classic)\nClick “Generate new token”\nSet expiration and select scopes (at minimum: repo, workflow)\nCopy the token immediately (you won’t see it again)\nUse this token as your password when prompted by Git",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#github-cli-installation-authentication",
    "href": "git-github-comprehensive-guide.html#github-cli-installation-authentication",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "# Install via Homebrew\nbrew install gh\n\n# Verify installation\ngh --version\n\n\n\n# Start authentication process\ngh auth login\n\n# Follow the prompts:\n# 1. Choose GitHub.com\n# 2. Choose HTTPS or SSH (SSH recommended if you've set it up)\n# 3. Authenticate via web browser or paste authentication token\n# 4. Choose default git protocol (ssh recommended)\n\n# Verify authentication\ngh auth status\n\n\n\n# Set default editor\ngh config set editor \"code --wait\"  # For VS Code\n\n# Set default browser\ngh config set browser safari\n\n# View current configuration\ngh config list",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#setting-up-your-first-repository",
    "href": "git-github-comprehensive-guide.html#setting-up-your-first-repository",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "# Using HTTPS\ngit clone https://github.com/username/repository.git\n\n# Using SSH (recommended if configured)\ngit clone git@github.com:username/repository.git\n\n# Using GitHub CLI\ngh repo clone username/repository\n\n# Clone into specific directory\ngit clone git@github.com:username/repository.git my-project\n\n\n\n\n\n\nClick the “+” icon → “New repository”\nEnter repository name\nAdd description (optional)\nChoose public or private\nInitialize with README (recommended)\nAdd .gitignore (select template)\nChoose a license\nClick “Create repository”\n\n\n\n\n# Create a new repository on GitHub\ngh repo create my-project --public --clone\n\n# With more options\ngh repo create my-project \\\n  --public \\\n  --description \"My awesome project\" \\\n  --clone \\\n  --add-readme \\\n  --license mit \\\n  --gitignore Python\n\n\n\n\n# Navigate to your project\ncd my-existing-project\n\n# Initialize git repository\ngit init\n\n# Add all files\ngit add .\n\n# Create initial commit\ngit commit -m \"Initial commit\"\n\n# Create repository on GitHub\ngh repo create my-project --source=. --public --push\n\n# Or manually add remote and push\ngit remote add origin git@github.com:username/my-project.git\ngit branch -M main\ngit push -u origin main",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#essential-git-commands",
    "href": "git-github-comprehensive-guide.html#essential-git-commands",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "# Check Git version\ngit --version\n\n# Get help\ngit help &lt;command&gt;\ngit &lt;command&gt; --help\n\n# Initialize repository\ngit init\n\n# Clone repository\ngit clone &lt;url&gt;\n\n# Check status\ngit status\n\n# View commit history\ngit log\ngit log --oneline\ngit log --graph --oneline --all\n\n\n\n# Add files to staging area\ngit add &lt;file&gt;\ngit add .                    # Add all files\ngit add *.js                 # Add all JavaScript files\ngit add -p                   # Interactive staging\n\n# Remove files from staging\ngit reset HEAD &lt;file&gt;\ngit restore --staged &lt;file&gt;  # Git 2.23+\n\n# Commit changes\ngit commit -m \"Commit message\"\ngit commit -am \"Message\"     # Add and commit (tracked files only)\ngit commit --amend           # Amend last commit\n\n# View differences\ngit diff                     # Unstaged changes\ngit diff --staged           # Staged changes\ngit diff HEAD~1             # Changes since last commit\n\n\n\n# List branches\ngit branch                   # Local branches\ngit branch -r               # Remote branches\ngit branch -a               # All branches\n\n# Create branch\ngit branch &lt;branch-name&gt;\ngit checkout -b &lt;branch-name&gt;  # Create and switch\ngit switch -c &lt;branch-name&gt;    # Git 2.23+ (create and switch)\n\n# Switch branches\ngit checkout &lt;branch-name&gt;\ngit switch &lt;branch-name&gt;       # Git 2.23+\n\n# Merge branch\ngit merge &lt;branch-name&gt;\n\n# Delete branch\ngit branch -d &lt;branch-name&gt;    # Safe delete\ngit branch -D &lt;branch-name&gt;    # Force delete\n\n# Rename branch\ngit branch -m &lt;old-name&gt; &lt;new-name&gt;\n\n\n\n# View remotes\ngit remote -v\n\n# Add remote\ngit remote add &lt;name&gt; &lt;url&gt;\ngit remote add origin git@github.com:username/repo.git\n\n# Remove remote\ngit remote remove &lt;name&gt;\n\n# Rename remote\ngit remote rename &lt;old&gt; &lt;new&gt;\n\n# Fetch changes\ngit fetch\ngit fetch origin\n\n# Pull changes\ngit pull\ngit pull origin main\n\n# Push changes\ngit push\ngit push origin main\ngit push -u origin main      # Set upstream\ngit push --force             # Force push (use with caution!)\n\n\n\n# Save changes temporarily\ngit stash\ngit stash save \"Work in progress\"\n\n# List stashes\ngit stash list\n\n# Apply stash\ngit stash apply              # Apply most recent\ngit stash apply stash@{0}   # Apply specific stash\n\n# Apply and remove stash\ngit stash pop\n\n# Remove stash\ngit stash drop stash@{0}\n\n# Clear all stashes\ngit stash clear\n\n\n\n# Discard changes in working directory\ngit checkout -- &lt;file&gt;\ngit restore &lt;file&gt;           # Git 2.23+\n\n# Unstage files\ngit reset HEAD &lt;file&gt;\ngit restore --staged &lt;file&gt;  # Git 2.23+\n\n# Reset to previous commit (keeping changes)\ngit reset --soft HEAD~1\n\n# Reset to previous commit (discard changes)\ngit reset --hard HEAD~1\n\n# Revert a commit (creates new commit)\ngit revert &lt;commit-hash&gt;\n\n\n\n# List tags\ngit tag\n\n# Create tag\ngit tag v1.0.0\ngit tag -a v1.0.0 -m \"Version 1.0.0\"  # Annotated tag\n\n# Push tags\ngit push origin v1.0.0\ngit push origin --tags       # Push all tags\n\n# Delete tag\ngit tag -d v1.0.0           # Local\ngit push origin :v1.0.0     # Remote",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#github-cli-essentials",
    "href": "git-github-comprehensive-guide.html#github-cli-essentials",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "# Set default repository\ngh repo set-default\n# Select from list or specify:\ngh repo set-default owner/repo\n\n# View repository\ngh repo view\ngh repo view owner/repo\n\n# Fork repository\ngh repo fork owner/repo\n\n# Create repository\ngh repo create my-repo --public --clone\n\n# Delete repository (use with caution!)\ngh repo delete owner/repo\n\n# Clone repository\ngh repo clone owner/repo\n\n# List repositories\ngh repo list\ngh repo list owner\n\n\n\n# Create pull request\ngh pr create\ngh pr create --title \"Feature X\" --body \"Description\"\ngh pr create --fill  # Use commit messages for title/body\ngh pr create --draft # Create as draft\ngh pr create --assignee @me --label bug,enhancement\n\n# List pull requests\ngh pr list\ngh pr list --state all\ngh pr list --author @me\n\n# View pull request\ngh pr view\ngh pr view 123\n\n# Checkout pull request\ngh pr checkout 123\n\n# Merge pull request\ngh pr merge 123\ngh pr merge 123 --merge    # Create merge commit\ngh pr merge 123 --rebase   # Rebase and merge\ngh pr merge 123 --squash   # Squash and merge\n\n# Close pull request\ngh pr close 123\n\n# Review pull request\ngh pr review 123 --approve\ngh pr review 123 --request-changes\ngh pr review 123 --comment\n\n# Check pull request status\ngh pr status\ngh pr checks 123\n\n\n\n# Create issue\ngh issue create\ngh issue create --title \"Bug report\" --body \"Description\"\n\n# List issues\ngh issue list\ngh issue list --assignee @me\ngh issue list --label bug\n\n# View issue\ngh issue view 123\n\n# Close issue\ngh issue close 123\n\n# Reopen issue\ngh issue reopen 123\n\n# Comment on issue\ngh issue comment 123 --body \"This is fixed\"\n\n\n\n# List workflows\ngh workflow list\n\n# View workflow runs\ngh run list\ngh run view\n\n# Watch workflow run\ngh run watch\n\n# Download artifacts\ngh run download\n\n# Trigger workflow\ngh workflow run &lt;workflow-name&gt;\n\n\n\n# Create gist\ngh gist create file.txt\ngh gist create --public file.txt\n\n# List gists\ngh gist list\n\n# View gist\ngh gist view &lt;id&gt;\n\n# Edit gist\ngh gist edit &lt;id&gt;",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#common-workflows",
    "href": "git-github-comprehensive-guide.html#common-workflows",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "# 1. Start your day - sync with remote\ngit pull origin main\n\n# 2. Create feature branch\ngit checkout -b feature/new-feature\n\n# 3. Make changes and commit\ngit add .\ngit commit -m \"Add new feature\"\n\n# 4. Push to remote\ngit push -u origin feature/new-feature\n\n# 5. Create pull request\ngh pr create --fill\n\n# 6. After PR is merged, clean up\ngit checkout main\ngit pull origin main\ngit branch -d feature/new-feature\n\n\n\n# 1. Pull latest changes\ngit pull origin main\n\n# 2. If conflicts occur, Git will notify you\n# 3. Open conflicted files and resolve manually\n# Look for conflict markers:\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# Your changes\n# =======\n# Their changes\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; branch-name\n\n# 4. After resolving, add the files\ngit add &lt;resolved-files&gt;\n\n# 5. Complete the merge\ngit commit -m \"Resolve merge conflicts\"\n\n# 6. Push changes\ngit push origin &lt;branch&gt;\n\n\n\n# 1. Add upstream remote (one time)\ngit remote add upstream https://github.com/original-owner/repo.git\n\n# 2. Fetch upstream changes\ngit fetch upstream\n\n# 3. Checkout main branch\ngit checkout main\n\n# 4. Merge upstream changes\ngit merge upstream/main\n\n# 5. Push to your fork\ngit push origin main\n\n# Using GitHub CLI\ngh repo sync owner/repo -b main\n\n\n\n# Interactive rebase for last 3 commits\ngit rebase -i HEAD~3\n\n# In the editor:\n# Change 'pick' to 'squash' for commits to combine\n# Save and close\n\n# Force push (if already pushed)\ngit push --force-with-lease origin &lt;branch&gt;\n\n\n\n# Apply specific commit to current branch\ngit cherry-pick &lt;commit-hash&gt;\n\n# Cherry-pick multiple commits\ngit cherry-pick &lt;hash1&gt; &lt;hash2&gt; &lt;hash3&gt;\n\n# Cherry-pick range\ngit cherry-pick &lt;oldest-hash&gt;^..&lt;newest-hash&gt;",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#best-practices",
    "href": "git-github-comprehensive-guide.html#best-practices",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "The Seven Rules of Great Commit Messages:\n\nSeparate subject from body with blank line\nLimit subject line to 50 characters\nCapitalize the subject line\nDon’t end subject line with period\nUse imperative mood (“Add feature” not “Added feature”)\nWrap body at 72 characters\nExplain what and why, not how\n\nExample:\nAdd user authentication feature\n\nImplement OAuth 2.0 authentication using GitHub as provider.\nThis allows users to sign in with their GitHub credentials\ninstead of creating separate accounts.\n\nResolves: #123\nSee also: #456, #789\n\n\n\nfeature/add-login-page\nbugfix/fix-navigation-menu\nhotfix/security-patch\nrelease/v2.0.0\ndocs/update-readme\ntest/add-unit-tests\nrefactor/optimize-database\n\n\n\nCreate a .gitignore file in your repository root:\n# macOS\n.DS_Store\n.AppleDouble\n.LSOverride\n\n# IDE\n.vscode/\n.idea/\n*.swp\n*.swo\n\n# Dependencies\nnode_modules/\nvendor/\n.env\n\n# Build outputs\ndist/\nbuild/\n*.log\n\n# Sensitive data\n*.pem\n*.key\n.env.local\nconfig/secrets.yml\n\n\n\n\nNever commit sensitive data:\n\nPasswords, API keys, tokens\nPrivate keys or certificates\nDatabase credentials\n.env files with secrets\n\nIf you accidentally commit secrets:\n# Remove from history (requires force push)\ngit filter-branch --force --index-filter \\\n  \"git rm --cached --ignore-unmatch path/to/file\" \\\n  --prune-empty --tag-name-filter cat -- --all\n\n# Or use BFG Repo-Cleaner (easier)\nbrew install bfg\nbfg --delete-files file-with-secrets.txt\nUse GitHub’s security features:\n\nEnable Dependabot alerts\nEnable secret scanning\nUse protected branches\nRequire PR reviews\n\n\n\n\n\n\nAlways work in branches - Never commit directly to main\nKeep PRs small - Easier to review and less likely to have conflicts\nWrite descriptive PR descriptions - Include what, why, and how\nReview others’ code - Learn and help maintain quality\nUpdate documentation - Keep README and docs current\nTest before pushing - Run tests locally first\nCommunicate - Use issues and PR comments effectively",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#troubleshooting",
    "href": "git-github-comprehensive-guide.html#troubleshooting",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "# Check SSH key is added\nssh-add -l\n\n# Add SSH key\nssh-add ~/.ssh/id_ed25519\n\n# Test connection\nssh -T git@github.com\n\n\n\n# Pull first, then push\ngit pull origin main --rebase\ngit push origin main\n\n# Or force push (careful!)\ngit push --force-with-lease\n\n\n\n# Create new branch with current commits\ngit branch new-branch\n\n# Reset original branch\ngit reset --hard HEAD~3  # Go back 3 commits\n\n# Switch to new branch\ngit checkout new-branch\n\n\n\n# Keep changes, undo commit\ngit reset --soft HEAD~1\n\n# Discard changes completely\ngit reset --hard HEAD~1\n\n\n\n# Install Git LFS\nbrew install git-lfs\ngit lfs install\n\n# Track large files\ngit lfs track \"*.psd\"\ngit add .gitattributes\ngit add large-file.psd\ngit commit -m \"Add large file with LFS\"\n\n\n\n# Update your branch\ngit checkout main\ngit pull origin main\ngit checkout your-branch\ngit rebase main\n\n# Resolve conflicts, then\ngit add .\ngit rebase --continue\ngit push --force-with-lease",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#quick-reference",
    "href": "git-github-comprehensive-guide.html#quick-reference",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "[alias]\n    st = status\n    co = checkout\n    ci = commit\n    br = branch\n    df = diff\n    lg = log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit\n    last = log -1 HEAD\n    unstage = reset HEAD --\n    amend = commit --amend\n    branches = branch -a\n    remotes = remote -v\n    contributors = shortlog --summary --numbered\n\n\n\n\nCmd + Shift + P → Git commands\nCtrl + Shift + G → Source control panel\nCmd + Enter → Commit staged changes\nOption + Cmd + Enter → Commit all changes\n\n\n\n\n# Git shortcuts\nalias g='git'\nalias gs='git status'\nalias ga='git add'\nalias gc='git commit -m'\nalias gp='git push'\nalias gpl='git pull'\nalias gco='git checkout'\nalias gb='git branch'\nalias glog='git log --oneline --graph --all'\n\n# GitHub CLI shortcuts\nalias ghr='gh repo'\nalias ghpr='gh pr'\nalias ghi='gh issue'",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#resources-links",
    "href": "git-github-comprehensive-guide.html#resources-links",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "Git Documentation: https://git-scm.com/doc\nGitHub Docs: https://docs.github.com\nGitHub CLI Manual: https://cli.github.com/manual\nGitHub Learning Lab: https://lab.github.com\n\n\n\n\n\nLearn Git Branching: https://learngitbranching.js.org\nGitHub Skills: https://skills.github.com\nAtlassian Git Tutorial: https://www.atlassian.com/git/tutorials\nOh My Git! (Game): https://ohmygit.org\n\n\n\n\n\nGitHub Git Cheat Sheet: https://education.github.com/git-cheat-sheet-education.pdf\nInteractive Git Cheat Sheet: https://ndpsoftware.com/git-cheatsheet.html\nGitHub CLI Cheat Sheet: https://github.com/cli/cli#commands\n\n\n\n\n\nPro Git Book (Free): https://git-scm.com/book\nGit Flow: https://nvie.com/posts/a-successful-git-branching-model\nConventional Commits: https://www.conventionalcommits.org\nSemantic Versioning: https://semver.org\n\n\n\n\n\nGitHub Desktop: https://desktop.github.com\nSourceTree: https://www.sourcetreeapp.com\nGitKraken: https://www.gitkraken.com\nTower: https://www.git-tower.com\n\n\n\n\n\nGitLens: Enhanced Git capabilities\nGit Graph: Visualize branch structure\nGitHub Pull Requests: Manage PRs from VS Code\nGit History: View and search git log\n\n\n\n\n\nGitHub Status: https://www.githubstatus.com\nStack Overflow Git Tag: https://stackoverflow.com/questions/tagged/git\nGitHub Community Forum: https://github.community\n\n\n\n\n\nGitHub YouTube: https://youtube.com/github\nThe Net Ninja Git Tutorial: Comprehensive video series\nTraversy Media Git Crash Course: Quick overview\n\n\n\n\n\nGitHub Flavored Markdown: https://github.github.com/gfm\nMarkdown Guide: https://www.markdownguide.org\nShields.io (Badges): https://shields.io",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "git-github-comprehensive-guide.html#appendix-quick-setup-script",
    "href": "git-github-comprehensive-guide.html#appendix-quick-setup-script",
    "title": "Git and GitHub Comprehensive Training Guide",
    "section": "",
    "text": "Save this as setup-git-github.sh and run to quickly set up your environment:\n#!/bin/bash\n\necho \"🚀 Git and GitHub Setup Script for macOS\"\necho \"=======================================\"\n\n# Install Homebrew if not present\nif ! command -v brew &&gt; /dev/null; then\n    echo \"📦 Installing Homebrew...\"\n    /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nfi\n\n# Install Git\necho \"📦 Installing Git...\"\nbrew install git\n\n# Install GitHub CLI\necho \"📦 Installing GitHub CLI...\"\nbrew install gh\n\n# Git configuration\necho \"⚙️ Configuring Git...\"\nread -p \"Enter your name: \" name\nread -p \"Enter your email: \" email\n\ngit config --global user.name \"$name\"\ngit config --global user.email \"$email\"\ngit config --global init.defaultBranch main\ngit config --global color.ui auto\n\n# Generate SSH key\necho \"🔑 Generating SSH key...\"\nssh-keygen -t ed25519 -C \"$email\" -f ~/.ssh/id_ed25519 -N \"\"\n\n# Start SSH agent and add key\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n\n# Copy SSH key to clipboard\npbcopy &lt; ~/.ssh/id_ed25519.pub\necho \"📋 SSH public key copied to clipboard!\"\n\n# GitHub CLI authentication\necho \"🔐 Authenticating with GitHub...\"\ngh auth login\n\necho \"✅ Setup complete!\"\necho \"\"\necho \"Next steps:\"\necho \"1. Go to GitHub Settings → SSH Keys\"\necho \"2. Add a new SSH key (already in clipboard)\"\necho \"3. Test with: ssh -T git@github.com\"\nMake executable and run:\nchmod +x setup-git-github.sh\n./setup-git-github.sh\n\nLast Updated: 2024 Version: 1.0\nThis guide is a living document. Contribute improvements at: [your-repo-url]",
    "crumbs": [
      "GitHub"
    ]
  },
  {
    "objectID": "GitHub/veda-preview.html",
    "href": "GitHub/veda-preview.html",
    "title": "The VEDA Project",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "GitHub",
      "The VEDA Project"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html",
    "href": "Jupyterhub/setup-disaster-repo.html",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "Prerequisites\nGitHub Account Setup\nConfigure Git Identity\nGitHub Authentication Setup\nClone the Repository\nWorking with Branches\nMaking Changes and Pushing\nTroubleshooting Common Issues\n\n\n\n\n\nBefore starting, ensure you have: - Git installed in your JupyterHub environment - Access to terminal in JupyterHub - Internet connection - GitHub account (we’ll create one if needed)\nCheck if Git is installed:\ngit --version\nIf not installed, contact your JupyterHub administrator.\n\n\n\n\n\n\n\nVisit https://github.com\nClick Sign up\nEnter your details:\n\nUsername: Choose carefully (this is permanent and public)\nEmail: Use your professional/institutional email\nPassword: Create a strong password\n\nVerify your email address\nComplete profile setup\n\n\n\n\n\nGo to Settings → Password and authentication\nClick Enable two-factor authentication\nUse an authenticator app (Google Authenticator, Authy, or Microsoft Authenticator)\nSave backup codes securely\n\n\n\n\n\n\nConfigure Git with your GitHub account information:\n# Set your name (visible in commits)\ngit config --global user.name \"Your Full Name\"\n\n# Set your email (MUST match your GitHub account email)\ngit config --global user.email \"your.email@example.com\"\n\n# Set default branch name to main\ngit config --global init.defaultBranch main\n\n# Enable colored output for better readability\ngit config --global color.ui auto\n\n# Verify your configuration\ngit config --list\nExample:\ngit config --global user.name \"Kyle Lesinger\"\ngit config --global user.email \"kyle.lesinger@example.com\"\n\n\n\n\nSince GitHub no longer supports password authentication, you need to use either: 1. Personal Access Token (Easier for JupyterHub) 2. SSH Keys (More secure, one-time setup) 3. GitHub CLI (Recommended - handles auth automatically)\n\n\n# Authenticate with GitHub CLI\ngh auth login\n\n# Follow the prompts:\n# 1. Choose: GitHub.com\n# 2. Choose: HTTPS (recommended for JupyterHub)\n# 3. Choose: Login with a web browser\n# 4. Copy the one-time code shown\n# 5. Press Enter to open browser (or manually visit https://github.com/login/device)\n# 6. Enter the code and authorize\n\n# Verify authentication\ngh auth status\n\n\n\n\nGo to GitHub.com → Settings → Developer settings\nClick Personal access tokens → Tokens (classic)\nClick Generate new token → Generate new token (classic)\nName it: “JupyterHub Access”\nSet expiration (90 days recommended)\nSelect scopes:\n\n✅ repo (Full control of private repositories)\n✅ workflow (Update GitHub Action workflows)\n\nClick Generate token\nCOPY THE TOKEN IMMEDIATELY (you won’t see it again!)\n\nStore the token securely for use when pushing:\n# Store credentials (will be saved after first use)\ngit config --global credential.helper store\n\n\n\n# Generate SSH key\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n# Press Enter for default location\n# Optionally set a passphrase\n\n# Display your public key\ncat ~/.ssh/id_ed25519.pub\n\n# Copy the entire output, then:\n# 1. Go to GitHub.com → Settings → SSH and GPG keys\n# 2. Click \"New SSH key\"\n# 3. Paste your key and save\n\n# Test SSH connection\nssh -T git@github.com\n\n\n\n\n\n\n\n# Navigate to your workspace\ncd ~/\n\n# Clone the repository (creates a new folder called 'conversion_scripts')\ngit clone https://github.com/kyle-lesinger/conversion_scripts.git\n\n# Navigate into the repository\ncd conversion_scripts\n\n# Verify the clone\nls -la\ngit status\n\n\n\n# Check current remotes\ngit remote -v\n\n# You should see:\n# origin  https://github.com/kyle-lesinger/conversion_scripts.git (fetch)\n# origin  https://github.com/kyle-lesinger/conversion_scripts.git (push)\n\n\n\nIf you set up SSH keys and prefer using SSH:\n# Remove HTTPS remote\ngit remote remove origin\n\n# Add SSH remote\ngit remote add origin git@github.com:kyle-lesinger/conversion_scripts.git\n\n# Verify the change\ngit remote -v\n\n\n\n\n\n\n\nAlways create a new branch for your work instead of committing directly to main:\n# Make sure you're on the main branch\ngit checkout main\n\n# Pull latest changes\ngit pull origin main\n\n# Create and switch to a new branch\ngit checkout -b feature/your-feature-name\n\n# Example branch names:\n# git checkout -b feature/add-preprocessing\n# git checkout -b bugfix/fix-data-pipeline\n# git checkout -b docs/update-readme\n\n\n\n# Check which branch you're on\ngit branch\n\n# List all branches (local and remote)\ngit branch -a\n\n\n\n\n\n\n\n# Create or edit files\necho \"# Conversion Scripts\" &gt; README.md\necho \"This repository contains data conversion scripts.\" &gt;&gt; README.md\n\n# Check what files have changed\ngit status\n\n\n\n# Add specific files\ngit add README.md\n\n# Or add all changes\ngit add .\n\n# Commit with descriptive message\ngit commit -m \"Add README with project description\"\n\n# View commit history\ngit log --oneline\n\n\n\n\n\n# Push and set upstream branch\ngit push -u origin feature/your-feature-name\n\n# If using Personal Access Token, enter:\n# Username: your-github-username\n# Password: your-personal-access-token (NOT your GitHub password!)\n\n\n\n# After upstream is set, simply:\ngit push\n\n\n\n\n# Using GitHub CLI (if authenticated)\ngh pr create --title \"Add README documentation\" --body \"Added project description\"\n\n# Or manually:\n# 1. Visit https://github.com/kyle-lesinger/conversion_scripts\n# 2. Click \"Compare & pull request\" button\n# 3. Add title and description\n# 4. Click \"Create pull request\"\n\n\n\n\n\nHere’s a complete example workflow from start to finish:\n# 1. Configure Git (one-time setup)\ngit config --global user.name \"Kyle Lesinger\"\ngit config --global user.email \"kyle.lesinger@example.com\"\n\n# 2. Authenticate with GitHub CLI\ngh auth login\n# Follow the interactive prompts\n\n# 3. Clone the repository\ncd ~/\ngit clone https://github.com/kyle-lesinger/conversion_scripts.git\ncd conversion_scripts\n\n# 4. Create a new branch\ngit checkout -b feature/add-conversion-script\n\n# 5. Create a new file\ncat &gt; convert_data.py &lt;&lt; 'EOF'\n#!/usr/bin/env python3\n\"\"\"\nData conversion utility script\n\"\"\"\n\ndef convert_format(input_file, output_file):\n    \"\"\"Convert data from one format to another\"\"\"\n    print(f\"Converting {input_file} to {output_file}\")\n    # Add conversion logic here\n\nif __name__ == \"__main__\":\n    convert_format(\"input.txt\", \"output.json\")\nEOF\n\n# 6. Stage and commit\ngit add convert_data.py\ngit commit -m \"Add data conversion utility script\"\n\n# 7. Push to GitHub\ngit push -u origin feature/add-conversion-script\n\n# 8. Create pull request\ngh pr create --title \"Add data conversion script\" --body \"Initial conversion utility\"\n\n\n\n\n\n\nError: remote: Invalid username or password\nSolution:\n# Use Personal Access Token instead of password\n# When prompted for password, paste your token\n\n# Or use GitHub CLI\ngh auth login\n\n\n\nError: git@github.com: Permission denied (publickey)\nSolution:\n# Check if SSH key exists\nls -la ~/.ssh/\n\n# Generate new key if needed\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Add to SSH agent\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n\n# Add public key to GitHub account\ncat ~/.ssh/id_ed25519.pub\n# Copy output and add to GitHub.com → Settings → SSH Keys\n\n\n\nError: error: remote origin already exists\nSolution:\n# Remove existing remote\ngit remote remove origin\n\n# Add new remote\ngit remote add origin https://github.com/kyle-lesinger/conversion_scripts.git\n\n\n\nError: ! [rejected] main -&gt; main (non-fast-forward)\nSolution:\n# Pull latest changes first\ngit pull origin main --rebase\n\n# Then push\ngit push origin main\n\n\n\nError: Working on main branch instead of feature branch\nSolution:\n# Create new branch with current changes\ngit checkout -b feature/my-changes\n\n# Push to new branch\ngit push -u origin feature/my-changes\n\n\n\n\n\n\nAlways work in branches - Never commit directly to main\nPull before pushing - Always sync with remote before pushing\nUse descriptive commit messages - Explain what and why\nCommit frequently - Small, logical commits are better\nKeep tokens secure - Never commit tokens or passwords\nTest locally - Run your code before committing\n\n\n\n\n\n# Clone repository\ngit clone https://github.com/kyle-lesinger/conversion_scripts.git\n\n# Create branch\ngit checkout -b feature/new-feature\n\n# Check status\ngit status\n\n# Add files\ngit add .\n\n# Commit\ngit commit -m \"Description of changes\"\n\n# Push new branch\ngit push -u origin feature/new-feature\n\n# Push existing branch\ngit push\n\n# Pull latest changes\ngit pull origin main\n\n# Switch branches\ngit checkout branch-name\n\n# List branches\ngit branch -a\n\n# Delete local branch\ngit branch -d branch-name\n\n# View commit history\ngit log --oneline --graph\n\n\n\n\n\nGitHub Docs\nGit Documentation\nGitHub CLI Manual\nPro Git Book (Free)\n\n\n\n\n\nIf you encounter issues not covered here:\n\nCheck the repository issues: https://github.com/kyle-lesinger/conversion_scripts/issues\nAsk in the JupyterHub support channel\nConsult the comprehensive Git/GitHub guide\n\n\nLast Updated: 2024 Version: 1.0",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#table-of-contents",
    "href": "Jupyterhub/setup-disaster-repo.html#table-of-contents",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "Prerequisites\nGitHub Account Setup\nConfigure Git Identity\nGitHub Authentication Setup\nClone the Repository\nWorking with Branches\nMaking Changes and Pushing\nTroubleshooting Common Issues",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#prerequisites",
    "href": "Jupyterhub/setup-disaster-repo.html#prerequisites",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "Before starting, ensure you have: - Git installed in your JupyterHub environment - Access to terminal in JupyterHub - Internet connection - GitHub account (we’ll create one if needed)\nCheck if Git is installed:\ngit --version\nIf not installed, contact your JupyterHub administrator.",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#github-account-setup",
    "href": "Jupyterhub/setup-disaster-repo.html#github-account-setup",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "Visit https://github.com\nClick Sign up\nEnter your details:\n\nUsername: Choose carefully (this is permanent and public)\nEmail: Use your professional/institutional email\nPassword: Create a strong password\n\nVerify your email address\nComplete profile setup\n\n\n\n\n\nGo to Settings → Password and authentication\nClick Enable two-factor authentication\nUse an authenticator app (Google Authenticator, Authy, or Microsoft Authenticator)\nSave backup codes securely",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#configure-git-identity",
    "href": "Jupyterhub/setup-disaster-repo.html#configure-git-identity",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "Configure Git with your GitHub account information:\n# Set your name (visible in commits)\ngit config --global user.name \"Your Full Name\"\n\n# Set your email (MUST match your GitHub account email)\ngit config --global user.email \"your.email@example.com\"\n\n# Set default branch name to main\ngit config --global init.defaultBranch main\n\n# Enable colored output for better readability\ngit config --global color.ui auto\n\n# Verify your configuration\ngit config --list\nExample:\ngit config --global user.name \"Kyle Lesinger\"\ngit config --global user.email \"kyle.lesinger@example.com\"",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#github-authentication-setup",
    "href": "Jupyterhub/setup-disaster-repo.html#github-authentication-setup",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "Since GitHub no longer supports password authentication, you need to use either: 1. Personal Access Token (Easier for JupyterHub) 2. SSH Keys (More secure, one-time setup) 3. GitHub CLI (Recommended - handles auth automatically)\n\n\n# Authenticate with GitHub CLI\ngh auth login\n\n# Follow the prompts:\n# 1. Choose: GitHub.com\n# 2. Choose: HTTPS (recommended for JupyterHub)\n# 3. Choose: Login with a web browser\n# 4. Copy the one-time code shown\n# 5. Press Enter to open browser (or manually visit https://github.com/login/device)\n# 6. Enter the code and authorize\n\n# Verify authentication\ngh auth status\n\n\n\n\nGo to GitHub.com → Settings → Developer settings\nClick Personal access tokens → Tokens (classic)\nClick Generate new token → Generate new token (classic)\nName it: “JupyterHub Access”\nSet expiration (90 days recommended)\nSelect scopes:\n\n✅ repo (Full control of private repositories)\n✅ workflow (Update GitHub Action workflows)\n\nClick Generate token\nCOPY THE TOKEN IMMEDIATELY (you won’t see it again!)\n\nStore the token securely for use when pushing:\n# Store credentials (will be saved after first use)\ngit config --global credential.helper store\n\n\n\n# Generate SSH key\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n# Press Enter for default location\n# Optionally set a passphrase\n\n# Display your public key\ncat ~/.ssh/id_ed25519.pub\n\n# Copy the entire output, then:\n# 1. Go to GitHub.com → Settings → SSH and GPG keys\n# 2. Click \"New SSH key\"\n# 3. Paste your key and save\n\n# Test SSH connection\nssh -T git@github.com",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#clone-the-repository",
    "href": "Jupyterhub/setup-disaster-repo.html#clone-the-repository",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "# Navigate to your workspace\ncd ~/\n\n# Clone the repository (creates a new folder called 'conversion_scripts')\ngit clone https://github.com/kyle-lesinger/conversion_scripts.git\n\n# Navigate into the repository\ncd conversion_scripts\n\n# Verify the clone\nls -la\ngit status\n\n\n\n# Check current remotes\ngit remote -v\n\n# You should see:\n# origin  https://github.com/kyle-lesinger/conversion_scripts.git (fetch)\n# origin  https://github.com/kyle-lesinger/conversion_scripts.git (push)\n\n\n\nIf you set up SSH keys and prefer using SSH:\n# Remove HTTPS remote\ngit remote remove origin\n\n# Add SSH remote\ngit remote add origin git@github.com:kyle-lesinger/conversion_scripts.git\n\n# Verify the change\ngit remote -v",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#working-with-branches",
    "href": "Jupyterhub/setup-disaster-repo.html#working-with-branches",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "Always create a new branch for your work instead of committing directly to main:\n# Make sure you're on the main branch\ngit checkout main\n\n# Pull latest changes\ngit pull origin main\n\n# Create and switch to a new branch\ngit checkout -b feature/your-feature-name\n\n# Example branch names:\n# git checkout -b feature/add-preprocessing\n# git checkout -b bugfix/fix-data-pipeline\n# git checkout -b docs/update-readme\n\n\n\n# Check which branch you're on\ngit branch\n\n# List all branches (local and remote)\ngit branch -a",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#making-changes-and-pushing",
    "href": "Jupyterhub/setup-disaster-repo.html#making-changes-and-pushing",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "# Create or edit files\necho \"# Conversion Scripts\" &gt; README.md\necho \"This repository contains data conversion scripts.\" &gt;&gt; README.md\n\n# Check what files have changed\ngit status\n\n\n\n# Add specific files\ngit add README.md\n\n# Or add all changes\ngit add .\n\n# Commit with descriptive message\ngit commit -m \"Add README with project description\"\n\n# View commit history\ngit log --oneline\n\n\n\n\n\n# Push and set upstream branch\ngit push -u origin feature/your-feature-name\n\n# If using Personal Access Token, enter:\n# Username: your-github-username\n# Password: your-personal-access-token (NOT your GitHub password!)\n\n\n\n# After upstream is set, simply:\ngit push\n\n\n\n\n# Using GitHub CLI (if authenticated)\ngh pr create --title \"Add README documentation\" --body \"Added project description\"\n\n# Or manually:\n# 1. Visit https://github.com/kyle-lesinger/conversion_scripts\n# 2. Click \"Compare & pull request\" button\n# 3. Add title and description\n# 4. Click \"Create pull request\"",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#complete-workflow-example",
    "href": "Jupyterhub/setup-disaster-repo.html#complete-workflow-example",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "Here’s a complete example workflow from start to finish:\n# 1. Configure Git (one-time setup)\ngit config --global user.name \"Kyle Lesinger\"\ngit config --global user.email \"kyle.lesinger@example.com\"\n\n# 2. Authenticate with GitHub CLI\ngh auth login\n# Follow the interactive prompts\n\n# 3. Clone the repository\ncd ~/\ngit clone https://github.com/kyle-lesinger/conversion_scripts.git\ncd conversion_scripts\n\n# 4. Create a new branch\ngit checkout -b feature/add-conversion-script\n\n# 5. Create a new file\ncat &gt; convert_data.py &lt;&lt; 'EOF'\n#!/usr/bin/env python3\n\"\"\"\nData conversion utility script\n\"\"\"\n\ndef convert_format(input_file, output_file):\n    \"\"\"Convert data from one format to another\"\"\"\n    print(f\"Converting {input_file} to {output_file}\")\n    # Add conversion logic here\n\nif __name__ == \"__main__\":\n    convert_format(\"input.txt\", \"output.json\")\nEOF\n\n# 6. Stage and commit\ngit add convert_data.py\ngit commit -m \"Add data conversion utility script\"\n\n# 7. Push to GitHub\ngit push -u origin feature/add-conversion-script\n\n# 8. Create pull request\ngh pr create --title \"Add data conversion script\" --body \"Initial conversion utility\"",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#troubleshooting-common-issues",
    "href": "Jupyterhub/setup-disaster-repo.html#troubleshooting-common-issues",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "Error: remote: Invalid username or password\nSolution:\n# Use Personal Access Token instead of password\n# When prompted for password, paste your token\n\n# Or use GitHub CLI\ngh auth login\n\n\n\nError: git@github.com: Permission denied (publickey)\nSolution:\n# Check if SSH key exists\nls -la ~/.ssh/\n\n# Generate new key if needed\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Add to SSH agent\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n\n# Add public key to GitHub account\ncat ~/.ssh/id_ed25519.pub\n# Copy output and add to GitHub.com → Settings → SSH Keys\n\n\n\nError: error: remote origin already exists\nSolution:\n# Remove existing remote\ngit remote remove origin\n\n# Add new remote\ngit remote add origin https://github.com/kyle-lesinger/conversion_scripts.git\n\n\n\nError: ! [rejected] main -&gt; main (non-fast-forward)\nSolution:\n# Pull latest changes first\ngit pull origin main --rebase\n\n# Then push\ngit push origin main\n\n\n\nError: Working on main branch instead of feature branch\nSolution:\n# Create new branch with current changes\ngit checkout -b feature/my-changes\n\n# Push to new branch\ngit push -u origin feature/my-changes",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#best-practices",
    "href": "Jupyterhub/setup-disaster-repo.html#best-practices",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "Always work in branches - Never commit directly to main\nPull before pushing - Always sync with remote before pushing\nUse descriptive commit messages - Explain what and why\nCommit frequently - Small, logical commits are better\nKeep tokens secure - Never commit tokens or passwords\nTest locally - Run your code before committing",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#quick-command-reference",
    "href": "Jupyterhub/setup-disaster-repo.html#quick-command-reference",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "# Clone repository\ngit clone https://github.com/kyle-lesinger/conversion_scripts.git\n\n# Create branch\ngit checkout -b feature/new-feature\n\n# Check status\ngit status\n\n# Add files\ngit add .\n\n# Commit\ngit commit -m \"Description of changes\"\n\n# Push new branch\ngit push -u origin feature/new-feature\n\n# Push existing branch\ngit push\n\n# Pull latest changes\ngit pull origin main\n\n# Switch branches\ngit checkout branch-name\n\n# List branches\ngit branch -a\n\n# Delete local branch\ngit branch -d branch-name\n\n# View commit history\ngit log --oneline --graph",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#additional-resources",
    "href": "Jupyterhub/setup-disaster-repo.html#additional-resources",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "GitHub Docs\nGit Documentation\nGitHub CLI Manual\nPro Git Book (Free)",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/setup-disaster-repo.html#getting-help",
    "href": "Jupyterhub/setup-disaster-repo.html#getting-help",
    "title": "Setting Up Disaster Repository - Step-by-Step Guide",
    "section": "",
    "text": "If you encounter issues not covered here:\n\nCheck the repository issues: https://github.com/kyle-lesinger/conversion_scripts/issues\nAsk in the JupyterHub support channel\nConsult the comprehensive Git/GitHub guide\n\n\nLast Updated: 2024 Version: 1.0",
    "crumbs": [
      "JupyterHub",
      "Setting Up Disaster Repository - Step-by-Step Guide"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html",
    "href": "Jupyterhub/jupyterhub-training-guide.html",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "Introduction\nGetting Started\nJupyterHub Interface Overview\nWorking with Jupyter Notebooks\nData Management\nEnvironment and Package Management\nTerminal and Command Line Access\nCollaboration and Sharing\nResource Management\nBest Practices\nTroubleshooting\nKeyboard Shortcuts\nResources and Links\n\n\n\n\n\n\n\nJupyterHub is a multi-user server that manages and provides web-based Jupyter notebook environments for multiple users. It allows you to:\n\nAccess powerful computing resources through your web browser\nWrite and execute code in Python, R, Julia, and other languages\nVisualize data with interactive plots and charts\nCollaborate with team members on shared projects\nWork from anywhere without local setup requirements\n\n\n\n\nThe Disasters Hub (https://hub.disasters.2i2c.cloud/) is a specialized JupyterHub instance designed for disaster response and analysis work. It provides:\n\nPre-configured environments for geospatial analysis\nAccess to disaster-related datasets\nCollaboration tools for response teams\nIntegration with cloud storage services\nScalable computing resources\n\n\n\n\n✅ No Installation Required - Everything runs in your browser\n✅ Pre-configured Environments - Common packages already installed\n✅ Persistent Storage - Your work is saved between sessions\n✅ Collaboration Ready - Share notebooks with team members\n✅ Scalable Resources - Access to GPU and high-memory instances when needed\n\n\n\n\n\n\n\n\nNavigate to the Hub\n\nOpen your web browser (Chrome, Firefox, Safari, or Edge recommended)\nGo to: https://hub.disasters.2i2c.cloud/\nBookmark this URL for easy access\n\nAuthentication\n\nYou’ll see a login screen with authentication options\nCommon authentication methods:\n\nGitHub: Use your GitHub credentials\nGoogle: Use your Google account\nInstitutional Login: Use your organization’s credentials\n\nSelect your authentication method and follow the prompts\n\nFirst-Time Login\n\nAccept terms of service if prompted\nYour home directory will be created automatically\nInitial setup may take 30-60 seconds\n\n\n\n\n\nAfter login, you may be presented with server options:\nServer Options:\n┌─────────────────────────────────────┐\n│ • Small (2 CPU, 4GB RAM)            │\n│ • Medium (4 CPU, 8GB RAM)           │\n│ • Large (8 CPU, 16GB RAM)           │\n│ • GPU Instance (if available)       │\n└─────────────────────────────────────┘\nTips for Server Selection: - Start with Small for basic notebook work - Use Medium for data processing tasks - Choose Large for machine learning or big data - Select GPU only when needed (limited availability)\n\n\n\n\n\n\n\nOnce logged in, you’ll see the JupyterLab interface:\n┌──────────────────────────────────────────────────────────┐\n│ [File] [Edit] [View] [Run] [Kernel] [Tabs] [Settings]    │\n├──────────────────────────────────────────────────────────┤\n│ 📁 File Browser │          Main Work Area                | \n│ ├── 📂 data     │                                        │\n│ ├── 📂 notebooks│      [Launcher Tab]                    │\n│ ├── 📂 scripts  │      • Notebook (Python 3)             │\n│ └── 📄 README   │      • Console                         │\n│                 │      • Terminal                        |\n│ [+] New         │      • Text File                       │\n└──────────────────────────────────────────────────────────┘\n\n\n\n\nTop Menu Bar\n\nFile operations, editing, running code\nKernel management\nView options and settings\n\nLeft Sidebar\n\nFile Browser (📁): Navigate and manage files\nRunning Terminals and Kernels (▶): Monitor active sessions\nCommand Palette (🔧): Access all commands\nExtension Manager (🧩): Add functionality\n\nMain Work Area\n\nMultiple tabs for notebooks, terminals, and files\nDrag tabs to rearrange or create split views\nRight-click tabs for additional options\n\nStatus Bar\n\nCurrent kernel status\nLine/column position\nFile encoding and type\n\n\n\n\n\n\nClick the Python 3 icon in the Launcher\nOr: File → New → Notebook\nSelect kernel (usually Python 3)\nRename your notebook: Right-click on “Untitled.ipynb” → Rename\n\n\n\n\n\n\n\n\nA Jupyter notebook consists of cells that can contain: - Code: Executable Python (or other language) code - Markdown: Formatted text, equations, and images - Raw: Unformatted text\n\n\n\n\n\n\nRun current cell: Shift + Enter (run and move to next)\nRun current cell in place: Ctrl + Enter (stay in cell)\nRun all cells: Menu → Run → Run All Cells\n\n\n\n\n# Code Cell Example\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv('data.csv')\ndata.head()\n# Markdown Cell Example\n## Analysis Results\n- **Finding 1**: Data shows increasing trend\n- **Finding 2**: Correlation coefficient: 0.85\n\n$$E = mc^2$$  # LaTeX equation\n\n\n\n\nInsert cell above: A (in command mode)\nInsert cell below: B (in command mode)\nDelete cell: DD (press D twice in command mode)\nCopy cell: C (in command mode)\nPaste cell: V (in command mode)\nUndo deletion: Z (in command mode)\n\n\n\n\n\nThe kernel is the computational engine that executes your code.\n\n\n\nRestart kernel: Kernel → Restart\nRestart and clear output: Kernel → Restart & Clear Output\nRestart and run all: Kernel → Restart & Run All\nInterrupt execution: Kernel → Interrupt (or I,I in command mode)\nChange kernel: Kernel → Change Kernel\n\n\n\n\n\n○: Kernel idle\n●: Kernel busy\n[*]: Cell currently executing\n[1]: Cell execution number\n\n\n\n\n\n\nUse meaningful cell divisions\n\nOne concept or operation per cell\nSeparate imports, data loading, processing, visualization\n\nDocument your work\n# Good practice: Add comments and markdown cells\n# Load disaster response data\ndf = pd.read_csv('disaster_data.csv')\n\n# Data preprocessing\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.dropna()\nClear output before sharing\n\nKernel → Restart & Clear Output\nReduces file size and removes sensitive output\n\n\n\n\n\n\n\n\n\n\n\n\nDrag and drop files directly into the file browser\nUpload button: Click the ⬆ button in the file browser toolbar\nTerminal upload: Use wget or curl in terminal\nwget https://example.com/data.csv\ncurl -O https://example.com/data.zip\n\n\n\n\n\nRight-click file in browser → Download\nFrom notebook:\nfrom IPython.display import FileLink\nFileLink('results.csv')  # Creates downloadable link\n\n\n\n\n\n\n\nimport boto3\nimport pandas as pd\n\n# Read from S3\ndf = pd.read_csv('s3://bucket-name/path/to/file.csv')\n\n# Write to S3\ndf.to_csv('s3://bucket-name/output/results.csv', index=False)\n\n\n\n# Read from GCS\ndf = pd.read_csv('gs://bucket-name/path/to/file.csv')\n\n# Using gsutil in terminal\n!gsutil cp gs://bucket/file.csv ./data/\n\n\n\n\nRecommended directory structure:\nhome/\n├── data/\n│   ├── raw/           # Original, immutable data\n│   ├── processed/     # Cleaned, transformed data\n│   └── external/      # Data from external sources\n├── notebooks/\n│   ├── exploratory/   # Initial explorations\n│   ├── analysis/      # Detailed analysis\n│   └── reports/       # Final reports\n├── scripts/           # Reusable Python scripts\n├── results/           # Output files, figures\n└── requirements.txt   # Package dependencies\n\n\n\n⚠️ Important: Your home directory is persistent, but understand the storage limits:\n\nHome directory: Usually 10-100 GB (persistent)\nShared data: Read-only datasets available to all users\nTemporary storage: /tmp cleared on restart\nBest practice: Store large datasets in cloud storage, not home directory\n\n\n\n\n\n\n\n\n\n\n# In a notebook cell\n!pip install package_name\n\n# Install specific version\n!pip install pandas==1.3.0\n\n# Install from requirements file\n!pip install -r requirements.txt\n\n# Install in user directory (if no write permissions)\n!pip install --user package_name\n\n\n\n# In a notebook cell\n!conda install -c conda-forge package_name -y\n\n# Install multiple packages\n!conda install numpy pandas matplotlib -y\n\n# Create new environment\n!conda create -n myenv python=3.9 -y\n!conda activate myenv  # Note: Activation in notebooks is tricky\n\n\n\n\n\n\nimport sys\nprint(sys.executable)  # Python interpreter path\nprint(sys.version)     # Python version\n\n# List installed packages\n!pip list\n!conda list\n\n\n\n# In terminal\npython -m venv myproject\nsource myproject/bin/activate  # Linux/Mac\npip install -r requirements.txt\n\n\n\n\n\nInstall IPython kernel:\npython -m ipykernel install --user --name mykernel --display-name \"My Kernel\"\nList available kernels:\njupyter kernelspec list\nRemove a kernel:\njupyter kernelspec uninstall mykernel\n\n\n\n\n\n\n\n\n\nFrom Launcher: Click “Terminal” icon\nFrom menu: File → New → Terminal\nKeyboard shortcut: (varies by setup)\n\n\n\n\n# Navigation\npwd                     # Print working directory\nls -la                  # List files with details\ncd ~/notebooks         # Change directory\n\n# File operations\nmkdir project          # Create directory\ncp file1.txt file2.txt # Copy file\nmv oldname newname     # Move/rename\nrm file.txt           # Delete file (careful!)\n\n# File viewing\ncat file.txt          # Display file contents\nhead -n 10 data.csv   # First 10 lines\ntail -n 10 log.txt    # Last 10 lines\nless large_file.txt   # Page through file\n\n# Process management\nps aux                # List processes\ntop                   # Monitor resources\nkill -9 PID          # Kill process\n\n# Git operations\ngit status\ngit add .\ngit commit -m \"message\"\ngit push\n\n\n\n# Count lines in file\nwc -l data.csv\n\n# View CSV structure\nhead -1 data.csv | tr ',' '\\n' | nl\n\n# Search in files\ngrep \"pattern\" file.txt\ngrep -r \"pattern\" ./directory\n\n# Compress/decompress\nzip archive.zip file1 file2\nunzip archive.zip\ntar -czf archive.tar.gz directory/\ntar -xzf archive.tar.gz\n\n\n\n\n\n\n\n\n\n\nDownload notebook: File → Download as → Notebook (.ipynb)\nShare via email, Slack, or file sharing service\nRecipient uploads to their JupyterHub\n\n\n\n\n# Initialize repository\ngit init\ngit add notebook.ipynb\ngit commit -m \"Add analysis notebook\"\ngit remote add origin https://github.com/user/repo.git\ngit push -u origin main\n\n\n\n\nHTML: File → Export Notebook As → HTML\nPDF: File → Export Notebook As → PDF (requires LaTeX)\nPython script: File → Export Notebook As → Python\nMarkdown: File → Export Notebook As → Markdown\n\n\n\n\n\nSome JupyterHub deployments support real-time collaboration:\n\nShare workspace link: Get shareable link from hub admin\nCollaborative editing: Multiple users can edit simultaneously\nSee collaborator cursors: Real-time cursor positions\nChat integration: Built-in chat for discussion\n\n\n\n\n\nClear outputs before committing:\njupyter nbconvert --clear-output notebook.ipynb\nUse .gitignore:\n.ipynb_checkpoints/\n__pycache__/\n*.pyc\n.DS_Store\ndata/  # Don't commit large data files\nNotebook diff tools:\n# Install nbdime for better notebook diffs\npip install nbdime\nnbdime config-git --enable\n\n\n\n\n\n\n\n\nYour JupyterHub instance has resource limits:\n# Check available resources\nimport psutil\n\n# Memory\nmemory = psutil.virtual_memory()\nprint(f\"Total RAM: {memory.total / 1e9:.2f} GB\")\nprint(f\"Available: {memory.available / 1e9:.2f} GB\")\nprint(f\"Used: {memory.percent}%\")\n\n# CPU\nprint(f\"CPU cores: {psutil.cpu_count()}\")\nprint(f\"CPU usage: {psutil.cpu_percent()}%\")\n\n# Disk\ndisk = psutil.disk_usage('/')\nprint(f\"Disk space: {disk.total / 1e9:.2f} GB\")\nprint(f\"Disk used: {disk.percent}%\")\n\n\n\n\n\n\nInstall Resource Usage extension\nShows real-time memory and CPU usage in status bar\n\n\n\n\n# Real-time resource monitoring\ntop\nhtop  # If installed\n\n# Memory usage\nfree -h\n\n# Disk usage\ndf -h\ndu -sh *  # Directory sizes\n\n\n\n\n\nClear variables when done:\n# Clear specific variable\ndel large_dataframe\n\n# Clear all variables\n%reset -f\n\n# Garbage collection\nimport gc\ngc.collect()\nUse efficient data types:\n# Use categories for strings with few unique values\ndf['category'] = df['category'].astype('category')\n\n# Use smaller numeric types when possible\ndf['count'] = df['count'].astype('int32')  # Instead of int64\nProcess data in chunks:\n# Read large CSV in chunks\nchunk_size = 10000\nfor chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n    process_chunk(chunk)\n\n\n\n\nAlways shut down kernels and terminals when done:\n\nShutdown kernel: Kernel → Shutdown\nClose terminals: Exit or Ctrl+D\nHub Control Panel: File → Hub Control Panel → Stop My Server\nLogout: File → Log Out\n\n⚠️ Important: Idle servers may be automatically culled after a period of inactivity (usually 1-2 hours).\n\n\n\n\n\n\n\n\nUse consistent naming:\n2024-01-15_earthquake_analysis.ipynb  # Good\nuntitled1.ipynb                       # Bad\nCreate project templates:\n# notebook_template.ipynb\n\n# 1. Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 2. Configuration\npd.set_option('display.max_columns', None)\nplt.style.use('seaborn')\n\n# 3. Data Loading\n\n# 4. Data Exploration\n\n# 5. Analysis\n\n# 6. Results\nDocument dependencies:\n# Generate requirements.txt\n!pip freeze &gt; requirements.txt\n\n\n\n\n\nNever commit credentials:\n# Bad\napi_key = \"sk-abc123def456\"\n\n# Good - Use environment variables\nimport os\napi_key = os.environ.get('API_KEY')\nUse secrets management:\n# Store secrets in .env file\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Access secrets\nsecret = os.getenv('SECRET_KEY')\nBe careful with outputs:\n\nClear cells containing sensitive information\nReview notebooks before sharing\n\n\n\n\n\n\nVectorize operations:\n# Slow\nresults = []\nfor i in range(len(df)):\n    results.append(df.iloc[i]['column'] * 2)\n\n# Fast\nresults = df['column'] * 2\nUse built-in functions:\n# Use pandas/numpy operations instead of loops\ndf['new_col'] = df['col1'] + df['col2']  # Vectorized\nProfile your code:\n%%time  # Time entire cell\n\n%timeit function()  # Time single line\n\n# Detailed profiling\n%load_ext line_profiler\n%lprun -f function_to_profile function_to_profile()\n\n\n\n\n\n\n\n\n\n\n\nCheck resources: Server might be full\nTry different kernel: Some kernels may be broken\nRestart server: Hub Control Panel → Stop → Start\n\n\n\n\n# Check if package is installed\nimport importlib\nif importlib.util.find_spec(\"package_name\") is None:\n    !pip install package_name\n    \n# Restart kernel after installation\nfrom IPython import get_ipython\nget_ipython().kernel.do_shutdown(True)\n\n\n\n\nClear unnecessary variables: del variable_name\nUse smaller data samples for testing\nRequest larger server instance\nProcess data in chunks\n\n\n\n\n\nCheck disk space: df -h in terminal\nCheck file permissions: ls -la notebook.ipynb\nSave with new name: File → Save As\nDownload backup: File → Download\n\n\n\n\n\nCheck internet connection\nTry different browser\nClear browser cache\nCheck if hub is under maintenance\n\n\n\n\n\n\nBuilt-in help:\nhelp(function_name)\nfunction_name?  # Quick help\nfunction_name??  # Source code\nDocumentation:\n\nJupyterHub docs: https://jupyterhub.readthedocs.io\nJupyterLab docs: https://jupyterlab.readthedocs.io\n2i2c docs: https://docs.2i2c.org\n\nCommunity support:\n\nDiscourse forum\nGitHub issues\nStack Overflow with tags: jupyter, jupyterhub\n\n\n\n\n\n\n\n\n\nPress Esc to enter command mode\n\n\n\nShortcut\nAction\n\n\n\n\nEnter\nEnter edit mode\n\n\nA\nInsert cell above\n\n\nB\nInsert cell below\n\n\nD,D\nDelete cell\n\n\nY\nChange to code cell\n\n\nM\nChange to markdown cell\n\n\nShift+Up/Down\nSelect multiple cells\n\n\nShift+M\nMerge selected cells\n\n\nC\nCopy cell\n\n\nX\nCut cell\n\n\nV\nPaste cell below\n\n\nShift+V\nPaste cell above\n\n\nZ\nUndo cell deletion\n\n\n0,0\nRestart kernel\n\n\nI,I\nInterrupt kernel\n\n\n\n\n\n\nPress Enter to enter edit mode\n\n\n\nShortcut\nAction\n\n\n\n\nEsc\nEnter command mode\n\n\nCtrl+Enter\nRun cell\n\n\nShift+Enter\nRun cell, select below\n\n\nAlt+Enter\nRun cell, insert below\n\n\nCtrl+S\nSave notebook\n\n\nTab\nCode completion\n\n\nShift+Tab\nTooltip\n\n\nCtrl+]\nIndent\n\n\nCtrl+[\nDedent\n\n\nCtrl+A\nSelect all\n\n\nCtrl+Z\nUndo\n\n\nCtrl+Y\nRedo\n\n\n\n\n\n\n\n\n\nShortcut\nAction\n\n\n\n\nCtrl+Shift+C\nCommand palette\n\n\nCtrl+B\nToggle left sidebar\n\n\nCtrl+Shift+D\nToggle file browser\n\n\nCtrl+Shift+F\nFind and replace\n\n\nCtrl+Shift+[\nPrevious tab\n\n\nCtrl+Shift+]\nNext tab\n\n\nAlt+W\nClose tab\n\n\n\n\n\n\n\n\n\n\n\nJupyterHub Documentation: https://jupyterhub.readthedocs.io\nJupyterLab Documentation: https://jupyterlab.readthedocs.io\nJupyter Notebook Documentation: https://jupyter-notebook.readthedocs.io\n2i2c Infrastructure Guide: https://docs.2i2c.org\n\n\n\n\n\nJupyter Tutorial: https://jupyter.org/try\nReal Python Jupyter Guide: https://realpython.com/jupyter-notebook-introduction/\nDataCamp Jupyter Tutorial: https://www.datacamp.com/tutorial/tutorial-jupyter-notebook\nOfficial Jupyter Examples: https://github.com/jupyter/jupyter/wiki/Gallery-of-Jupyter-Notebooks\n\n\n\n\n\nNASA Disasters Program: https://disasters.nasa.gov\nUSGS Hazards Data: https://www.usgs.gov/natural-hazards\nNOAA Disaster Data: https://www.ncdc.noaa.gov/billions/\nCopernicus Emergency Management: https://emergency.copernicus.eu\n\n\n\n\n# Geospatial analysis\nimport geopandas as gpd\nimport rasterio\nimport xarray as xr\nimport folium\n\n# Data processing\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# Machine learning\nfrom sklearn import *\nimport tensorflow as tf\nimport torch\n\n# Earth observation\nimport ee  # Google Earth Engine\nimport planetary_computer as pc\nimport pystac_client\n\n\n\nInstall JupyterLab extensions for enhanced functionality:\n# Variable inspector\njupyter labextension install @lckr/jupyterlab_variableinspector\n\n# Table of contents\njupyter labextension install @jupyterlab/toc\n\n# Git integration\npip install jupyterlab-git\n\n# Code formatter\npip install jupyterlab-code-formatter\n\n\n\n\nJupyter Discourse Forum: https://discourse.jupyter.org\nStack Overflow: https://stackoverflow.com/questions/tagged/jupyter\nGitHub Issues: https://github.com/jupyterhub/jupyterhub/issues\n2i2c Support: https://2i2c.org/support\nGitter Chat: https://gitter.im/jupyterhub/jupyterhub\n\n\n\n\n\nJupyterLab Cheat Sheet: https://www.datacamp.com/cheat-sheet/jupyterlab-cheat-sheet\nJupyter Shortcuts PDF: https://www.cheatography.com/weidadeyue/cheat-sheets/jupyter-notebook/\nMarkdown Guide: https://www.markdownguide.org/cheat-sheet/\n\n\n\n\n\n\nHere’s a complete example workflow for disaster analysis:\n# 1. Setup and Imports\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport folium\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 2. Load Data\n# Earthquake data\nearthquakes = pd.read_csv('https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_month.csv')\nearthquakes['time'] = pd.to_datetime(earthquakes['time'])\n\n# 3. Data Processing\n# Filter recent events\nrecent = earthquakes[earthquakes['time'] &gt; datetime.now() - timedelta(days=7)]\n\n# Convert to GeoDataFrame\ngeometry = gpd.points_from_xy(recent.longitude, recent.latitude)\ngeo_df = gpd.GeoDataFrame(recent, geometry=geometry, crs='EPSG:4326')\n\n# 4. Analysis\nprint(f\"Total earthquakes in last 7 days: {len(recent)}\")\nprint(f\"Average magnitude: {recent['mag'].mean():.2f}\")\nprint(f\"Largest earthquake: {recent['mag'].max():.2f}\")\n\n# 5. Visualization\n# Static plot\nfig, ax = plt.subplots(figsize=(12, 8))\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld.plot(ax=ax, color='lightgray', edgecolor='black')\ngeo_df.plot(ax=ax, color='red', markersize=geo_df['mag']**2, alpha=0.6)\nplt.title('Recent Earthquakes (M4.5+)')\nplt.show()\n\n# Interactive map\nm = folium.Map(location=[0, 0], zoom_start=2)\nfor idx, row in geo_df.iterrows():\n    folium.CircleMarker(\n        location=[row['latitude'], row['longitude']],\n        radius=row['mag']*2,\n        popup=f\"M{row['mag']} - {row['place']}\",\n        color='red',\n        fill=True\n    ).add_to(m)\nm.save('earthquake_map.html')\n\n# 6. Export Results\ngeo_df.to_csv('processed_earthquakes.csv', index=False)\nprint(\"Analysis complete! Results saved.\")\n\nLast Updated: 2024\nVersion: 1.0\nDisasters Hub Training Guide\nFor additional assistance, contact your hub administrator or visit the 2i2c support portal.",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#table-of-contents",
    "href": "Jupyterhub/jupyterhub-training-guide.html#table-of-contents",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "Introduction\nGetting Started\nJupyterHub Interface Overview\nWorking with Jupyter Notebooks\nData Management\nEnvironment and Package Management\nTerminal and Command Line Access\nCollaboration and Sharing\nResource Management\nBest Practices\nTroubleshooting\nKeyboard Shortcuts\nResources and Links",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#introduction",
    "href": "Jupyterhub/jupyterhub-training-guide.html#introduction",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "JupyterHub is a multi-user server that manages and provides web-based Jupyter notebook environments for multiple users. It allows you to:\n\nAccess powerful computing resources through your web browser\nWrite and execute code in Python, R, Julia, and other languages\nVisualize data with interactive plots and charts\nCollaborate with team members on shared projects\nWork from anywhere without local setup requirements\n\n\n\n\nThe Disasters Hub (https://hub.disasters.2i2c.cloud/) is a specialized JupyterHub instance designed for disaster response and analysis work. It provides:\n\nPre-configured environments for geospatial analysis\nAccess to disaster-related datasets\nCollaboration tools for response teams\nIntegration with cloud storage services\nScalable computing resources\n\n\n\n\n✅ No Installation Required - Everything runs in your browser\n✅ Pre-configured Environments - Common packages already installed\n✅ Persistent Storage - Your work is saved between sessions\n✅ Collaboration Ready - Share notebooks with team members\n✅ Scalable Resources - Access to GPU and high-memory instances when needed",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#getting-started",
    "href": "Jupyterhub/jupyterhub-training-guide.html#getting-started",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "Navigate to the Hub\n\nOpen your web browser (Chrome, Firefox, Safari, or Edge recommended)\nGo to: https://hub.disasters.2i2c.cloud/\nBookmark this URL for easy access\n\nAuthentication\n\nYou’ll see a login screen with authentication options\nCommon authentication methods:\n\nGitHub: Use your GitHub credentials\nGoogle: Use your Google account\nInstitutional Login: Use your organization’s credentials\n\nSelect your authentication method and follow the prompts\n\nFirst-Time Login\n\nAccept terms of service if prompted\nYour home directory will be created automatically\nInitial setup may take 30-60 seconds\n\n\n\n\n\nAfter login, you may be presented with server options:\nServer Options:\n┌─────────────────────────────────────┐\n│ • Small (2 CPU, 4GB RAM)            │\n│ • Medium (4 CPU, 8GB RAM)           │\n│ • Large (8 CPU, 16GB RAM)           │\n│ • GPU Instance (if available)       │\n└─────────────────────────────────────┘\nTips for Server Selection: - Start with Small for basic notebook work - Use Medium for data processing tasks - Choose Large for machine learning or big data - Select GPU only when needed (limited availability)",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#jupyterhub-interface-overview",
    "href": "Jupyterhub/jupyterhub-training-guide.html#jupyterhub-interface-overview",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "Once logged in, you’ll see the JupyterLab interface:\n┌──────────────────────────────────────────────────────────┐\n│ [File] [Edit] [View] [Run] [Kernel] [Tabs] [Settings]    │\n├──────────────────────────────────────────────────────────┤\n│ 📁 File Browser │          Main Work Area                | \n│ ├── 📂 data     │                                        │\n│ ├── 📂 notebooks│      [Launcher Tab]                    │\n│ ├── 📂 scripts  │      • Notebook (Python 3)             │\n│ └── 📄 README   │      • Console                         │\n│                 │      • Terminal                        |\n│ [+] New         │      • Text File                       │\n└──────────────────────────────────────────────────────────┘\n\n\n\n\nTop Menu Bar\n\nFile operations, editing, running code\nKernel management\nView options and settings\n\nLeft Sidebar\n\nFile Browser (📁): Navigate and manage files\nRunning Terminals and Kernels (▶): Monitor active sessions\nCommand Palette (🔧): Access all commands\nExtension Manager (🧩): Add functionality\n\nMain Work Area\n\nMultiple tabs for notebooks, terminals, and files\nDrag tabs to rearrange or create split views\nRight-click tabs for additional options\n\nStatus Bar\n\nCurrent kernel status\nLine/column position\nFile encoding and type\n\n\n\n\n\n\nClick the Python 3 icon in the Launcher\nOr: File → New → Notebook\nSelect kernel (usually Python 3)\nRename your notebook: Right-click on “Untitled.ipynb” → Rename",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#working-with-jupyter-notebooks",
    "href": "Jupyterhub/jupyterhub-training-guide.html#working-with-jupyter-notebooks",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "A Jupyter notebook consists of cells that can contain: - Code: Executable Python (or other language) code - Markdown: Formatted text, equations, and images - Raw: Unformatted text\n\n\n\n\n\n\nRun current cell: Shift + Enter (run and move to next)\nRun current cell in place: Ctrl + Enter (stay in cell)\nRun all cells: Menu → Run → Run All Cells\n\n\n\n\n# Code Cell Example\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv('data.csv')\ndata.head()\n# Markdown Cell Example\n## Analysis Results\n- **Finding 1**: Data shows increasing trend\n- **Finding 2**: Correlation coefficient: 0.85\n\n$$E = mc^2$$  # LaTeX equation\n\n\n\n\nInsert cell above: A (in command mode)\nInsert cell below: B (in command mode)\nDelete cell: DD (press D twice in command mode)\nCopy cell: C (in command mode)\nPaste cell: V (in command mode)\nUndo deletion: Z (in command mode)\n\n\n\n\n\nThe kernel is the computational engine that executes your code.\n\n\n\nRestart kernel: Kernel → Restart\nRestart and clear output: Kernel → Restart & Clear Output\nRestart and run all: Kernel → Restart & Run All\nInterrupt execution: Kernel → Interrupt (or I,I in command mode)\nChange kernel: Kernel → Change Kernel\n\n\n\n\n\n○: Kernel idle\n●: Kernel busy\n[*]: Cell currently executing\n[1]: Cell execution number\n\n\n\n\n\n\nUse meaningful cell divisions\n\nOne concept or operation per cell\nSeparate imports, data loading, processing, visualization\n\nDocument your work\n# Good practice: Add comments and markdown cells\n# Load disaster response data\ndf = pd.read_csv('disaster_data.csv')\n\n# Data preprocessing\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.dropna()\nClear output before sharing\n\nKernel → Restart & Clear Output\nReduces file size and removes sensitive output",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#data-management",
    "href": "Jupyterhub/jupyterhub-training-guide.html#data-management",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "Drag and drop files directly into the file browser\nUpload button: Click the ⬆ button in the file browser toolbar\nTerminal upload: Use wget or curl in terminal\nwget https://example.com/data.csv\ncurl -O https://example.com/data.zip\n\n\n\n\n\nRight-click file in browser → Download\nFrom notebook:\nfrom IPython.display import FileLink\nFileLink('results.csv')  # Creates downloadable link\n\n\n\n\n\n\n\nimport boto3\nimport pandas as pd\n\n# Read from S3\ndf = pd.read_csv('s3://bucket-name/path/to/file.csv')\n\n# Write to S3\ndf.to_csv('s3://bucket-name/output/results.csv', index=False)\n\n\n\n# Read from GCS\ndf = pd.read_csv('gs://bucket-name/path/to/file.csv')\n\n# Using gsutil in terminal\n!gsutil cp gs://bucket/file.csv ./data/\n\n\n\n\nRecommended directory structure:\nhome/\n├── data/\n│   ├── raw/           # Original, immutable data\n│   ├── processed/     # Cleaned, transformed data\n│   └── external/      # Data from external sources\n├── notebooks/\n│   ├── exploratory/   # Initial explorations\n│   ├── analysis/      # Detailed analysis\n│   └── reports/       # Final reports\n├── scripts/           # Reusable Python scripts\n├── results/           # Output files, figures\n└── requirements.txt   # Package dependencies\n\n\n\n⚠️ Important: Your home directory is persistent, but understand the storage limits:\n\nHome directory: Usually 10-100 GB (persistent)\nShared data: Read-only datasets available to all users\nTemporary storage: /tmp cleared on restart\nBest practice: Store large datasets in cloud storage, not home directory",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#environment-and-package-management",
    "href": "Jupyterhub/jupyterhub-training-guide.html#environment-and-package-management",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "# In a notebook cell\n!pip install package_name\n\n# Install specific version\n!pip install pandas==1.3.0\n\n# Install from requirements file\n!pip install -r requirements.txt\n\n# Install in user directory (if no write permissions)\n!pip install --user package_name\n\n\n\n# In a notebook cell\n!conda install -c conda-forge package_name -y\n\n# Install multiple packages\n!conda install numpy pandas matplotlib -y\n\n# Create new environment\n!conda create -n myenv python=3.9 -y\n!conda activate myenv  # Note: Activation in notebooks is tricky\n\n\n\n\n\n\nimport sys\nprint(sys.executable)  # Python interpreter path\nprint(sys.version)     # Python version\n\n# List installed packages\n!pip list\n!conda list\n\n\n\n# In terminal\npython -m venv myproject\nsource myproject/bin/activate  # Linux/Mac\npip install -r requirements.txt\n\n\n\n\n\nInstall IPython kernel:\npython -m ipykernel install --user --name mykernel --display-name \"My Kernel\"\nList available kernels:\njupyter kernelspec list\nRemove a kernel:\njupyter kernelspec uninstall mykernel",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#terminal-and-command-line-access",
    "href": "Jupyterhub/jupyterhub-training-guide.html#terminal-and-command-line-access",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "From Launcher: Click “Terminal” icon\nFrom menu: File → New → Terminal\nKeyboard shortcut: (varies by setup)\n\n\n\n\n# Navigation\npwd                     # Print working directory\nls -la                  # List files with details\ncd ~/notebooks         # Change directory\n\n# File operations\nmkdir project          # Create directory\ncp file1.txt file2.txt # Copy file\nmv oldname newname     # Move/rename\nrm file.txt           # Delete file (careful!)\n\n# File viewing\ncat file.txt          # Display file contents\nhead -n 10 data.csv   # First 10 lines\ntail -n 10 log.txt    # Last 10 lines\nless large_file.txt   # Page through file\n\n# Process management\nps aux                # List processes\ntop                   # Monitor resources\nkill -9 PID          # Kill process\n\n# Git operations\ngit status\ngit add .\ngit commit -m \"message\"\ngit push\n\n\n\n# Count lines in file\nwc -l data.csv\n\n# View CSV structure\nhead -1 data.csv | tr ',' '\\n' | nl\n\n# Search in files\ngrep \"pattern\" file.txt\ngrep -r \"pattern\" ./directory\n\n# Compress/decompress\nzip archive.zip file1 file2\nunzip archive.zip\ntar -czf archive.tar.gz directory/\ntar -xzf archive.tar.gz",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#collaboration-and-sharing",
    "href": "Jupyterhub/jupyterhub-training-guide.html#collaboration-and-sharing",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "Download notebook: File → Download as → Notebook (.ipynb)\nShare via email, Slack, or file sharing service\nRecipient uploads to their JupyterHub\n\n\n\n\n# Initialize repository\ngit init\ngit add notebook.ipynb\ngit commit -m \"Add analysis notebook\"\ngit remote add origin https://github.com/user/repo.git\ngit push -u origin main\n\n\n\n\nHTML: File → Export Notebook As → HTML\nPDF: File → Export Notebook As → PDF (requires LaTeX)\nPython script: File → Export Notebook As → Python\nMarkdown: File → Export Notebook As → Markdown\n\n\n\n\n\nSome JupyterHub deployments support real-time collaboration:\n\nShare workspace link: Get shareable link from hub admin\nCollaborative editing: Multiple users can edit simultaneously\nSee collaborator cursors: Real-time cursor positions\nChat integration: Built-in chat for discussion\n\n\n\n\n\nClear outputs before committing:\njupyter nbconvert --clear-output notebook.ipynb\nUse .gitignore:\n.ipynb_checkpoints/\n__pycache__/\n*.pyc\n.DS_Store\ndata/  # Don't commit large data files\nNotebook diff tools:\n# Install nbdime for better notebook diffs\npip install nbdime\nnbdime config-git --enable",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#resource-management",
    "href": "Jupyterhub/jupyterhub-training-guide.html#resource-management",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "Your JupyterHub instance has resource limits:\n# Check available resources\nimport psutil\n\n# Memory\nmemory = psutil.virtual_memory()\nprint(f\"Total RAM: {memory.total / 1e9:.2f} GB\")\nprint(f\"Available: {memory.available / 1e9:.2f} GB\")\nprint(f\"Used: {memory.percent}%\")\n\n# CPU\nprint(f\"CPU cores: {psutil.cpu_count()}\")\nprint(f\"CPU usage: {psutil.cpu_percent()}%\")\n\n# Disk\ndisk = psutil.disk_usage('/')\nprint(f\"Disk space: {disk.total / 1e9:.2f} GB\")\nprint(f\"Disk used: {disk.percent}%\")\n\n\n\n\n\n\nInstall Resource Usage extension\nShows real-time memory and CPU usage in status bar\n\n\n\n\n# Real-time resource monitoring\ntop\nhtop  # If installed\n\n# Memory usage\nfree -h\n\n# Disk usage\ndf -h\ndu -sh *  # Directory sizes\n\n\n\n\n\nClear variables when done:\n# Clear specific variable\ndel large_dataframe\n\n# Clear all variables\n%reset -f\n\n# Garbage collection\nimport gc\ngc.collect()\nUse efficient data types:\n# Use categories for strings with few unique values\ndf['category'] = df['category'].astype('category')\n\n# Use smaller numeric types when possible\ndf['count'] = df['count'].astype('int32')  # Instead of int64\nProcess data in chunks:\n# Read large CSV in chunks\nchunk_size = 10000\nfor chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n    process_chunk(chunk)\n\n\n\n\nAlways shut down kernels and terminals when done:\n\nShutdown kernel: Kernel → Shutdown\nClose terminals: Exit or Ctrl+D\nHub Control Panel: File → Hub Control Panel → Stop My Server\nLogout: File → Log Out\n\n⚠️ Important: Idle servers may be automatically culled after a period of inactivity (usually 1-2 hours).",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#best-practices",
    "href": "Jupyterhub/jupyterhub-training-guide.html#best-practices",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "Use consistent naming:\n2024-01-15_earthquake_analysis.ipynb  # Good\nuntitled1.ipynb                       # Bad\nCreate project templates:\n# notebook_template.ipynb\n\n# 1. Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 2. Configuration\npd.set_option('display.max_columns', None)\nplt.style.use('seaborn')\n\n# 3. Data Loading\n\n# 4. Data Exploration\n\n# 5. Analysis\n\n# 6. Results\nDocument dependencies:\n# Generate requirements.txt\n!pip freeze &gt; requirements.txt\n\n\n\n\n\nNever commit credentials:\n# Bad\napi_key = \"sk-abc123def456\"\n\n# Good - Use environment variables\nimport os\napi_key = os.environ.get('API_KEY')\nUse secrets management:\n# Store secrets in .env file\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Access secrets\nsecret = os.getenv('SECRET_KEY')\nBe careful with outputs:\n\nClear cells containing sensitive information\nReview notebooks before sharing\n\n\n\n\n\n\nVectorize operations:\n# Slow\nresults = []\nfor i in range(len(df)):\n    results.append(df.iloc[i]['column'] * 2)\n\n# Fast\nresults = df['column'] * 2\nUse built-in functions:\n# Use pandas/numpy operations instead of loops\ndf['new_col'] = df['col1'] + df['col2']  # Vectorized\nProfile your code:\n%%time  # Time entire cell\n\n%timeit function()  # Time single line\n\n# Detailed profiling\n%load_ext line_profiler\n%lprun -f function_to_profile function_to_profile()",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#troubleshooting",
    "href": "Jupyterhub/jupyterhub-training-guide.html#troubleshooting",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "Check resources: Server might be full\nTry different kernel: Some kernels may be broken\nRestart server: Hub Control Panel → Stop → Start\n\n\n\n\n# Check if package is installed\nimport importlib\nif importlib.util.find_spec(\"package_name\") is None:\n    !pip install package_name\n    \n# Restart kernel after installation\nfrom IPython import get_ipython\nget_ipython().kernel.do_shutdown(True)\n\n\n\n\nClear unnecessary variables: del variable_name\nUse smaller data samples for testing\nRequest larger server instance\nProcess data in chunks\n\n\n\n\n\nCheck disk space: df -h in terminal\nCheck file permissions: ls -la notebook.ipynb\nSave with new name: File → Save As\nDownload backup: File → Download\n\n\n\n\n\nCheck internet connection\nTry different browser\nClear browser cache\nCheck if hub is under maintenance\n\n\n\n\n\n\nBuilt-in help:\nhelp(function_name)\nfunction_name?  # Quick help\nfunction_name??  # Source code\nDocumentation:\n\nJupyterHub docs: https://jupyterhub.readthedocs.io\nJupyterLab docs: https://jupyterlab.readthedocs.io\n2i2c docs: https://docs.2i2c.org\n\nCommunity support:\n\nDiscourse forum\nGitHub issues\nStack Overflow with tags: jupyter, jupyterhub",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#keyboard-shortcuts",
    "href": "Jupyterhub/jupyterhub-training-guide.html#keyboard-shortcuts",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "Press Esc to enter command mode\n\n\n\nShortcut\nAction\n\n\n\n\nEnter\nEnter edit mode\n\n\nA\nInsert cell above\n\n\nB\nInsert cell below\n\n\nD,D\nDelete cell\n\n\nY\nChange to code cell\n\n\nM\nChange to markdown cell\n\n\nShift+Up/Down\nSelect multiple cells\n\n\nShift+M\nMerge selected cells\n\n\nC\nCopy cell\n\n\nX\nCut cell\n\n\nV\nPaste cell below\n\n\nShift+V\nPaste cell above\n\n\nZ\nUndo cell deletion\n\n\n0,0\nRestart kernel\n\n\nI,I\nInterrupt kernel\n\n\n\n\n\n\nPress Enter to enter edit mode\n\n\n\nShortcut\nAction\n\n\n\n\nEsc\nEnter command mode\n\n\nCtrl+Enter\nRun cell\n\n\nShift+Enter\nRun cell, select below\n\n\nAlt+Enter\nRun cell, insert below\n\n\nCtrl+S\nSave notebook\n\n\nTab\nCode completion\n\n\nShift+Tab\nTooltip\n\n\nCtrl+]\nIndent\n\n\nCtrl+[\nDedent\n\n\nCtrl+A\nSelect all\n\n\nCtrl+Z\nUndo\n\n\nCtrl+Y\nRedo\n\n\n\n\n\n\n\n\n\nShortcut\nAction\n\n\n\n\nCtrl+Shift+C\nCommand palette\n\n\nCtrl+B\nToggle left sidebar\n\n\nCtrl+Shift+D\nToggle file browser\n\n\nCtrl+Shift+F\nFind and replace\n\n\nCtrl+Shift+[\nPrevious tab\n\n\nCtrl+Shift+]\nNext tab\n\n\nAlt+W\nClose tab",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#resources-and-links",
    "href": "Jupyterhub/jupyterhub-training-guide.html#resources-and-links",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "JupyterHub Documentation: https://jupyterhub.readthedocs.io\nJupyterLab Documentation: https://jupyterlab.readthedocs.io\nJupyter Notebook Documentation: https://jupyter-notebook.readthedocs.io\n2i2c Infrastructure Guide: https://docs.2i2c.org\n\n\n\n\n\nJupyter Tutorial: https://jupyter.org/try\nReal Python Jupyter Guide: https://realpython.com/jupyter-notebook-introduction/\nDataCamp Jupyter Tutorial: https://www.datacamp.com/tutorial/tutorial-jupyter-notebook\nOfficial Jupyter Examples: https://github.com/jupyter/jupyter/wiki/Gallery-of-Jupyter-Notebooks\n\n\n\n\n\nNASA Disasters Program: https://disasters.nasa.gov\nUSGS Hazards Data: https://www.usgs.gov/natural-hazards\nNOAA Disaster Data: https://www.ncdc.noaa.gov/billions/\nCopernicus Emergency Management: https://emergency.copernicus.eu\n\n\n\n\n# Geospatial analysis\nimport geopandas as gpd\nimport rasterio\nimport xarray as xr\nimport folium\n\n# Data processing\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# Machine learning\nfrom sklearn import *\nimport tensorflow as tf\nimport torch\n\n# Earth observation\nimport ee  # Google Earth Engine\nimport planetary_computer as pc\nimport pystac_client\n\n\n\nInstall JupyterLab extensions for enhanced functionality:\n# Variable inspector\njupyter labextension install @lckr/jupyterlab_variableinspector\n\n# Table of contents\njupyter labextension install @jupyterlab/toc\n\n# Git integration\npip install jupyterlab-git\n\n# Code formatter\npip install jupyterlab-code-formatter\n\n\n\n\nJupyter Discourse Forum: https://discourse.jupyter.org\nStack Overflow: https://stackoverflow.com/questions/tagged/jupyter\nGitHub Issues: https://github.com/jupyterhub/jupyterhub/issues\n2i2c Support: https://2i2c.org/support\nGitter Chat: https://gitter.im/jupyterhub/jupyterhub\n\n\n\n\n\nJupyterLab Cheat Sheet: https://www.datacamp.com/cheat-sheet/jupyterlab-cheat-sheet\nJupyter Shortcuts PDF: https://www.cheatography.com/weidadeyue/cheat-sheets/jupyter-notebook/\nMarkdown Guide: https://www.markdownguide.org/cheat-sheet/",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "Jupyterhub/jupyterhub-training-guide.html#appendix-sample-workflow",
    "href": "Jupyterhub/jupyterhub-training-guide.html#appendix-sample-workflow",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "Here’s a complete example workflow for disaster analysis:\n# 1. Setup and Imports\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport folium\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 2. Load Data\n# Earthquake data\nearthquakes = pd.read_csv('https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_month.csv')\nearthquakes['time'] = pd.to_datetime(earthquakes['time'])\n\n# 3. Data Processing\n# Filter recent events\nrecent = earthquakes[earthquakes['time'] &gt; datetime.now() - timedelta(days=7)]\n\n# Convert to GeoDataFrame\ngeometry = gpd.points_from_xy(recent.longitude, recent.latitude)\ngeo_df = gpd.GeoDataFrame(recent, geometry=geometry, crs='EPSG:4326')\n\n# 4. Analysis\nprint(f\"Total earthquakes in last 7 days: {len(recent)}\")\nprint(f\"Average magnitude: {recent['mag'].mean():.2f}\")\nprint(f\"Largest earthquake: {recent['mag'].max():.2f}\")\n\n# 5. Visualization\n# Static plot\nfig, ax = plt.subplots(figsize=(12, 8))\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld.plot(ax=ax, color='lightgray', edgecolor='black')\ngeo_df.plot(ax=ax, color='red', markersize=geo_df['mag']**2, alpha=0.6)\nplt.title('Recent Earthquakes (M4.5+)')\nplt.show()\n\n# Interactive map\nm = folium.Map(location=[0, 0], zoom_start=2)\nfor idx, row in geo_df.iterrows():\n    folium.CircleMarker(\n        location=[row['latitude'], row['longitude']],\n        radius=row['mag']*2,\n        popup=f\"M{row['mag']} - {row['place']}\",\n        color='red',\n        fill=True\n    ).add_to(m)\nm.save('earthquake_map.html')\n\n# 6. Export Results\ngeo_df.to_csv('processed_earthquakes.csv', index=False)\nprint(\"Analysis complete! Results saved.\")\n\nLast Updated: 2024\nVersion: 1.0\nDisasters Hub Training Guide\nFor additional assistance, contact your hub administrator or visit the 2i2c support portal.",
    "crumbs": [
      "JupyterHub",
      "JupyterHub Training Guide - Disasters Hub"
    ]
  },
  {
    "objectID": "GitHub/setup.html",
    "href": "GitHub/setup.html",
    "title": "GitHub Setup and Installation",
    "section": "",
    "text": "Prerequisites & System Setup\nGit Installation\nGitHub Account Setup\nGitHub CLI Installation & Authentication\n\n\n\n\n\n\n\n\nmacOS 10.15 (Catalina) or later\nAdministrator access to install software\nInternet connection\nTerminal application (built into macOS)\n\n\n\n\n\nText Editor: VS Code, Sublime Text, or vim\nTerminal: iTerm2 or built-in Terminal app\nGit GUI (optional): SourceTree, GitHub Desktop, or GitKraken\n\n\n\n\n\n\n\n\n# Install Homebrew if not already installed\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install Git\nbrew install git\n\n# Verify installation\ngit --version\n\n\n\n# This will prompt to install Xcode Command Line Tools\ngit --version\n\n# Follow the prompts to complete installation\n\n\n\n\nVisit https://git-scm.com/download/mac\nDownload the installer\nRun the installer package\nVerify: git --version\n\n\n\n\n# Set your name (visible in commits)\ngit config --global user.name \"Your Name\"\n\n# Set your email (should match GitHub account)\ngit config --global user.email \"your.email@example.com\"\n\n# Set default branch name to 'main'\ngit config --global init.defaultBranch main\n\n# Set default editor (optional)\ngit config --global core.editor \"code --wait\"  # For VS Code\n# git config --global core.editor \"vim\"        # For vim\n# git config --global core.editor \"nano\"       # For nano\n\n# Enable color output\ngit config --global color.ui auto\n\n# View all settings\ngit config --list\n\n\n\n\n\n\n\n\nVisit https://github.com\nClick “Sign up” in the top right\nEnter your details:\n\nUsername: Choose wisely - this is permanent and public\nEmail: Use a professional email address\nPassword: Use a strong, unique password\n\nVerify your email address\nComplete the profile setup\n\n\n\n\n\nEnable Two-Factor Authentication (2FA):\n\nGo to Settings → Security\nClick “Enable two-factor authentication”\nUse an authenticator app (Google Authenticator, Authy)\nSave backup codes securely\n\nAdd SSH Key (recommended for secure authentication):\n\n# Generate SSH key\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Press Enter for default location\n# Set a passphrase (recommended)\n\n# Start SSH agent\neval \"$(ssh-agent -s)\"\n\n# Add SSH key to agent\nssh-add ~/.ssh/id_ed25519\n\n# Copy public key to clipboard\npbcopy &lt; ~/.ssh/id_ed25519.pub\n\nAdd SSH Key to GitHub:\n\nGo to Settings → SSH and GPG keys\nClick “New SSH key”\nPaste your key and give it a descriptive title\nClick “Add SSH key”\n\nTest SSH connection:\n\nssh -T git@github.com\n# You should see: \"Hi username! You've successfully authenticated...\"\n\n\n\n\nGo to Settings → Developer settings → Personal access tokens → Tokens (classic)\nClick “Generate new token”\nSet expiration and select scopes (at minimum: repo, workflow)\nCopy the token immediately (you won’t see it again)\nUse this token as your password when prompted by Git\n\n\n\n\n\n\n\n\n# Install via Homebrew\nbrew install gh\n\n# Verify installation\ngh --version\n\n\n\n# Start authentication process\ngh auth login\n\n# Follow the prompts:\n# 1. Choose GitHub.com\n# 2. Choose HTTPS or SSH (SSH recommended if you've set it up)\n# 3. Authenticate via web browser or paste authentication token\n# 4. Choose default git protocol (ssh recommended)\n\n# Verify authentication\ngh auth status\n\n\n\n# Set default editor\ngh config set editor \"code --wait\"  # For VS Code\n\n# Set default browser\ngh config set browser safari\n\n# View current configuration\ngh config list",
    "crumbs": [
      "GitHub",
      "GitHub Setup and Installation"
    ]
  },
  {
    "objectID": "GitHub/commands.html",
    "href": "GitHub/commands.html",
    "title": "Basic Uses",
    "section": "",
    "text": "Setting Up Your First Repository\nEssential Git Commands\nGitHub CLI Essentials\nCommon Workflows\nBest Practices\nTroubleshooting\n\n\n\n\n\n\n\n# Using HTTPS\ngit clone https://github.com/username/repository.git\n\n# Using SSH (recommended if configured)\ngit clone git@github.com:username/repository.git\n\n# Using GitHub CLI\ngh repo clone username/repository\n\n# Clone into specific directory\ngit clone git@github.com:username/repository.git my-project\n\n\n\n\n\n\nClick the “+” icon → “New repository”\nEnter repository name\nAdd description (optional)\nChoose public or private\nInitialize with README (recommended)\nAdd .gitignore (select template)\nChoose a license\nClick “Create repository”\n\n\n\n\n# Create a new repository on GitHub\ngh repo create my-project --public --clone\n\n# With more options\ngh repo create my-project \\\n  --public \\\n  --description \"My awesome project\" \\\n  --clone \\\n  --add-readme \\\n  --license mit \\\n  --gitignore Python\n\n\n\n\n# Navigate to your project\ncd my-existing-project\n\n# Initialize git repository\ngit init\n\n# Add all files\ngit add .\n\n# Create initial commit\ngit commit -m \"Initial commit\"\n\n# Create repository on GitHub\ngh repo create my-project --source=. --public --push\n\n# Or manually add remote and push\ngit remote add origin git@github.com:username/my-project.git\ngit branch -M main\ngit push -u origin main\n\n\n\n\n\n\n\n# Check Git version\ngit --version\n\n# Get help\ngit help &lt;command&gt;\ngit &lt;command&gt; --help\n\n# Initialize repository\ngit init\n\n# Clone repository\ngit clone &lt;url&gt;\n\n# Check status\ngit status\n\n# View commit history\ngit log\ngit log --oneline\ngit log --graph --oneline --all\n\n\n\n# Add files to staging area\ngit add &lt;file&gt;\ngit add .                    # Add all files\ngit add *.js                 # Add all JavaScript files\ngit add -p                   # Interactive staging\n\n# Remove files from staging\ngit reset HEAD &lt;file&gt;\ngit restore --staged &lt;file&gt;  # Git 2.23+\n\n# Commit changes\ngit commit -m \"Commit message\"\ngit commit -am \"Message\"     # Add and commit (tracked files only)\ngit commit --amend           # Amend last commit\n\n# View differences\ngit diff                     # Unstaged changes\ngit diff --staged           # Staged changes\ngit diff HEAD~1             # Changes since last commit\n\n\n\n# List branches\ngit branch                   # Local branches\ngit branch -r               # Remote branches\ngit branch -a               # All branches\n\n# Create branch\ngit branch &lt;branch-name&gt;\ngit checkout -b &lt;branch-name&gt;  # Create and switch\ngit switch -c &lt;branch-name&gt;    # Git 2.23+ (create and switch)\n\n# Switch branches\ngit checkout &lt;branch-name&gt;\ngit switch &lt;branch-name&gt;       # Git 2.23+\n\n# Merge branch\ngit merge &lt;branch-name&gt;\n\n# Delete branch\ngit branch -d &lt;branch-name&gt;    # Safe delete\ngit branch -D &lt;branch-name&gt;    # Force delete\n\n# Rename branch\ngit branch -m &lt;old-name&gt; &lt;new-name&gt;\n\n\n\n# View remotes\ngit remote -v\n\n# Add remote\ngit remote add &lt;name&gt; &lt;url&gt;\ngit remote add origin git@github.com:username/repo.git\n\n# Remove remote\ngit remote remove &lt;name&gt;\n\n# Rename remote\ngit remote rename &lt;old&gt; &lt;new&gt;\n\n# Fetch changes\ngit fetch\ngit fetch origin\n\n# Pull changes\ngit pull\ngit pull origin main\n\n# Push changes\ngit push\ngit push origin main\ngit push -u origin main      # Set upstream\ngit push --force             # Force push (use with caution!)\n\n\n\n# Save changes temporarily\ngit stash\ngit stash save \"Work in progress\"\n\n# List stashes\ngit stash list\n\n# Apply stash\ngit stash apply              # Apply most recent\ngit stash apply stash@{0}   # Apply specific stash\n\n# Apply and remove stash\ngit stash pop\n\n# Remove stash\ngit stash drop stash@{0}\n\n# Clear all stashes\ngit stash clear\n\n\n\n# Discard changes in working directory\ngit checkout -- &lt;file&gt;\ngit restore &lt;file&gt;           # Git 2.23+\n\n# Unstage files\ngit reset HEAD &lt;file&gt;\ngit restore --staged &lt;file&gt;  # Git 2.23+\n\n# Reset to previous commit (keeping changes)\ngit reset --soft HEAD~1\n\n# Reset to previous commit (discard changes)\ngit reset --hard HEAD~1\n\n# Revert a commit (creates new commit)\ngit revert &lt;commit-hash&gt;\n\n\n\n# List tags\ngit tag\n\n# Create tag\ngit tag v1.0.0\ngit tag -a v1.0.0 -m \"Version 1.0.0\"  # Annotated tag\n\n# Push tags\ngit push origin v1.0.0\ngit push origin --tags       # Push all tags\n\n# Delete tag\ngit tag -d v1.0.0           # Local\ngit push origin :v1.0.0     # Remote\n\n\n\n\n\n\n\n# Set default repository\ngh repo set-default\n# Select from list or specify:\ngh repo set-default owner/repo\n\n# View repository\ngh repo view\ngh repo view owner/repo\n\n# Fork repository\ngh repo fork owner/repo\n\n# Create repository\ngh repo create my-repo --public --clone\n\n# Delete repository (use with caution!)\ngh repo delete owner/repo\n\n# Clone repository\ngh repo clone owner/repo\n\n# List repositories\ngh repo list\ngh repo list owner\n\n\n\n# Create pull request\ngh pr create\ngh pr create --title \"Feature X\" --body \"Description\"\ngh pr create --fill  # Use commit messages for title/body\ngh pr create --draft # Create as draft\ngh pr create --assignee @me --label bug,enhancement\n\n# List pull requests\ngh pr list\ngh pr list --state all\ngh pr list --author @me\n\n# View pull request\ngh pr view\ngh pr view 123\n\n# Checkout pull request\ngh pr checkout 123\n\n# Merge pull request\ngh pr merge 123\ngh pr merge 123 --merge    # Create merge commit\ngh pr merge 123 --rebase   # Rebase and merge\ngh pr merge 123 --squash   # Squash and merge\n\n# Close pull request\ngh pr close 123\n\n# Review pull request\ngh pr review 123 --approve\ngh pr review 123 --request-changes\ngh pr review 123 --comment\n\n# Check pull request status\ngh pr status\ngh pr checks 123\n\n\n\n# Create issue\ngh issue create\ngh issue create --title \"Bug report\" --body \"Description\"\n\n# List issues\ngh issue list\ngh issue list --assignee @me\ngh issue list --label bug\n\n# View issue\ngh issue view 123\n\n# Close issue\ngh issue close 123\n\n# Reopen issue\ngh issue reopen 123\n\n# Comment on issue\ngh issue comment 123 --body \"This is fixed\"\n\n\n\n# List workflows\ngh workflow list\n\n# View workflow runs\ngh run list\ngh run view\n\n# Watch workflow run\ngh run watch\n\n# Download artifacts\ngh run download\n\n# Trigger workflow\ngh workflow run &lt;workflow-name&gt;\n\n\n\n# Create gist\ngh gist create file.txt\ngh gist create --public file.txt\n\n# List gists\ngh gist list\n\n# View gist\ngh gist view &lt;id&gt;\n\n# Edit gist\ngh gist edit &lt;id&gt;\n\n\n\n\n\n\n\n# 1. Start your day - sync with remote\ngit pull origin main\n\n# 2. Create feature branch\ngit checkout -b feature/new-feature\n\n# 3. Make changes and commit\ngit add .\ngit commit -m \"Add new feature\"\n\n# 4. Push to remote\ngit push -u origin feature/new-feature\n\n# 5. Create pull request\ngh pr create --fill\n\n# 6. After PR is merged, clean up\ngit checkout main\ngit pull origin main\ngit branch -d feature/new-feature\n\n\n\n# 1. Pull latest changes\ngit pull origin main\n\n# 2. If conflicts occur, Git will notify you\n# 3. Open conflicted files and resolve manually\n# Look for conflict markers:\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# Your changes\n# =======\n# Their changes\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; branch-name\n\n# 4. After resolving, add the files\ngit add &lt;resolved-files&gt;\n\n# 5. Complete the merge\ngit commit -m \"Resolve merge conflicts\"\n\n# 6. Push changes\ngit push origin &lt;branch&gt;\n\n\n\n# 1. Add upstream remote (one time)\ngit remote add upstream https://github.com/original-owner/repo.git\n\n# 2. Fetch upstream changes\ngit fetch upstream\n\n# 3. Checkout main branch\ngit checkout main\n\n# 4. Merge upstream changes\ngit merge upstream/main\n\n# 5. Push to your fork\ngit push origin main\n\n# Using GitHub CLI\ngh repo sync owner/repo -b main\n\n\n\n# Interactive rebase for last 3 commits\ngit rebase -i HEAD~3\n\n# In the editor:\n# Change 'pick' to 'squash' for commits to combine\n# Save and close\n\n# Force push (if already pushed)\ngit push --force-with-lease origin &lt;branch&gt;\n\n\n\n# Apply specific commit to current branch\ngit cherry-pick &lt;commit-hash&gt;\n\n# Cherry-pick multiple commits\ngit cherry-pick &lt;hash1&gt; &lt;hash2&gt; &lt;hash3&gt;\n\n# Cherry-pick range\ngit cherry-pick &lt;oldest-hash&gt;^..&lt;newest-hash&gt;\n\n\n\n\n\n\n\nThe Seven Rules of Great Commit Messages:\n\nSeparate subject from body with blank line\nLimit subject line to 50 characters\nCapitalize the subject line\nDon’t end subject line with period\nUse imperative mood (“Add feature” not “Added feature”)\nWrap body at 72 characters\nExplain what and why, not how\n\nExample:\nAdd user authentication feature\n\nImplement OAuth 2.0 authentication using GitHub as provider.\nThis allows users to sign in with their GitHub credentials\ninstead of creating separate accounts.\n\nResolves: #123\nSee also: #456, #789\n\n\n\nfeature/add-login-page\nbugfix/fix-navigation-menu\nhotfix/security-patch\nrelease/v2.0.0\ndocs/update-readme\ntest/add-unit-tests\nrefactor/optimize-database\n\n\n\nCreate a .gitignore file in your repository root:\n# macOS\n.DS_Store\n.AppleDouble\n.LSOverride\n\n# IDE\n.vscode/\n.idea/\n*.swp\n*.swo\n\n# Dependencies\nnode_modules/\nvendor/\n.env\n\n# Build outputs\ndist/\nbuild/\n*.log\n\n# Sensitive data\n*.pem\n*.key\n.env.local\nconfig/secrets.yml\n\n\n\n\nNever commit sensitive data:\n\nPasswords, API keys, tokens\nPrivate keys or certificates\nDatabase credentials\n.env files with secrets\n\nIf you accidentally commit secrets:\n# Remove from history (requires force push)\ngit filter-branch --force --index-filter \\\n  \"git rm --cached --ignore-unmatch path/to/file\" \\\n  --prune-empty --tag-name-filter cat -- --all\n\n# Or use BFG Repo-Cleaner (easier)\nbrew install bfg\nbfg --delete-files file-with-secrets.txt\nUse GitHub’s security features:\n\nEnable Dependabot alerts\nEnable secret scanning\nUse protected branches\nRequire PR reviews\n\n\n\n\n\n\nAlways work in branches - Never commit directly to main\nKeep PRs small - Easier to review and less likely to have conflicts\nWrite descriptive PR descriptions - Include what, why, and how\nReview others’ code - Learn and help maintain quality\nUpdate documentation - Keep README and docs current\nTest before pushing - Run tests locally first\nCommunicate - Use issues and PR comments effectively\n\n\n\n\n\n\n\n\n\n\n# Check SSH key is added\nssh-add -l\n\n# Add SSH key\nssh-add ~/.ssh/id_ed25519\n\n# Test connection\nssh -T git@github.com\n\n\n\n# Pull first, then push\ngit pull origin main --rebase\ngit push origin main\n\n# Or force push (careful!)\ngit push --force-with-lease\n\n\n\n# Create new branch with current commits\ngit branch new-branch\n\n# Reset original branch\ngit reset --hard HEAD~3  # Go back 3 commits\n\n# Switch to new branch\ngit checkout new-branch\n\n\n\n# Keep changes, undo commit\ngit reset --soft HEAD~1\n\n# Discard changes completely\ngit reset --hard HEAD~1\n\n\n\n# Install Git LFS\nbrew install git-lfs\ngit lfs install\n\n# Track large files\ngit lfs track \"*.psd\"\ngit add .gitattributes\ngit add large-file.psd\ngit commit -m \"Add large file with LFS\"\n\n\n\n# Update your branch\ngit checkout main\ngit pull origin main\ngit checkout your-branch\ngit rebase main\n\n# Resolve conflicts, then\ngit add .\ngit rebase --continue\ngit push --force-with-lease\n\n\n\n\n\n\n\n\n[alias]\n    st = status\n    co = checkout\n    ci = commit\n    br = branch\n    df = diff\n    lg = log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit\n    last = log -1 HEAD\n    unstage = reset HEAD --\n    amend = commit --amend\n    branches = branch -a\n    remotes = remote -v\n    contributors = shortlog --summary --numbered\n\n\n\n\nCmd + Shift + P → Git commands\nCtrl + Shift + G → Source control panel\nCmd + Enter → Commit staged changes\nOption + Cmd + Enter → Commit all changes\n\n\n\n\n# Git shortcuts\nalias g='git'\nalias gs='git status'\nalias ga='git add'\nalias gc='git commit -m'\nalias gp='git push'\nalias gpl='git pull'\nalias gco='git checkout'\nalias gb='git branch'\nalias glog='git log --oneline --graph --all'\n\n# GitHub CLI shortcuts\nalias ghr='gh repo'\nalias ghpr='gh pr'\nalias ghi='gh issue'",
    "crumbs": [
      "GitHub",
      "Basic Uses"
    ]
  },
  {
    "objectID": "GitHub/resources.html",
    "href": "GitHub/resources.html",
    "title": "NASA Disasters Documentation",
    "section": "",
    "text": "Git Documentation: https://git-scm.com/doc\nGitHub Docs: https://docs.github.com\nGitHub CLI Manual: https://cli.github.com/manual\nGitHub Learning Lab: https://lab.github.com\n\n\n\n\n\nLearn Git Branching: https://learngitbranching.js.org\nGitHub Skills: https://skills.github.com\nAtlassian Git Tutorial: https://www.atlassian.com/git/tutorials\nOh My Git! (Game): https://ohmygit.org\n\n\n\n\n\nGitHub Git Cheat Sheet: https://education.github.com/git-cheat-sheet-education.pdf\nInteractive Git Cheat Sheet: https://ndpsoftware.com/git-cheatsheet.html\nGitHub CLI Cheat Sheet: https://github.com/cli/cli#commands\n\n\n\n\n\nPro Git Book (Free): https://git-scm.com/book\nGit Flow: https://nvie.com/posts/a-successful-git-branching-model\nConventional Commits: https://www.conventionalcommits.org\nSemantic Versioning: https://semver.org\n\n\n\n\n\nGitHub Desktop: https://desktop.github.com\nSourceTree: https://www.sourcetreeapp.com\nGitKraken: https://www.gitkraken.com\nTower: https://www.git-tower.com\n\n\n\n\n\nGitLens: Enhanced Git capabilities\nGit Graph: Visualize branch structure\nGitHub Pull Requests: Manage PRs from VS Code\nGit History: View and search git log\n\n\n\n\n\nGitHub Status: https://www.githubstatus.com\nStack Overflow Git Tag: https://stackoverflow.com/questions/tagged/git\nGitHub Community Forum: https://github.community\n\n\n\n\n\nGitHub YouTube: https://youtube.com/github\nThe Net Ninja Git Tutorial: Comprehensive video series\nTraversy Media Git Crash Course: Quick overview\n\n\n\n\n\nGitHub Flavored Markdown: https://github.github.com/gfm\nMarkdown Guide: https://www.markdownguide.org\nShields.io (Badges): https://shields.io",
    "crumbs": [
      "GitHub",
      "Resources & Links"
    ]
  },
  {
    "objectID": "GitHub/setup.html#table-of-contents",
    "href": "GitHub/setup.html#table-of-contents",
    "title": "GitHub Setup and Installation",
    "section": "",
    "text": "Prerequisites & System Setup\nGit Installation\nGitHub Account Setup\nGitHub CLI Installation & Authentication",
    "crumbs": [
      "GitHub",
      "GitHub Setup and Installation"
    ]
  },
  {
    "objectID": "GitHub/setup.html#prerequisites-system-setup",
    "href": "GitHub/setup.html#prerequisites-system-setup",
    "title": "GitHub Setup and Installation",
    "section": "",
    "text": "macOS 10.15 (Catalina) or later\nAdministrator access to install software\nInternet connection\nTerminal application (built into macOS)\n\n\n\n\n\nText Editor: VS Code, Sublime Text, or vim\nTerminal: iTerm2 or built-in Terminal app\nGit GUI (optional): SourceTree, GitHub Desktop, or GitKraken",
    "crumbs": [
      "GitHub",
      "GitHub Setup and Installation"
    ]
  },
  {
    "objectID": "GitHub/setup.html#git-installation",
    "href": "GitHub/setup.html#git-installation",
    "title": "GitHub Setup and Installation",
    "section": "",
    "text": "# Install Homebrew if not already installed\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n# Install Git\nbrew install git\n\n# Verify installation\ngit --version\n\n\n\n# This will prompt to install Xcode Command Line Tools\ngit --version\n\n# Follow the prompts to complete installation\n\n\n\n\nVisit https://git-scm.com/download/mac\nDownload the installer\nRun the installer package\nVerify: git --version\n\n\n\n\n# Set your name (visible in commits)\ngit config --global user.name \"Your Name\"\n\n# Set your email (should match GitHub account)\ngit config --global user.email \"your.email@example.com\"\n\n# Set default branch name to 'main'\ngit config --global init.defaultBranch main\n\n# Set default editor (optional)\ngit config --global core.editor \"code --wait\"  # For VS Code\n# git config --global core.editor \"vim\"        # For vim\n# git config --global core.editor \"nano\"       # For nano\n\n# Enable color output\ngit config --global color.ui auto\n\n# View all settings\ngit config --list",
    "crumbs": [
      "GitHub",
      "GitHub Setup and Installation"
    ]
  },
  {
    "objectID": "GitHub/setup.html#github-account-setup",
    "href": "GitHub/setup.html#github-account-setup",
    "title": "GitHub Setup and Installation",
    "section": "",
    "text": "Visit https://github.com\nClick “Sign up” in the top right\nEnter your details:\n\nUsername: Choose wisely - this is permanent and public\nEmail: Use a professional email address\nPassword: Use a strong, unique password\n\nVerify your email address\nComplete the profile setup\n\n\n\n\n\nEnable Two-Factor Authentication (2FA):\n\nGo to Settings → Security\nClick “Enable two-factor authentication”\nUse an authenticator app (Google Authenticator, Authy)\nSave backup codes securely\n\nAdd SSH Key (recommended for secure authentication):\n\n# Generate SSH key\nssh-keygen -t ed25519 -C \"your.email@example.com\"\n\n# Press Enter for default location\n# Set a passphrase (recommended)\n\n# Start SSH agent\neval \"$(ssh-agent -s)\"\n\n# Add SSH key to agent\nssh-add ~/.ssh/id_ed25519\n\n# Copy public key to clipboard\npbcopy &lt; ~/.ssh/id_ed25519.pub\n\nAdd SSH Key to GitHub:\n\nGo to Settings → SSH and GPG keys\nClick “New SSH key”\nPaste your key and give it a descriptive title\nClick “Add SSH key”\n\nTest SSH connection:\n\nssh -T git@github.com\n# You should see: \"Hi username! You've successfully authenticated...\"\n\n\n\n\nGo to Settings → Developer settings → Personal access tokens → Tokens (classic)\nClick “Generate new token”\nSet expiration and select scopes (at minimum: repo, workflow)\nCopy the token immediately (you won’t see it again)\nUse this token as your password when prompted by Git",
    "crumbs": [
      "GitHub",
      "GitHub Setup and Installation"
    ]
  },
  {
    "objectID": "GitHub/setup.html#github-cli-installation-authentication",
    "href": "GitHub/setup.html#github-cli-installation-authentication",
    "title": "GitHub Setup and Installation",
    "section": "",
    "text": "# Install via Homebrew\nbrew install gh\n\n# Verify installation\ngh --version\n\n\n\n# Start authentication process\ngh auth login\n\n# Follow the prompts:\n# 1. Choose GitHub.com\n# 2. Choose HTTPS or SSH (SSH recommended if you've set it up)\n# 3. Authenticate via web browser or paste authentication token\n# 4. Choose default git protocol (ssh recommended)\n\n# Verify authentication\ngh auth status\n\n\n\n# Set default editor\ngh config set editor \"code --wait\"  # For VS Code\n\n# Set default browser\ngh config set browser safari\n\n# View current configuration\ngh config list",
    "crumbs": [
      "GitHub",
      "GitHub Setup and Installation"
    ]
  },
  {
    "objectID": "GitHub/commands.html#table-of-contents",
    "href": "GitHub/commands.html#table-of-contents",
    "title": "Basic Uses",
    "section": "",
    "text": "Setting Up Your First Repository\nEssential Git Commands\nGitHub CLI Essentials\nCommon Workflows\nBest Practices\nTroubleshooting",
    "crumbs": [
      "GitHub",
      "Basic Uses"
    ]
  },
  {
    "objectID": "GitHub/commands.html#setting-up-your-first-repository",
    "href": "GitHub/commands.html#setting-up-your-first-repository",
    "title": "Basic Uses",
    "section": "",
    "text": "# Using HTTPS\ngit clone https://github.com/username/repository.git\n\n# Using SSH (recommended if configured)\ngit clone git@github.com:username/repository.git\n\n# Using GitHub CLI\ngh repo clone username/repository\n\n# Clone into specific directory\ngit clone git@github.com:username/repository.git my-project\n\n\n\n\n\n\nClick the “+” icon → “New repository”\nEnter repository name\nAdd description (optional)\nChoose public or private\nInitialize with README (recommended)\nAdd .gitignore (select template)\nChoose a license\nClick “Create repository”\n\n\n\n\n# Create a new repository on GitHub\ngh repo create my-project --public --clone\n\n# With more options\ngh repo create my-project \\\n  --public \\\n  --description \"My awesome project\" \\\n  --clone \\\n  --add-readme \\\n  --license mit \\\n  --gitignore Python\n\n\n\n\n# Navigate to your project\ncd my-existing-project\n\n# Initialize git repository\ngit init\n\n# Add all files\ngit add .\n\n# Create initial commit\ngit commit -m \"Initial commit\"\n\n# Create repository on GitHub\ngh repo create my-project --source=. --public --push\n\n# Or manually add remote and push\ngit remote add origin git@github.com:username/my-project.git\ngit branch -M main\ngit push -u origin main",
    "crumbs": [
      "GitHub",
      "Basic Uses"
    ]
  },
  {
    "objectID": "GitHub/commands.html#essential-git-commands",
    "href": "GitHub/commands.html#essential-git-commands",
    "title": "Basic Uses",
    "section": "",
    "text": "# Check Git version\ngit --version\n\n# Get help\ngit help &lt;command&gt;\ngit &lt;command&gt; --help\n\n# Initialize repository\ngit init\n\n# Clone repository\ngit clone &lt;url&gt;\n\n# Check status\ngit status\n\n# View commit history\ngit log\ngit log --oneline\ngit log --graph --oneline --all\n\n\n\n# Add files to staging area\ngit add &lt;file&gt;\ngit add .                    # Add all files\ngit add *.js                 # Add all JavaScript files\ngit add -p                   # Interactive staging\n\n# Remove files from staging\ngit reset HEAD &lt;file&gt;\ngit restore --staged &lt;file&gt;  # Git 2.23+\n\n# Commit changes\ngit commit -m \"Commit message\"\ngit commit -am \"Message\"     # Add and commit (tracked files only)\ngit commit --amend           # Amend last commit\n\n# View differences\ngit diff                     # Unstaged changes\ngit diff --staged           # Staged changes\ngit diff HEAD~1             # Changes since last commit\n\n\n\n# List branches\ngit branch                   # Local branches\ngit branch -r               # Remote branches\ngit branch -a               # All branches\n\n# Create branch\ngit branch &lt;branch-name&gt;\ngit checkout -b &lt;branch-name&gt;  # Create and switch\ngit switch -c &lt;branch-name&gt;    # Git 2.23+ (create and switch)\n\n# Switch branches\ngit checkout &lt;branch-name&gt;\ngit switch &lt;branch-name&gt;       # Git 2.23+\n\n# Merge branch\ngit merge &lt;branch-name&gt;\n\n# Delete branch\ngit branch -d &lt;branch-name&gt;    # Safe delete\ngit branch -D &lt;branch-name&gt;    # Force delete\n\n# Rename branch\ngit branch -m &lt;old-name&gt; &lt;new-name&gt;\n\n\n\n# View remotes\ngit remote -v\n\n# Add remote\ngit remote add &lt;name&gt; &lt;url&gt;\ngit remote add origin git@github.com:username/repo.git\n\n# Remove remote\ngit remote remove &lt;name&gt;\n\n# Rename remote\ngit remote rename &lt;old&gt; &lt;new&gt;\n\n# Fetch changes\ngit fetch\ngit fetch origin\n\n# Pull changes\ngit pull\ngit pull origin main\n\n# Push changes\ngit push\ngit push origin main\ngit push -u origin main      # Set upstream\ngit push --force             # Force push (use with caution!)\n\n\n\n# Save changes temporarily\ngit stash\ngit stash save \"Work in progress\"\n\n# List stashes\ngit stash list\n\n# Apply stash\ngit stash apply              # Apply most recent\ngit stash apply stash@{0}   # Apply specific stash\n\n# Apply and remove stash\ngit stash pop\n\n# Remove stash\ngit stash drop stash@{0}\n\n# Clear all stashes\ngit stash clear\n\n\n\n# Discard changes in working directory\ngit checkout -- &lt;file&gt;\ngit restore &lt;file&gt;           # Git 2.23+\n\n# Unstage files\ngit reset HEAD &lt;file&gt;\ngit restore --staged &lt;file&gt;  # Git 2.23+\n\n# Reset to previous commit (keeping changes)\ngit reset --soft HEAD~1\n\n# Reset to previous commit (discard changes)\ngit reset --hard HEAD~1\n\n# Revert a commit (creates new commit)\ngit revert &lt;commit-hash&gt;\n\n\n\n# List tags\ngit tag\n\n# Create tag\ngit tag v1.0.0\ngit tag -a v1.0.0 -m \"Version 1.0.0\"  # Annotated tag\n\n# Push tags\ngit push origin v1.0.0\ngit push origin --tags       # Push all tags\n\n# Delete tag\ngit tag -d v1.0.0           # Local\ngit push origin :v1.0.0     # Remote",
    "crumbs": [
      "GitHub",
      "Basic Uses"
    ]
  },
  {
    "objectID": "GitHub/commands.html#github-cli-essentials",
    "href": "GitHub/commands.html#github-cli-essentials",
    "title": "Basic Uses",
    "section": "",
    "text": "# Set default repository\ngh repo set-default\n# Select from list or specify:\ngh repo set-default owner/repo\n\n# View repository\ngh repo view\ngh repo view owner/repo\n\n# Fork repository\ngh repo fork owner/repo\n\n# Create repository\ngh repo create my-repo --public --clone\n\n# Delete repository (use with caution!)\ngh repo delete owner/repo\n\n# Clone repository\ngh repo clone owner/repo\n\n# List repositories\ngh repo list\ngh repo list owner\n\n\n\n# Create pull request\ngh pr create\ngh pr create --title \"Feature X\" --body \"Description\"\ngh pr create --fill  # Use commit messages for title/body\ngh pr create --draft # Create as draft\ngh pr create --assignee @me --label bug,enhancement\n\n# List pull requests\ngh pr list\ngh pr list --state all\ngh pr list --author @me\n\n# View pull request\ngh pr view\ngh pr view 123\n\n# Checkout pull request\ngh pr checkout 123\n\n# Merge pull request\ngh pr merge 123\ngh pr merge 123 --merge    # Create merge commit\ngh pr merge 123 --rebase   # Rebase and merge\ngh pr merge 123 --squash   # Squash and merge\n\n# Close pull request\ngh pr close 123\n\n# Review pull request\ngh pr review 123 --approve\ngh pr review 123 --request-changes\ngh pr review 123 --comment\n\n# Check pull request status\ngh pr status\ngh pr checks 123\n\n\n\n# Create issue\ngh issue create\ngh issue create --title \"Bug report\" --body \"Description\"\n\n# List issues\ngh issue list\ngh issue list --assignee @me\ngh issue list --label bug\n\n# View issue\ngh issue view 123\n\n# Close issue\ngh issue close 123\n\n# Reopen issue\ngh issue reopen 123\n\n# Comment on issue\ngh issue comment 123 --body \"This is fixed\"\n\n\n\n# List workflows\ngh workflow list\n\n# View workflow runs\ngh run list\ngh run view\n\n# Watch workflow run\ngh run watch\n\n# Download artifacts\ngh run download\n\n# Trigger workflow\ngh workflow run &lt;workflow-name&gt;\n\n\n\n# Create gist\ngh gist create file.txt\ngh gist create --public file.txt\n\n# List gists\ngh gist list\n\n# View gist\ngh gist view &lt;id&gt;\n\n# Edit gist\ngh gist edit &lt;id&gt;",
    "crumbs": [
      "GitHub",
      "Basic Uses"
    ]
  },
  {
    "objectID": "GitHub/commands.html#common-workflows",
    "href": "GitHub/commands.html#common-workflows",
    "title": "Basic Uses",
    "section": "",
    "text": "# 1. Start your day - sync with remote\ngit pull origin main\n\n# 2. Create feature branch\ngit checkout -b feature/new-feature\n\n# 3. Make changes and commit\ngit add .\ngit commit -m \"Add new feature\"\n\n# 4. Push to remote\ngit push -u origin feature/new-feature\n\n# 5. Create pull request\ngh pr create --fill\n\n# 6. After PR is merged, clean up\ngit checkout main\ngit pull origin main\ngit branch -d feature/new-feature\n\n\n\n# 1. Pull latest changes\ngit pull origin main\n\n# 2. If conflicts occur, Git will notify you\n# 3. Open conflicted files and resolve manually\n# Look for conflict markers:\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# Your changes\n# =======\n# Their changes\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; branch-name\n\n# 4. After resolving, add the files\ngit add &lt;resolved-files&gt;\n\n# 5. Complete the merge\ngit commit -m \"Resolve merge conflicts\"\n\n# 6. Push changes\ngit push origin &lt;branch&gt;\n\n\n\n# 1. Add upstream remote (one time)\ngit remote add upstream https://github.com/original-owner/repo.git\n\n# 2. Fetch upstream changes\ngit fetch upstream\n\n# 3. Checkout main branch\ngit checkout main\n\n# 4. Merge upstream changes\ngit merge upstream/main\n\n# 5. Push to your fork\ngit push origin main\n\n# Using GitHub CLI\ngh repo sync owner/repo -b main\n\n\n\n# Interactive rebase for last 3 commits\ngit rebase -i HEAD~3\n\n# In the editor:\n# Change 'pick' to 'squash' for commits to combine\n# Save and close\n\n# Force push (if already pushed)\ngit push --force-with-lease origin &lt;branch&gt;\n\n\n\n# Apply specific commit to current branch\ngit cherry-pick &lt;commit-hash&gt;\n\n# Cherry-pick multiple commits\ngit cherry-pick &lt;hash1&gt; &lt;hash2&gt; &lt;hash3&gt;\n\n# Cherry-pick range\ngit cherry-pick &lt;oldest-hash&gt;^..&lt;newest-hash&gt;",
    "crumbs": [
      "GitHub",
      "Basic Uses"
    ]
  },
  {
    "objectID": "GitHub/commands.html#best-practices",
    "href": "GitHub/commands.html#best-practices",
    "title": "Basic Uses",
    "section": "",
    "text": "The Seven Rules of Great Commit Messages:\n\nSeparate subject from body with blank line\nLimit subject line to 50 characters\nCapitalize the subject line\nDon’t end subject line with period\nUse imperative mood (“Add feature” not “Added feature”)\nWrap body at 72 characters\nExplain what and why, not how\n\nExample:\nAdd user authentication feature\n\nImplement OAuth 2.0 authentication using GitHub as provider.\nThis allows users to sign in with their GitHub credentials\ninstead of creating separate accounts.\n\nResolves: #123\nSee also: #456, #789\n\n\n\nfeature/add-login-page\nbugfix/fix-navigation-menu\nhotfix/security-patch\nrelease/v2.0.0\ndocs/update-readme\ntest/add-unit-tests\nrefactor/optimize-database\n\n\n\nCreate a .gitignore file in your repository root:\n# macOS\n.DS_Store\n.AppleDouble\n.LSOverride\n\n# IDE\n.vscode/\n.idea/\n*.swp\n*.swo\n\n# Dependencies\nnode_modules/\nvendor/\n.env\n\n# Build outputs\ndist/\nbuild/\n*.log\n\n# Sensitive data\n*.pem\n*.key\n.env.local\nconfig/secrets.yml\n\n\n\n\nNever commit sensitive data:\n\nPasswords, API keys, tokens\nPrivate keys or certificates\nDatabase credentials\n.env files with secrets\n\nIf you accidentally commit secrets:\n# Remove from history (requires force push)\ngit filter-branch --force --index-filter \\\n  \"git rm --cached --ignore-unmatch path/to/file\" \\\n  --prune-empty --tag-name-filter cat -- --all\n\n# Or use BFG Repo-Cleaner (easier)\nbrew install bfg\nbfg --delete-files file-with-secrets.txt\nUse GitHub’s security features:\n\nEnable Dependabot alerts\nEnable secret scanning\nUse protected branches\nRequire PR reviews\n\n\n\n\n\n\nAlways work in branches - Never commit directly to main\nKeep PRs small - Easier to review and less likely to have conflicts\nWrite descriptive PR descriptions - Include what, why, and how\nReview others’ code - Learn and help maintain quality\nUpdate documentation - Keep README and docs current\nTest before pushing - Run tests locally first\nCommunicate - Use issues and PR comments effectively",
    "crumbs": [
      "GitHub",
      "Basic Uses"
    ]
  },
  {
    "objectID": "GitHub/commands.html#troubleshooting",
    "href": "GitHub/commands.html#troubleshooting",
    "title": "Basic Uses",
    "section": "",
    "text": "# Check SSH key is added\nssh-add -l\n\n# Add SSH key\nssh-add ~/.ssh/id_ed25519\n\n# Test connection\nssh -T git@github.com\n\n\n\n# Pull first, then push\ngit pull origin main --rebase\ngit push origin main\n\n# Or force push (careful!)\ngit push --force-with-lease\n\n\n\n# Create new branch with current commits\ngit branch new-branch\n\n# Reset original branch\ngit reset --hard HEAD~3  # Go back 3 commits\n\n# Switch to new branch\ngit checkout new-branch\n\n\n\n# Keep changes, undo commit\ngit reset --soft HEAD~1\n\n# Discard changes completely\ngit reset --hard HEAD~1\n\n\n\n# Install Git LFS\nbrew install git-lfs\ngit lfs install\n\n# Track large files\ngit lfs track \"*.psd\"\ngit add .gitattributes\ngit add large-file.psd\ngit commit -m \"Add large file with LFS\"\n\n\n\n# Update your branch\ngit checkout main\ngit pull origin main\ngit checkout your-branch\ngit rebase main\n\n# Resolve conflicts, then\ngit add .\ngit rebase --continue\ngit push --force-with-lease",
    "crumbs": [
      "GitHub",
      "Basic Uses"
    ]
  },
  {
    "objectID": "GitHub/commands.html#quick-reference",
    "href": "GitHub/commands.html#quick-reference",
    "title": "Basic Uses",
    "section": "",
    "text": "[alias]\n    st = status\n    co = checkout\n    ci = commit\n    br = branch\n    df = diff\n    lg = log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit\n    last = log -1 HEAD\n    unstage = reset HEAD --\n    amend = commit --amend\n    branches = branch -a\n    remotes = remote -v\n    contributors = shortlog --summary --numbered\n\n\n\n\nCmd + Shift + P → Git commands\nCtrl + Shift + G → Source control panel\nCmd + Enter → Commit staged changes\nOption + Cmd + Enter → Commit all changes\n\n\n\n\n# Git shortcuts\nalias g='git'\nalias gs='git status'\nalias ga='git add'\nalias gc='git commit -m'\nalias gp='git push'\nalias gpl='git pull'\nalias gco='git checkout'\nalias gb='git branch'\nalias glog='git log --oneline --graph --all'\n\n# GitHub CLI shortcuts\nalias ghr='gh repo'\nalias ghpr='gh pr'\nalias ghi='gh issue'",
    "crumbs": [
      "GitHub",
      "Basic Uses"
    ]
  },
  {
    "objectID": "GitHub/resources.html#resources-links",
    "href": "GitHub/resources.html#resources-links",
    "title": "NASA Disasters Documentation",
    "section": "",
    "text": "Git Documentation: https://git-scm.com/doc\nGitHub Docs: https://docs.github.com\nGitHub CLI Manual: https://cli.github.com/manual\nGitHub Learning Lab: https://lab.github.com\n\n\n\n\n\nLearn Git Branching: https://learngitbranching.js.org\nGitHub Skills: https://skills.github.com\nAtlassian Git Tutorial: https://www.atlassian.com/git/tutorials\nOh My Git! (Game): https://ohmygit.org\n\n\n\n\n\nGitHub Git Cheat Sheet: https://education.github.com/git-cheat-sheet-education.pdf\nInteractive Git Cheat Sheet: https://ndpsoftware.com/git-cheatsheet.html\nGitHub CLI Cheat Sheet: https://github.com/cli/cli#commands\n\n\n\n\n\nPro Git Book (Free): https://git-scm.com/book\nGit Flow: https://nvie.com/posts/a-successful-git-branching-model\nConventional Commits: https://www.conventionalcommits.org\nSemantic Versioning: https://semver.org\n\n\n\n\n\nGitHub Desktop: https://desktop.github.com\nSourceTree: https://www.sourcetreeapp.com\nGitKraken: https://www.gitkraken.com\nTower: https://www.git-tower.com\n\n\n\n\n\nGitLens: Enhanced Git capabilities\nGit Graph: Visualize branch structure\nGitHub Pull Requests: Manage PRs from VS Code\nGit History: View and search git log\n\n\n\n\n\nGitHub Status: https://www.githubstatus.com\nStack Overflow Git Tag: https://stackoverflow.com/questions/tagged/git\nGitHub Community Forum: https://github.community\n\n\n\n\n\nGitHub YouTube: https://youtube.com/github\nThe Net Ninja Git Tutorial: Comprehensive video series\nTraversy Media Git Crash Course: Quick overview\n\n\n\n\n\nGitHub Flavored Markdown: https://github.github.com/gfm\nMarkdown Guide: https://www.markdownguide.org\nShields.io (Badges): https://shields.io",
    "crumbs": [
      "GitHub",
      "Resources & Links"
    ]
  },
  {
    "objectID": "GitHub/resources.html#appendix-quick-setup-script",
    "href": "GitHub/resources.html#appendix-quick-setup-script",
    "title": "NASA Disasters Documentation",
    "section": "Appendix: Quick Setup Script",
    "text": "Appendix: Quick Setup Script\nSave this as setup-git-github.sh and run to quickly set up your environment:\n#!/bin/bash\n\necho \"🚀 Git and GitHub Setup Script for macOS\"\necho \"=======================================\"\n\n# Install Homebrew if not present\nif ! command -v brew &&gt; /dev/null; then\n    echo \"📦 Installing Homebrew...\"\n    /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nfi\n\n# Install Git\necho \"📦 Installing Git...\"\nbrew install git\n\n# Install GitHub CLI\necho \"📦 Installing GitHub CLI...\"\nbrew install gh\n\n# Git configuration\necho \"⚙️ Configuring Git...\"\nread -p \"Enter your name: \" name\nread -p \"Enter your email: \" email\n\ngit config --global user.name \"$name\"\ngit config --global user.email \"$email\"\ngit config --global init.defaultBranch main\ngit config --global color.ui auto\n\n# Generate SSH key\necho \"🔑 Generating SSH key...\"\nssh-keygen -t ed25519 -C \"$email\" -f ~/.ssh/id_ed25519 -N \"\"\n\n# Start SSH agent and add key\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n\n# Copy SSH key to clipboard\npbcopy &lt; ~/.ssh/id_ed25519.pub\necho \"📋 SSH public key copied to clipboard!\"\n\n# GitHub CLI authentication\necho \"🔐 Authenticating with GitHub...\"\ngh auth login\n\necho \"✅ Setup complete!\"\necho \"\"\necho \"Next steps:\"\necho \"1. Go to GitHub Settings → SSH Keys\"\necho \"2. Add a new SSH key (already in clipboard)\"\necho \"3. Test with: ssh -T git@github.com\"\nMake executable and run:\nchmod +x setup-git-github.sh\n./setup-git-github.sh\n\nLast Updated: 2024 Version: 1.0\nThis guide is a living document. Contribute improvements at: [your-repo-url]",
    "crumbs": [
      "GitHub",
      "Resources & Links"
    ]
  },
  {
    "objectID": "Jupyterhub/simple_disaster_template.html",
    "href": "Jupyterhub/simple_disaster_template.html",
    "title": "🌍 Simple Disaster COG Processing",
    "section": "",
    "text": "This simplified notebook converts disaster satellite imagery to Cloud Optimized GeoTIFFs (COGs) with just a few cells.",
    "crumbs": [
      "JupyterHub",
      "🌍 Simple Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/simple_disaster_template.html#features",
    "href": "Jupyterhub/simple_disaster_template.html#features",
    "title": "🌍 Simple Disaster COG Processing",
    "section": "✨ Features",
    "text": "✨ Features\n\nSee files first - List S3 files before configuring\nSmart configuration - Define filename functions after seeing actual files\nAuto-discovery - Automatically categorizes your files\nSimple processing - Just run the cells in order\n\n\n\n🚀 Launch in VEDA JupyterHub (requires access)\n\n\nTo obtain credentials to VEDA Hub,  follow this link for more information.\n\n\nDisclaimer: it is highly recommended to run a tutorial within NASA VEDA JupyterHub, which already includes functions for processing and visualizing data specific to VEDA stories. Running the tutorial outside of the VEDA JupyterHub may lead to errors, specifically related to EarthData authentication. Additionally, it is recommended to use the Pangeo workspace within the VEDA JupyterHub, since certain packages relevant to this tutorial are already installed.\n\n\nIf you do not have a VEDA Jupyterhub Account you can launch this notebook on your local environment using MyBinder by clicking the icon below.",
    "crumbs": [
      "JupyterHub",
      "🌍 Simple Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/simple_disaster_template.html#step-1-basic-configuration",
    "href": "Jupyterhub/simple_disaster_template.html#step-1-basic-configuration",
    "title": "🌍 Simple Disaster COG Processing",
    "section": "📋 Step 1: Basic Configuration",
    "text": "📋 Step 1: Basic Configuration\nSet your event details and S3 paths:\n\n# ========================================\n# INPUTS\n# ========================================\n\n# S3 Paths\nBUCKET = 'nasa-disasters'    # S3 bucket (DO NOT CHANGE)\nDESTINATION_BASE = 'drcs_activations_new'  # Where to save COGs in S3 bucket (DO NOT CHANGE)\nGEOTIFF_DIR = 'geotiffs_to_convert' # This is where all non-converted files should be placed\n\n\n# Event Details\nSATELLITE_NAME = '202408_TropicalStorm_Debby'  # Your sensor or product name (e.g, Sentinel-1, Planet, Landsat)\nSUB_PRODUCT_NAME = 'landsat8'         # Sub-directories within PRODUCT_NAME (RGB, trueColor, SWIR, etc.). Can leave blank and it will read from PRODUCT_NAME.\nSOURCE_PATH = f'{GEOTIFF_DIR}/{SATELLITE_NAME}/{SUB_PRODUCT_NAME}'      # Where your files are\n\n\n# Processing Options\nOVERWRITE = False      # Set to True to replace existing files\nVERIFY = True          # Set to True to verify results after processing\nSAVE_RESULTS = True    # Set to False to skip saving results CSV to /output directory\n\nprint(f\"Event: {EVENT_NAME}\")\nprint(f\"Source: s3://{BUCKET}/{SOURCE_PATH}\")\nprint(f\"Destination: s3://{BUCKET}/{DESTINATION_BASE}/\")\n\nEvent: 202408_TropicalStorm_Debby\nSource: s3://nasa-disasters/geotiffs_to_convert/202408_TropicalStorm_Debby/landsat8\nDestination: s3://nasa-disasters/drcs_activations_new/",
    "crumbs": [
      "JupyterHub",
      "🌍 Simple Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/simple_disaster_template.html#step-2-connect-to-s3-and-list-files",
    "href": "Jupyterhub/simple_disaster_template.html#step-2-connect-to-s3-and-list-files",
    "title": "🌍 Simple Disaster COG Processing",
    "section": "🔍 Step 2: Connect to S3 and List Files",
    "text": "🔍 Step 2: Connect to S3 and List Files\nLet’s see what files are available before configuring filename transformations:\n\n# Import necessary modules\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path\nsys.path.insert(0, str(Path('..').resolve()))\n\n# Import S3 operations\nfrom core.s3_operations import (\n    initialize_s3_client,\n    list_s3_files,\n    get_file_size_from_s3\n)\n\n# Initialize S3 client\nprint(\"🌐 Connecting to S3...\")\ns3_client, _ = initialize_s3_client(bucket_name=BUCKET, verbose=False)\n\nif s3_client:\n    print(\"✅ Connected to S3\\n\")\n    \n    # List all TIF files\n    print(f\"📂 Files in s3://{BUCKET}/{SOURCE_PATH}:\")\n    print(\"=\"*60)\n    \n    files = list_s3_files(s3_client, BUCKET, SOURCE_PATH, suffix='.tif')\n    \n    if files:\n        print(f\"Found {len(files)} .tif files:\\n\")\n        for i, file_path in enumerate(files[:10], 1):  # Show first 10\n            filename = os.path.basename(file_path)\n            try:\n                size_gb = get_file_size_from_s3(s3_client, BUCKET, file_path)\n                print(f\"{i:2}. {filename:&lt;60} ({size_gb:.2f} GB)\")\n            except:\n                print(f\"{i:2}. {filename}\")\n        \n        if len(files) &gt; 10:\n            print(f\"\\n... and {len(files) - 10} more files\")\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"\\n💡 Use this information to create filename functions in Step 3\")\n    else:\n        print(\"⚠️ No .tif files found in the specified path.\")\n        print(\"   Check your SOURCE_PATH configuration.\")\nelse:\n    print(\"❌ Could not connect to S3. Check your AWS credentials.\")\n    files = []\n\n🌐 Connecting to S3...\n✅ Connected to S3\n\n📂 Files in s3://nasa-disasters/geotiffs_to_convert/202408_TropicalStorm_Debby/landsat8:\n============================================================\n⚠️ No .tif files found in the specified path.\n   Check your SOURCE_PATH configuration.",
    "crumbs": [
      "JupyterHub",
      "🌍 Simple Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/simple_disaster_template.html#step-3a-define-categorization-and-filename-transformations",
    "href": "Jupyterhub/simple_disaster_template.html#step-3a-define-categorization-and-filename-transformations",
    "title": "🌍 Simple Disaster COG Processing",
    "section": "🏷️ Step 3a: Define Categorization and Filename Transformations",
    "text": "🏷️ Step 3a: Define Categorization and Filename Transformations\nBased on the files you see above, configure: 1. Categorization patterns - Regex patterns to identify file types 2. Filename functions - How to transform filenames 3. Output directories - Where each category should be saved\n\n# ========================================\n# CATEGORIZATION AND OUTPUT CONFIGURATION\n# ========================================\n\nimport re\n\n# STEP 1: Define how to extract dates from filenames\ndef extract_date_from_filename(filename):\n    \"\"\"Extract date from filename in YYYYMMDD format.\"\"\"\n    dates = re.findall(r'\\d{8}', filename)\n    if dates:\n        date_str = dates[0]\n        return f\"{date_str[0:4]}-{date_str[4:6]}-{date_str[6:8]}\"\n    return None\n\n# STEP 2: Define filename transformation functions for each category\ndef create_truecolor_filename(original_path, event_name):\n    \"\"\"Create filename for trueColor products.\"\"\"\n    filename = os.path.basename(original_path)\n    stem = os.path.splitext(filename)[0]\n    date = extract_date_from_filename(stem)\n    \n    if date:\n        stem_clean = re.sub(r'_\\d{8}', '', stem)\n        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n    return f\"{event_name}_{stem}_day.tif\"\n\ndef create_colorinfrared_filename(original_path, event_name):\n    \"\"\"Create filename for colorInfrared products.\"\"\"\n    filename = os.path.basename(original_path)\n    stem = os.path.splitext(filename)[0]\n    date = extract_date_from_filename(stem)\n    \n    if date:\n        stem_clean = re.sub(r'_\\d{8}', '', stem)\n        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n    return f\"{event_name}_{stem}_day.tif\"\n\ndef create_naturalcolor_filename(original_path, event_name):\n    \"\"\"Create filename for naturalColor products.\"\"\"\n    filename = os.path.basename(original_path)\n    stem = os.path.splitext(filename)[0]\n    date = extract_date_from_filename(stem)\n    \n    if date:\n        stem_clean = re.sub(r'_\\d{8}', '', stem)\n        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n    return f\"{event_name}_{stem}_day.tif\"\n\n# STEP 3: Configure categorization patterns (REQUIRED)\n# These regex patterns determine which files belong to which category\nCATEGORIZATION_PATTERNS = {\n    'trueColor': r'trueColor|truecolor|true_color',\n    'colorInfrared': r'colorInfrared|colorIR|color_infrared',\n    'naturalColor': r'naturalColor|natural_color',\n    # Add patterns for ALL file types you want to process\n    # Files not matching any pattern will be skipped with a warning\n}\n\n# STEP 4: Map categories to filename transformation functions\nFILENAME_CREATORS = {\n    'trueColor': create_truecolor_filename,\n    'colorInfrared': create_colorinfrared_filename,\n    'naturalColor': create_naturalcolor_filename,\n    # Must have an entry for each category in CATEGORIZATION_PATTERNS\n}\n\n# STEP 5: Specify output directories for each category\nOUTPUT_DIRS = {\n    'trueColor': 'Landsat/trueColor',\n    'colorInfrared': 'Landsat/colorIR',\n    'naturalColor': 'Landsat/naturalColor',\n    # Must have an entry for each category in CATEGORIZATION_PATTERNS\n}\n\n# OPTIONAL: Specify no-data values (None = auto-detect)\nNODATA_VALUES = {\n    'trueColor': 0,\n    'colorInfrared': 0,\n    'naturalColor': 0\n    # Leave empty or set to None for auto-detection\n}",
    "crumbs": [
      "JupyterHub",
      "🌍 Simple Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/simple_disaster_template.html#step-3b-test-the-new-functions-to-verify-what-the-inputs-and-outputs-will-be.",
    "href": "Jupyterhub/simple_disaster_template.html#step-3b-test-the-new-functions-to-verify-what-the-inputs-and-outputs-will-be.",
    "title": "🌍 Simple Disaster COG Processing",
    "section": "🏷️ Step 3b: Test the new functions to verify what the inputs and outputs will be.",
    "text": "🏷️ Step 3b: Test the new functions to verify what the inputs and outputs will be.\n\nprint(\"✅ Configuration defined\")\nprint(f\"\\n📂 Categories and output paths:\")\nfor category, path in OUTPUT_DIRS.items():\n    pattern = CATEGORIZATION_PATTERNS.get(category, 'No pattern defined')\n    print(f\"   • {category}:\")\n    print(f\"     Pattern: {pattern}\")\n    print(f\"     Output:  {DESTINATION_BASE}/{path}\")\n\n# Test with sample filename if files exist\nif files:\n    sample_file = files[0]\n    sample_name = os.path.basename(sample_file)\n    \n    # Check which category it would match\n    matched_category = None\n    for cat, pattern in CATEGORIZATION_PATTERNS.items():\n        if re.search(pattern, sample_name, re.IGNORECASE):\n            matched_category = cat\n            break\n    \n    if matched_category:\n        new_name = FILENAME_CREATORS[matched_category](sample_file, EVENT_NAME)\n        print(f\"\\n📝 Example transformation:\")\n        print(f\"   Original: {sample_name}\")\n        print(f\"   Category: {matched_category}\")\n        print(f\"   → New:    {new_name}\")\n        print(f\"   → Output: {DESTINATION_BASE}/{OUTPUT_DIRS[matched_category]}/{new_name}\")\n    else:\n        print(f\"\\n⚠️ Warning: Sample file doesn't match any category pattern:\")\n        print(f\"   File: {sample_name}\")\n        print(f\"   Add a pattern to CATEGORIZATION_PATTERNS to process this file type\")\n\n✅ Configuration defined\n\n📂 Categories and output paths:\n   • trueColor:\n     Pattern: trueColor|truecolor|true_color\n     Output:  drcs_activations_new/Landsat/trueColor\n   • colorInfrared:\n     Pattern: colorInfrared|colorIR|color_infrared\n     Output:  drcs_activations_new/Landsat/colorIR\n   • naturalColor:\n     Pattern: naturalColor|natural_color\n     Output:  drcs_activations_new/Landsat/naturalColor",
    "crumbs": [
      "JupyterHub",
      "🌍 Simple Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/simple_disaster_template.html#step-4-initialize-processor-and-preview",
    "href": "Jupyterhub/simple_disaster_template.html#step-4-initialize-processor-and-preview",
    "title": "🌍 Simple Disaster COG Processing",
    "section": "🚀 Step 4: Initialize Processor and Preview",
    "text": "🚀 Step 4: Initialize Processor and Preview\nNow let’s set up the processor and preview all transformations:",
    "crumbs": [
      "JupyterHub",
      "🌍 Simple Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/simple_disaster_template.html#step-5-process-files",
    "href": "Jupyterhub/simple_disaster_template.html#step-5-process-files",
    "title": "🌍 Simple Disaster COG Processing",
    "section": "⚙️ Step 5: Process Files",
    "text": "⚙️ Step 5: Process Files\nRun this cell to start processing all files:\n\n# Import our simplified helper\nfrom notebooks.notebook_helpers import SimpleProcessor\n\n# Create full configuration with categorization patterns\nconfig = {\n    'event_name': EVENT_NAME,\n    'bucket': BUCKET,\n    'source_path': SOURCE_PATH,\n    'destination_base': DESTINATION_BASE,\n    'overwrite': OVERWRITE,\n    'verify': VERIFY,\n    'save_results': SAVE_RESULTS,  # Add save results flag\n    'categorization_patterns': CATEGORIZATION_PATTERNS,  # IMPORTANT: Include patterns\n    'filename_creators': FILENAME_CREATORS,\n    'output_dirs': OUTPUT_DIRS,\n    'nodata_values': NODATA_VALUES\n}\n\n# Initialize processor\nprocessor = SimpleProcessor(config)\n\n# Connect to S3 (already connected, but needed for processor)\nif processor.connect_to_s3():\n    print(\"✅ Processor ready\\n\")\n    \n    # Discover and categorize files\n    num_files = processor.discover_files()\n    \n    if num_files &gt; 0:\n        # Show preview of transformations\n        processor.preview_processing()\n        \n        print(\"\\n📌 Review the transformations above.\")\n        print(\"   • Files will be saved to the directories specified in OUTPUT_DIRS\")\n        print(\"   • If files appear as 'uncategorized', add patterns to CATEGORIZATION_PATTERNS\")\n        print(\"   • When ready, proceed to Step 5 to process the files.\")\n    else:\n        print(\"⚠️ No files found to process.\")\nelse:\n    print(\"❌ Could not initialize processor.\")\n\n✅ All modules loaded successfully\n\n🌐 Connecting to S3...\n✅ Connected to S3 successfully\n✅ Processor ready\n\n\n🔍 Searching for files in: geotiffs_to_convert/202408_TropicalStorm_Debby/landsat8\n⚠️ No .tif files found\n⚠️ No files found to process.\n\n\n\n# Process all files\nif 'num_files' in locals() and num_files &gt; 0:\n    print(\"🚀 Starting processing...\")\n    print(\"This may take several minutes depending on file sizes.\\n\")\n    \n    # Process everything\n    results = processor.process_all()\n    \n    # Display results\n    if not results.empty:\n        print(\"\\n📊 Processing Complete!\")\n        display(results) if 'display' in dir() else print(results)\nelse:\n    print(\"⚠️ No files to process. Complete Steps 1-4 first.\")\n\n🚀 Starting processing...\nThis may take several minutes depending on file sizes.\n\n\n🚀 Starting processing...\n\n📦 Processing colorInfrared (3 files)\n  ⏭️ Skipped: LC08_colorInfrared_20240715_155319_016036.tif (exists)\n  ⏭️ Skipped: LC08_colorInfrared_20240715_155343_016037.tif (exists)\n  ⏭️ Skipped: LC08_colorInfrared_20240715_15547_016038.tif (exists)\n\n📦 Processing naturalColor (3 files)\n  ⏭️ Skipped: LC08_naturalColor_20240715_155319_016036.tif (exists)\n  ⏭️ Skipped: LC08_naturalColor_20240715_155343_016037.tif (exists)\n  ⏭️ Skipped: LC08_naturalColor_20240715_15547_016038.tif (exists)\n\n📦 Processing trueColor (3 files)\n  ⏭️ Skipped: LC08_trueColor_20240715_155319_016036.tif (exists)\n  ⏭️ Skipped: LC08_trueColor_20240715_155343_016037.tif (exists)\n  ⏭️ Skipped: LC08_trueColor_20240715_15547_016038.tif (exists)\n\n============================================================\n✅ PROCESSING COMPLETE\n============================================================\n\nResults:\n  ⏭️ Skipped: 9\n\nProcessing time: 0.0 minutes\n\n📁 Results saved to: output/202408_TropicalStorm_Debby/results_20250929_191143.csv\n============================================================\n\n📊 Processing Complete!\n                                     source_file       category   status  \\\n0  LC08_colorInfrared_20240715_155319_016036.tif  colorInfrared  skipped   \n1  LC08_colorInfrared_20240715_155343_016037.tif  colorInfrared  skipped   \n2   LC08_colorInfrared_20240715_15547_016038.tif  colorInfrared  skipped   \n3   LC08_naturalColor_20240715_155319_016036.tif   naturalColor  skipped   \n4   LC08_naturalColor_20240715_155343_016037.tif   naturalColor  skipped   \n5    LC08_naturalColor_20240715_15547_016038.tif   naturalColor  skipped   \n6      LC08_trueColor_20240715_155319_016036.tif      trueColor  skipped   \n7      LC08_trueColor_20240715_155343_016037.tif      trueColor  skipped   \n8       LC08_trueColor_20240715_15547_016038.tif      trueColor  skipped   \n\n           reason                                        output_path  \\\n0  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n1  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n2  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n3  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n4  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n5  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n6  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n7  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n8  already exists  s3://nasa-disasters/drcs_activations_new/Lands...   \n\n   time_seconds  \n0             0  \n1             0  \n2             0  \n3             0  \n4             0  \n5             0  \n6             0  \n7             0  \n8             0  \n\n\n\n# Analyze results\nif 'results' in locals() and not results.empty:\n    print(\"📊 PROCESSING STATISTICS\")\n    print(\"=\"*40)\n    \n    # Success rate\n    total = len(results)\n    success = len(results[results['status'] == 'success'])\n    failed = len(results[results['status'] == 'failed'])\n    skipped = len(results[results['status'] == 'skipped'])\n    \n    print(f\"Total files: {total}\")\n    print(f\"✅ Success: {success}\")\n    print(f\"❌ Failed: {failed}\")\n    print(f\"⏭️ Skipped: {skipped}\")\n    print(f\"\\nSuccess rate: {(success/total*100):.1f}%\")\n    \n    # Failed files\n    if failed &gt; 0:\n        print(\"\\n❌ Failed files:\")\n        failed_df = results[results['status'] == 'failed']\n        for idx, row in failed_df.iterrows():\n            print(f\"  - {row['source_file']}: {row.get('error', 'Unknown error')}\")\n    \n    # Processing times\n    if 'time_seconds' in results.columns:\n        success_df = results[results['status'] == 'success']\n        if not success_df.empty:\n            avg_time = success_df['time_seconds'].mean()\n            max_time = success_df['time_seconds'].max()\n            print(f\"\\n⏱️ Timing:\")\n            print(f\"Average: {avg_time:.1f} seconds per file\")\n            print(f\"Slowest: {max_time:.1f} seconds\")\nelse:\n    print(\"No results to analyze. Run Step 5 first.\")",
    "crumbs": [
      "JupyterHub",
      "🌍 Simple Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/simple_disaster_template.html#tips-troubleshooting",
    "href": "Jupyterhub/simple_disaster_template.html#tips-troubleshooting",
    "title": "🌍 Simple Disaster COG Processing",
    "section": "💡 Tips & Troubleshooting",
    "text": "💡 Tips & Troubleshooting\n\nWorkflow Summary:\n\nConfigure basic settings (Step 1)\nList files from S3 to see naming patterns (Step 2)\nDefine functions to transform filenames (Step 3)\nPreview transformations (Step 4)\nProcess all files (Step 5)\nReview results (Step 6)\n\n\n\nCommon Issues:\n\n“No files found”\n\nCheck SOURCE_PATH in Step 1\nVerify bucket permissions\nEnsure files have .tif extension\n\nWrong filename transformations\n\nReview actual filenames in Step 2\nAdjust functions in Step 3\nRe-run Step 4 to preview\n\nFiles being skipped\n\nFiles already exist in destination\nSet OVERWRITE = True in Step 1\n\nProcessing errors\n\nCheck AWS credentials\nVerify S3 write permissions\nCheck available disk space for temp files\n\n\n\n\nNeed More Control?\nUse the full template at disaster_processing_template.ipynb for: - Manual chunk configuration - Custom compression settings - Detailed memory management - Advanced processing options",
    "crumbs": [
      "JupyterHub",
      "🌍 Simple Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/convert_to_geotiff.html",
    "href": "Jupyterhub/convert_to_geotiff.html",
    "title": "Jupyter Notebook Guide for Disaster COG Processing",
    "section": "",
    "text": "This guide helps you get started with converting disaster satellite imagery to Cloud Optimized GeoTIFFs (COGs).\n\n\n\n\nUse templates/simple_disaster_template.ipynb for a streamlined experience with just 5 cells:\n\nOpen the notebook\njupyter notebook templates/simple_disaster_template.ipynb\nConfigure your event (Cell 1)\n\nSet EVENT_NAME (e.g., ‘202408_TropicalStorm_Debby’)\nSet PRODUCT_NAME (e.g., ‘landsat8’)\nModify filename functions to control output names\n\nRun the cells in order\n\nCell 2: Imports and initializes\nCell 3: Discovers files and shows preview\nCell 4: Processes all files\nCell 5: Reviews results\n\n\n\n\n\nUse templates/disaster_processing_template.ipynb for full control over: - Memory management - Chunk configurations - Processing parameters - Verification options - Detailed error handling\n\n\n\n\n\n\nEVENT_NAME = '202408_TropicalStorm_Debby'\nPRODUCT_NAME = 'landsat8'\nBUCKET = 'nasa-disasters'\nSOURCE_PATH = f'drcs_activations/{EVENT_NAME}/{PRODUCT_NAME}'\nDESTINATION_BASE = 'drcs_activations_new'\nOVERWRITE = False  # Set True to replace existing files\n\n\n\nDefine how your files are renamed:\ndef create_truecolor_filename(original_path, event_name):\n    \"\"\"Create filename for trueColor products.\"\"\"\n    filename = os.path.basename(original_path)\n    stem = os.path.splitext(filename)[0]\n    date = extract_date_from_filename(stem)\n\n    if date:\n        stem_clean = re.sub(r'_\\d{8}', '', stem)\n        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n    return f\"{event_name}_{stem}_day.tif\"\n\n\n\nFILENAME_CREATORS = {\n    'trueColor': create_truecolor_filename,\n    'colorInfrared': create_colorinfrared_filename,\n    'naturalColor': create_naturalcolor_filename,\n}\n\n\n\n\nThe system automatically: - Discovers files in your S3 source path - Categorizes them by product type (trueColor, NDVI, etc.) - Applies the appropriate filename function - Saves to organized output directories\n\n\ndrcs_activations_new/\n├── imagery/\n│   ├── trueColor/\n│   ├── colorIR/\n│   └── naturalColor/\n├── indices/\n│   ├── NDVI/\n│   └── MNDWI/\n└── SAR/\n    └── processed/\n\n\n\n\n\n\nThe system automatically detects and processes different product types:\n# Files are auto-categorized by these patterns:\n- 'trueColor' → imagery/trueColor/\n- 'colorInfrared' → imagery/colorIR/\n- 'NDVI' → indices/NDVI/\n- 'MNDWI' → indices/MNDWI/\n- 'SAR' → SAR/processed/\n\n\n\nNODATA_VALUES = {\n    'NDVI': -9999,      # Specific value for NDVI\n    'MNDWI': -9999,     # Specific value for MNDWI\n    'trueColor': None,  # Auto-detect for imagery\n}\n\n\n\nOUTPUT_DIRS = {\n    'trueColor': 'Landsat/trueColor',\n    'colorInfrared': 'Landsat/colorIR',\n    'naturalColor': 'Landsat/naturalColor',\n}\n\n\n\n\n\n\n\nCheck SOURCE_PATH is correct\nVerify files exist: aws s3 ls s3://bucket/path/\n\n\n\n\n\nCheck AWS credentials: aws configure list\nEnsure bucket access permissions\n\n\n\n\n\nFiles already exist in destination\nSet OVERWRITE = True to reprocess\n\n\n\n\n\nModify filename creator functions\nRe-run from discovery step to preview\n\n\n\n\n\nLarge files take time (normal)\nSystem automatically uses GDAL optimization\nFiles &gt;1.5GB use optimized chunking\n\n\n\n\n\n\nFile Size Optimization\n\nFiles &lt;1.5GB: Processed whole (fastest)\nFiles &gt;1.5GB: Smart chunking\nFiles &gt;7GB: Ultra-large file handling\n\nCompression\n\nUses ZSTD level 22 (maximum compression)\nAutomatic predictor selection\nIntelligent resampling based on data type\n\nParallel Processing\n\nFor batch processing multiple events, use:\n\nfrom batch_processor_parallel import process_files_parallel\n\n\n\n\n\n\nfrom notebooks.notebook_helpers import quick_process\n\nresults = quick_process({\n    'event_name': '202408_TropicalStorm_Debby',\n    'bucket': 'nasa-disasters',\n    'source_path': 'drcs_activations/202408_TropicalStorm_Debby/landsat8',\n    'destination_base': 'drcs_activations_new',\n    'overwrite': False,\n    'filename_creators': FILENAME_CREATORS\n})\n\n\n\nevents = [\n    '202408_TropicalStorm_Debby',\n    '202409_Hurricane_Example',\n    '202410_Wildfire_Sample'\n]\n\nfor event in events:\n    config['event_name'] = event\n    config['source_path'] = f'drcs_activations/{event}/landsat8'\n    processor = SimpleProcessor(config)\n    processor.connect_to_s3()\n    processor.discover_files()\n    processor.process_all()\n\n\n\n\n\nStart with the simple template\nRun a small test batch\nVerify output filenames are correct\nProcess full dataset\nCheck results in S3\n\nFor more details, see the main README.md or review the RESAMPLING_GUIDE.md for data type handling.",
    "crumbs": [
      "JupyterHub",
      "Jupyter Notebook Guide for Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/convert_to_geotiff.html#quick-start",
    "href": "Jupyterhub/convert_to_geotiff.html#quick-start",
    "title": "Jupyter Notebook Guide for Disaster COG Processing",
    "section": "",
    "text": "Use templates/simple_disaster_template.ipynb for a streamlined experience with just 5 cells:\n\nOpen the notebook\njupyter notebook templates/simple_disaster_template.ipynb\nConfigure your event (Cell 1)\n\nSet EVENT_NAME (e.g., ‘202408_TropicalStorm_Debby’)\nSet PRODUCT_NAME (e.g., ‘landsat8’)\nModify filename functions to control output names\n\nRun the cells in order\n\nCell 2: Imports and initializes\nCell 3: Discovers files and shows preview\nCell 4: Processes all files\nCell 5: Reviews results\n\n\n\n\n\nUse templates/disaster_processing_template.ipynb for full control over: - Memory management - Chunk configurations - Processing parameters - Verification options - Detailed error handling",
    "crumbs": [
      "JupyterHub",
      "Jupyter Notebook Guide for Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/convert_to_geotiff.html#configuration-examples",
    "href": "Jupyterhub/convert_to_geotiff.html#configuration-examples",
    "title": "Jupyter Notebook Guide for Disaster COG Processing",
    "section": "",
    "text": "EVENT_NAME = '202408_TropicalStorm_Debby'\nPRODUCT_NAME = 'landsat8'\nBUCKET = 'nasa-disasters'\nSOURCE_PATH = f'drcs_activations/{EVENT_NAME}/{PRODUCT_NAME}'\nDESTINATION_BASE = 'drcs_activations_new'\nOVERWRITE = False  # Set True to replace existing files\n\n\n\nDefine how your files are renamed:\ndef create_truecolor_filename(original_path, event_name):\n    \"\"\"Create filename for trueColor products.\"\"\"\n    filename = os.path.basename(original_path)\n    stem = os.path.splitext(filename)[0]\n    date = extract_date_from_filename(stem)\n\n    if date:\n        stem_clean = re.sub(r'_\\d{8}', '', stem)\n        return f\"{event_name}_{stem_clean}_{date}_day.tif\"\n    return f\"{event_name}_{stem}_day.tif\"\n\n\n\nFILENAME_CREATORS = {\n    'trueColor': create_truecolor_filename,\n    'colorInfrared': create_colorinfrared_filename,\n    'naturalColor': create_naturalcolor_filename,\n}",
    "crumbs": [
      "JupyterHub",
      "Jupyter Notebook Guide for Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/convert_to_geotiff.html#file-organization",
    "href": "Jupyterhub/convert_to_geotiff.html#file-organization",
    "title": "Jupyter Notebook Guide for Disaster COG Processing",
    "section": "",
    "text": "The system automatically: - Discovers files in your S3 source path - Categorizes them by product type (trueColor, NDVI, etc.) - Applies the appropriate filename function - Saves to organized output directories\n\n\ndrcs_activations_new/\n├── imagery/\n│   ├── trueColor/\n│   ├── colorIR/\n│   └── naturalColor/\n├── indices/\n│   ├── NDVI/\n│   └── MNDWI/\n└── SAR/\n    └── processed/",
    "crumbs": [
      "JupyterHub",
      "Jupyter Notebook Guide for Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/convert_to_geotiff.html#common-patterns",
    "href": "Jupyterhub/convert_to_geotiff.html#common-patterns",
    "title": "Jupyter Notebook Guide for Disaster COG Processing",
    "section": "",
    "text": "The system automatically detects and processes different product types:\n# Files are auto-categorized by these patterns:\n- 'trueColor' → imagery/trueColor/\n- 'colorInfrared' → imagery/colorIR/\n- 'NDVI' → indices/NDVI/\n- 'MNDWI' → indices/MNDWI/\n- 'SAR' → SAR/processed/\n\n\n\nNODATA_VALUES = {\n    'NDVI': -9999,      # Specific value for NDVI\n    'MNDWI': -9999,     # Specific value for MNDWI\n    'trueColor': None,  # Auto-detect for imagery\n}\n\n\n\nOUTPUT_DIRS = {\n    'trueColor': 'Landsat/trueColor',\n    'colorInfrared': 'Landsat/colorIR',\n    'naturalColor': 'Landsat/naturalColor',\n}",
    "crumbs": [
      "JupyterHub",
      "Jupyter Notebook Guide for Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/convert_to_geotiff.html#troubleshooting",
    "href": "Jupyterhub/convert_to_geotiff.html#troubleshooting",
    "title": "Jupyter Notebook Guide for Disaster COG Processing",
    "section": "",
    "text": "Check SOURCE_PATH is correct\nVerify files exist: aws s3 ls s3://bucket/path/\n\n\n\n\n\nCheck AWS credentials: aws configure list\nEnsure bucket access permissions\n\n\n\n\n\nFiles already exist in destination\nSet OVERWRITE = True to reprocess\n\n\n\n\n\nModify filename creator functions\nRe-run from discovery step to preview\n\n\n\n\n\nLarge files take time (normal)\nSystem automatically uses GDAL optimization\nFiles &gt;1.5GB use optimized chunking",
    "crumbs": [
      "JupyterHub",
      "Jupyter Notebook Guide for Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/convert_to_geotiff.html#performance-tips",
    "href": "Jupyterhub/convert_to_geotiff.html#performance-tips",
    "title": "Jupyter Notebook Guide for Disaster COG Processing",
    "section": "",
    "text": "File Size Optimization\n\nFiles &lt;1.5GB: Processed whole (fastest)\nFiles &gt;1.5GB: Smart chunking\nFiles &gt;7GB: Ultra-large file handling\n\nCompression\n\nUses ZSTD level 22 (maximum compression)\nAutomatic predictor selection\nIntelligent resampling based on data type\n\nParallel Processing\n\nFor batch processing multiple events, use:\n\nfrom batch_processor_parallel import process_files_parallel",
    "crumbs": [
      "JupyterHub",
      "Jupyter Notebook Guide for Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/convert_to_geotiff.html#advanced-features",
    "href": "Jupyterhub/convert_to_geotiff.html#advanced-features",
    "title": "Jupyter Notebook Guide for Disaster COG Processing",
    "section": "",
    "text": "from notebooks.notebook_helpers import quick_process\n\nresults = quick_process({\n    'event_name': '202408_TropicalStorm_Debby',\n    'bucket': 'nasa-disasters',\n    'source_path': 'drcs_activations/202408_TropicalStorm_Debby/landsat8',\n    'destination_base': 'drcs_activations_new',\n    'overwrite': False,\n    'filename_creators': FILENAME_CREATORS\n})\n\n\n\nevents = [\n    '202408_TropicalStorm_Debby',\n    '202409_Hurricane_Example',\n    '202410_Wildfire_Sample'\n]\n\nfor event in events:\n    config['event_name'] = event\n    config['source_path'] = f'drcs_activations/{event}/landsat8'\n    processor = SimpleProcessor(config)\n    processor.connect_to_s3()\n    processor.discover_files()\n    processor.process_all()",
    "crumbs": [
      "JupyterHub",
      "Jupyter Notebook Guide for Disaster COG Processing"
    ]
  },
  {
    "objectID": "Jupyterhub/convert_to_geotiff.html#next-steps",
    "href": "Jupyterhub/convert_to_geotiff.html#next-steps",
    "title": "Jupyter Notebook Guide for Disaster COG Processing",
    "section": "",
    "text": "Start with the simple template\nRun a small test batch\nVerify output filenames are correct\nProcess full dataset\nCheck results in S3\n\nFor more details, see the main README.md or review the RESAMPLING_GUIDE.md for data type handling.",
    "crumbs": [
      "JupyterHub",
      "Jupyter Notebook Guide for Disaster COG Processing"
    ]
  },
  {
    "objectID": "data_workflow/eccodarwin-co2flux-monthgrid-v5_Data_Flow.html",
    "href": "data_workflow/eccodarwin-co2flux-monthgrid-v5_Data_Flow.html",
    "title": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5",
    "section": "",
    "text": "Air-Sea CO₂ Flux, ECCO-Darwin Model v5\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Flow Diagrams",
      "Air-Sea CO₂ Flux, ECCO-Darwin Model v5"
    ]
  },
  {
    "objectID": "data_workflow2/test.html",
    "href": "data_workflow2/test.html",
    "title": "NASA Disasters Documentation",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_workflow2/NRT_directory_structure.html",
    "href": "data_workflow2/NRT_directory_structure.html",
    "title": "NRT Directory Structure",
    "section": "",
    "text": "NRT Directory Structure\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Diagrams",
      "NRT Directory Structure"
    ]
  },
  {
    "objectID": "data_workflow2/NRT_data_download.html",
    "href": "data_workflow2/NRT_data_download.html",
    "title": "NRT Data Download",
    "section": "",
    "text": "NRT Data Download\n\n\n\nData Flow Diagram Extending From Acquisition/Creation to User Delivery\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Diagrams",
      "NRT Data Download"
    ]
  },
  {
    "objectID": "workflow2.html",
    "href": "workflow2.html",
    "title": "NASA Disasters: Data Flow Diagrams",
    "section": "",
    "text": "Welcome to the homepage for NASA Disasters data flow diagrams. These diagrams summarize the process of how to find, download, and process data for NASA Disasters.\nClick on a dataset name to view the data flow diagram for that dataset.\nView the NASA Disasters Resources\n\nNRT Data Download\nNRT Directory Structure\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Workflow Diagrams"
    ]
  },
  {
    "objectID": "jupyterhub.html",
    "href": "jupyterhub.html",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "JupyterHub is a multi-user server that manages and provides web-based Jupyter notebook environments for multiple users. It allows you to:\n\nAccess powerful computing resources through your web browser\nWrite and execute code in Python, R, Julia, and other languages\nVisualize data with interactive plots and charts\nCollaborate with team members on shared projects\nWork from anywhere without local setup requirements\n\n\n\n\nThe Disasters Hub is a specialized JupyterHub instance designed for disaster response and analysis work. It provides:\n\nPre-configured environments for geospatial analysis\nAccess to disaster-related datasets\nCollaboration tools for response teams\nIntegration with cloud storage services\nScalable computing resources\n\n\n\n\n✅ No Installation Required - Everything runs in your browser\n✅ Pre-configured Environments - Common packages already installed\n✅ Persistent Storage - Your work is saved between sessions\n✅ Collaboration Ready - Share notebooks with team members\n✅ Scalable Resources - Access to GPU and high-memory instances when needed",
    "crumbs": [
      "JupyterHub"
    ]
  },
  {
    "objectID": "jupyterhub.html#introduction",
    "href": "jupyterhub.html#introduction",
    "title": "JupyterHub Training Guide - Disasters Hub",
    "section": "",
    "text": "JupyterHub is a multi-user server that manages and provides web-based Jupyter notebook environments for multiple users. It allows you to:\n\nAccess powerful computing resources through your web browser\nWrite and execute code in Python, R, Julia, and other languages\nVisualize data with interactive plots and charts\nCollaborate with team members on shared projects\nWork from anywhere without local setup requirements\n\n\n\n\nThe Disasters Hub is a specialized JupyterHub instance designed for disaster response and analysis work. It provides:\n\nPre-configured environments for geospatial analysis\nAccess to disaster-related datasets\nCollaboration tools for response teams\nIntegration with cloud storage services\nScalable computing resources\n\n\n\n\n✅ No Installation Required - Everything runs in your browser\n✅ Pre-configured Environments - Common packages already installed\n✅ Persistent Storage - Your work is saved between sessions\n✅ Collaboration Ready - Share notebooks with team members\n✅ Scalable Resources - Access to GPU and high-memory instances when needed",
    "crumbs": [
      "JupyterHub"
    ]
  },
  {
    "objectID": "quarto-guide.html",
    "href": "quarto-guide.html",
    "title": "Quarto Guide for Beginners",
    "section": "",
    "text": "Quarto is an open-source scientific and technical publishing system built on Pandoc. It allows you to create dynamic documents that combine:\n\nNarrative text (written in Markdown)\nCode (Python, R, Julia, Observable JS)\nCode outputs (plots, tables, results)\nEquations, citations, cross-references\n\nThink of Quarto as a powerful tool to create everything from simple documents to complex websites, presentations, and books.\n\n\n\nMultiple outputs from one source - Write once, publish to HTML, PDF, Word, PowerPoint\nLanguage agnostic - Works with Python, R, Julia, and more\nReproducible - Code and narrative in the same document\nProfessional - Publication-quality output",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#what-is-quarto",
    "href": "quarto-guide.html#what-is-quarto",
    "title": "Quarto Guide for Beginners",
    "section": "",
    "text": "Quarto is an open-source scientific and technical publishing system built on Pandoc. It allows you to create dynamic documents that combine:\n\nNarrative text (written in Markdown)\nCode (Python, R, Julia, Observable JS)\nCode outputs (plots, tables, results)\nEquations, citations, cross-references\n\nThink of Quarto as a powerful tool to create everything from simple documents to complex websites, presentations, and books.\n\n\n\nMultiple outputs from one source - Write once, publish to HTML, PDF, Word, PowerPoint\nLanguage agnostic - Works with Python, R, Julia, and more\nReproducible - Code and narrative in the same document\nProfessional - Publication-quality output",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#file-types-and-formats",
    "href": "quarto-guide.html#file-types-and-formats",
    "title": "Quarto Guide for Beginners",
    "section": "File Types and Formats",
    "text": "File Types and Formats\n\nSource Files\n\n.qmd Files (Quarto Markdown)\nThe primary file type for Quarto documents:\n---\ntitle: \"My Document\"\nformat: html\n---\n\n## Introduction\n\nThis is a Quarto document with **markdown** formatting.\n\n::: {#8c88dc2e .cell execution_count=1}\n``` {.python .cell-code}\n# You can include code\nprint(\"Hello from Python!\")\n\nHello from Python!\n\n:::\n\n#### .ipynb Files (Jupyter Notebooks)\n\nQuarto can directly render Jupyter notebooks:\n\n- Keep your existing notebook workflow\n- Add Quarto features through cell metadata\n- Render notebooks to any format\n\n#### .md Files (Plain Markdown)\n\nStandard Markdown files can be rendered by Quarto, though they lack code execution capabilities.\n\n### Output Formats\n\nQuarto can generate:\n\n| Format | Extension | Use Case |\n|--------|-----------|----------|\n| HTML | .html | Websites, interactive documents |\n| PDF | .pdf | Print publications, reports |\n| Word | .docx | Microsoft Word documents |\n| PowerPoint | .pptx | Presentations |\n| Reveal.js | .html | Interactive HTML presentations |\n| EPUB | .epub | E-books |\n| Markdown | .md | GitHub, other platforms |\n\n## Creating Different Types of Documents\n\n### Basic Document\n\nSimplest Quarto document:\n\n```yaml\n---\ntitle: \"My First Document\"\nauthor: \"Your Name\"\ndate: today\nformat: html\n---\n\n## Section 1\n\nContent goes here.\n\n\n\nMulti-Format Document\nOutput to multiple formats:\n---\ntitle: \"Multi-Format Document\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n  pdf:\n    documentclass: article\n  docx:\n    reference-doc: template.docx\n---\n\n\nPresentation (PowerPoint)\n---\ntitle: \"My Presentation\"\nformat:\n  pptx:\n    slide-level: 2\n---\n\n# Section Title\n\n## Slide 1\n\n- First point\n- Second point\n\n## Slide 2\n\nMore content here.\nImportant: Level 1 headers (#) create section dividers, Level 2 headers (##) create new slides.\n\n\nPresentation (Reveal.js)\n---\ntitle: \"Interactive Presentation\"\nformat:\n  revealjs:\n    theme: dark\n    transition: slide\n---\n\n\nWebsite/Book\nRequires a _quarto.yml configuration file (covered below).",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#yaml-front-matter",
    "href": "quarto-guide.html#yaml-front-matter",
    "title": "Quarto Guide for Beginners",
    "section": "YAML Front Matter",
    "text": "YAML Front Matter\nEvery .qmd file starts with YAML front matter between --- markers. This controls document behavior.\n\nEssential YAML Options\n---\ntitle: \"Document Title\"           # Required\nsubtitle: \"Optional subtitle\"     # Optional\nauthor: \"Author Name\"             # Optional\ndate: today                       # or \"2024-01-15\"\nformat: html                      # Output format\n---\n\n\nFormat-Specific Options\n\nHTML Options\nformat:\n  html:\n    toc: true                    # Table of contents\n    toc-depth: 3                 # How many header levels\n    code-fold: false             # Collapsible code\n    code-tools: true             # Code display options\n    theme: cosmo                 # Visual theme\n    css: styles.css              # Custom CSS\n\n\nPDF Options\nformat:\n  pdf:\n    documentclass: article\n    geometry:\n      - margin=1in\n    toc: true\n    number-sections: true\n\n\nPowerPoint Options\nformat:\n  pptx:\n    reference-doc: template.pptx  # Custom template\n    slide-level: 2                # Which heading creates slides\n    incremental: true             # Bullets appear one by one",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#the-_quarto.yml-configuration-file",
    "href": "quarto-guide.html#the-_quarto.yml-configuration-file",
    "title": "Quarto Guide for Beginners",
    "section": "The _quarto.yml Configuration File",
    "text": "The _quarto.yml Configuration File\nThe _quarto.yml file controls project-level settings for websites and books.\n\nBasic Website Configuration\nproject:\n  type: website\n\nwebsite:\n  title: \"My Website\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - href: about.qmd\n        text: About\n\n\nSidebar Navigation\nFor documentation sites with a sidebar:\nwebsite:\n  sidebar:\n    style: \"docked\"\n    search: true\n    contents:\n      - href: index.qmd\n        text: Welcome\n      - section: \"Getting Started\"\n        contents:\n          - guide1.qmd\n          - guide2.qmd\n      - section: \"Advanced\"\n        contents:\n          - advanced1.qmd\n\n\nAdding Files to Navigation\n\nAdding a Single Document\nwebsite:\n  sidebar:\n    contents:\n      - href: new-document.qmd\n        text: \"Display Name\"\n\n\nAdding a Section with Multiple Documents\nwebsite:\n  sidebar:\n    contents:\n      - section: \"Section Name\"\n        contents:\n          - doc1.qmd\n          - doc2.qmd\n          - text: \"Custom Name\"\n            href: doc3.qmd\n\n\nAdding External Links\nwebsite:\n  sidebar:\n    tools:\n      - icon: github\n        href: https://github.com/your/repo\n        text: \"GitHub\"\n\n\n\nGlobal Format Settings\nApply settings to all documents:\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: true\n    code-copy: true",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#working-with-code",
    "href": "quarto-guide.html#working-with-code",
    "title": "Quarto Guide for Beginners",
    "section": "Working with Code",
    "text": "Working with Code\n\nCode Blocks\n```python\n# Python code\nimport pandas as pd\ndata = pd.read_csv('file.csv')\nprint(data.head())\n```\n\n\nExecutable vs. Non-Executable Code\nBy default, code blocks in .qmd files are executable. To display code without running:\n\n::: {#f6819db2 .cell execution_count=2}\n``` {.python .cell-code}\n# This code is shown but not executed\n```\n:::\n\n\n\nInline Code\nExecute code inline: 4\nOutput: 4\n\n\nCode Options\n#| echo: false        # Hide code, show output\n#| eval: false        # Show code, don't run\n#| warning: false     # Hide warnings\n#| message: false     # Hide messages\n#| label: fig-plot    # For cross-references\n#| fig-cap: \"Caption\" # Figure caption",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#markdown-essentials",
    "href": "quarto-guide.html#markdown-essentials",
    "title": "Quarto Guide for Beginners",
    "section": "Markdown Essentials",
    "text": "Markdown Essentials\n\nHeaders\n# Level 1\n## Level 2\n### Level 3\n\n\nText Formatting\n**bold**\n*italic*\n***bold italic***\n`code`\n~~strikethrough~~\n\n\nLists\n- Unordered list\n- Second item\n  - Nested item\n\n1. Ordered list\n2. Second item\n   1. Nested item\n\n\nLinks and Images\n[Link text](https://url.com)\n\n![Image caption](path/to/image.png)\n\n![Figure with options](image.png){#fig-id width=50%}\n\n\nTables\n| Column 1 | Column 2 | Column 3 |\n|----------|----------|----------|\n| Data 1   | Data 2   | Data 3   |\n| Data 4   | Data 5   | Data 6   |\n\n\nCross-References\nSee @fig-plot for details.\n\n![My plot](plot.png){#fig-plot}\n\nSee @tbl-results for the data.\n\n| Col 1 | Col 2 |\n|-------|-------|\n| A     | B     |\n\n: Results {#tbl-results}",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#rendering-documents",
    "href": "quarto-guide.html#rendering-documents",
    "title": "Quarto Guide for Beginners",
    "section": "Rendering Documents",
    "text": "Rendering Documents\n\nCommand Line\n# Render a single document\nquarto render document.qmd\n\n# Render to specific format\nquarto render document.qmd --to pdf\n\n# Render entire project\nquarto render\n\n# Preview with live reload\nquarto preview\n\n# Publish\nquarto publish gh-pages\n\n\nFrom RStudio or VS Code\n\nRStudio: Click the “Render” button\nVS Code: Click “Preview” or use the Quarto extension\n\n\n\nRender Options\nControl rendering in YAML:\nexecute:\n  echo: true      # Show code\n  warning: false  # Hide warnings\n  error: true     # Show errors\n  cache: true     # Cache results",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#special-features",
    "href": "quarto-guide.html#special-features",
    "title": "Quarto Guide for Beginners",
    "section": "Special Features",
    "text": "Special Features\n\nCallout Blocks\n::: {.callout-note}\nThis is a note callout.\n:::\n\n::: {.callout-warning}\nThis is a warning!\n:::\n\n::: {.callout-important}\nThis is important information.\n:::\n\n\nTabsets\n::: {.panel-tabset}\n\n## Tab 1\nContent for tab 1\n\n## Tab 2\nContent for tab 2\n\n:::\n\n\nColumns Layout\n::: {.columns}\n\n::: {.column width=\"50%\"}\nLeft column content\n:::\n\n::: {.column width=\"50%\"}\nRight column content\n:::\n\n:::\n\n\nDiagrams (Mermaid)\n```{mermaid}\ngraph LR\n  A[Start] --&gt; B[Process]\n  B --&gt; C[End]\n```",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#what-quarto-cannot-do",
    "href": "quarto-guide.html#what-quarto-cannot-do",
    "title": "Quarto Guide for Beginners",
    "section": "What Quarto Cannot Do",
    "text": "What Quarto Cannot Do\n\nLimitations\n\nNo Real-Time Collaboration\n\nNot like Google Docs\nUse Git for version control instead\n\nLimited WYSIWYG\n\nMust render to see final output\nNot a visual editor (though RStudio has visual mode)\n\nLaTeX Required for PDF\n\nMust install LaTeX distribution for PDF output\nCan be large download (several GB)\n\nCode Must Be Installed\n\nPython/R/Julia must be installed separately\nNeed required packages installed\n\nNo Dynamic Forms\n\nCannot create user input forms in HTML output\nUse Shiny for interactive applications\n\nProcessing Speed\n\nLarge projects can be slow to render\nUse caching to speed up\n\nNot a Database\n\nCannot query data directly in document\nMust load data first with code\n\nLimited Styling Control\n\nHTML/CSS knowledge needed for custom designs\nPDF styling requires LaTeX knowledge",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#best-practices",
    "href": "quarto-guide.html#best-practices",
    "title": "Quarto Guide for Beginners",
    "section": "Best Practices",
    "text": "Best Practices\n\nProject Organization\nmy-project/\n├── _quarto.yml           # Project config\n├── index.qmd             # Homepage\n├── guide1.qmd            # Content files\n├── guide2.qmd\n├── images/               # Images folder\n│   ├── logo.png\n│   └── diagram.svg\n├── data/                 # Data folder\n│   └── dataset.csv\n├── scripts/              # Helper scripts\n│   └── utils.py\n└── _site/               # Generated output (don't edit)\n\n\nFile Naming\n\nUse lowercase, hyphens for spaces: my-document.qmd\nBe descriptive: serverless-monitoring.qmd not doc1.qmd\nGroup related files: tutorial-1.qmd, tutorial-2.qmd\n\n\n\nYAML Best Practices\n---\n# Put most important info first\ntitle: \"Clear, Descriptive Title\"\nsubtitle: \"Helpful subtitle\"\nauthor: \"Author Name\"\ndate: today\n\n# Group related options\nformat:\n  html:\n    toc: true\n    theme: cosmo\n\n# Use comments\nexecute:\n  echo: true  # Show code by default\n---\n\n\nCode Best Practices\n\nUse meaningful labels: {#fig-scatter} not {#fig-1}\nSet global options in YAML instead of repeating\nCache expensive computations: cache: true\nOrganize code into separate scripts if complex\nTest frequently - render often to catch errors early",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#common-issues-and-solutions",
    "href": "quarto-guide.html#common-issues-and-solutions",
    "title": "Quarto Guide for Beginners",
    "section": "Common Issues and Solutions",
    "text": "Common Issues and Solutions\n\nIssue: “File not found”\nProblem: Referenced image or data file not found\nSolution: Use relative paths from the .qmd file location\n# Good\n![Logo](images/logo.png)\n\n# Bad (absolute paths break portability)\n![Logo](/Users/name/project/images/logo.png)\n\n\nIssue: Code not executing\nProblem: Code block shown but not running\nSolution: Check code block syntax\n# Correct (with braces)\n\n::: {#65165942 .cell execution_count=3}\n``` {.python .cell-code}\nprint(\"Hello\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHello\n```\n:::\n:::\n\n\n# Wrong (no braces - just displays)\n```python\nprint(\"Hello\")\n```\n\n\nIssue: Links not working in rendered site\nProblem: Cross-references broken after rendering\nSolution: Use .qmd extension in links - Quarto converts automatically\n# Correct\n[See guide](guide.qmd)\n\n# Incorrect\n[See guide](guide.html)\n\n\nIssue: Changes not showing\nProblem: Made changes but don’t see them in preview\nSolution: - Refresh browser (hard refresh: Cmd+Shift+R or Ctrl+Shift+R) - Stop and restart quarto preview - Check if file is saved\n\n\nIssue: Table of contents not showing\nProblem: TOC enabled but not visible\nSolution: Need at least 2 headers for TOC to appear\n---\nformat:\n  html:\n    toc: true\n---\n\n## First Section\nContent\n\n## Second Section\nMore content",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#quick-reference",
    "href": "quarto-guide.html#quick-reference",
    "title": "Quarto Guide for Beginners",
    "section": "Quick Reference",
    "text": "Quick Reference\n\nCommon Quarto Commands\nquarto render               # Render project\nquarto render file.qmd      # Render single file\nquarto preview              # Live preview\nquarto create project       # New project wizard\nquarto check                # Check installation\nquarto --help               # Help\n\n\nCommon YAML Settings\ntitle: \"Title\"\nauthor: \"Name\"\ndate: today\nformat: html\ntoc: true\nnumber-sections: true\ncode-fold: true\ntheme: cosmo\n\n\nFormat Options Quick Reference\n# Multiple formats\nformat:\n  html: default\n  pdf: default\n  docx: default\n\n# Or with options\nformat:\n  html:\n    theme: cosmo\n  pdf:\n    documentclass: article",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#next-steps",
    "href": "quarto-guide.html#next-steps",
    "title": "Quarto Guide for Beginners",
    "section": "Next Steps",
    "text": "Next Steps\n\nLearning Resources\n\nOfficial Docs: https://quarto.org/docs/guide/\nGallery: https://quarto.org/docs/gallery/\nExamples: Browse _site/ in this project for rendered examples\n\n\n\nPractice Exercises\n\nCreate a simple .qmd document with text and headers\nAdd a code block that creates a simple plot\nRender to HTML, PDF, and Word\nCreate a presentation with 5 slides\nAdd your document to the website navigation\n\n\n\nGetting Help\n\nCheck the Quarto documentation\nSearch GitHub Issues\nAsk on Quarto Discussions",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "quarto-guide.html#summary",
    "href": "quarto-guide.html#summary",
    "title": "Quarto Guide for Beginners",
    "section": "Summary",
    "text": "Summary\nQuarto is powerful for creating reproducible, multi-format documents. Key takeaways:\n✅ Files: Create .qmd files with YAML + Markdown + Code ✅ Formats: Output to HTML, PDF, Word, PowerPoint, and more ✅ Configuration: Use _quarto.yml for project-level settings ✅ Navigation: Add files to sidebar in _quarto.yml ✅ Rendering: Use quarto render or IDE buttons ✅ Limitations: Know what Quarto can’t do (real-time collab, dynamic forms) ✅ Best Practices: Organize files, use clear names, test often\nReady to create? Start with a simple document and experiment!",
    "crumbs": [
      "Documentation",
      "Quarto Guide for Beginners"
    ]
  },
  {
    "objectID": "Presentations/aws-grafana-tutorial.html",
    "href": "Presentations/aws-grafana-tutorial.html",
    "title": "AWS and Grafana Monitoring Tutorial",
    "section": "",
    "text": "This page provides comprehensive instructions and guidance for integrating Amazon Web Services (AWS) with Grafana for monitoring and observability.\n\n\n\n\n\n\nNoteAbout This Tutorial\n\n\n\nThis presentation covers AWS and Grafana monitoring setup, configuration, and best practices. The presentation is hosted on Google Slides and embedded here for easy access.\nControls: - Use arrow keys or click arrows to navigate slides - Click the “︙” menu for additional options - Press Esc to exit fullscreen mode\nVersion: V1.0, Updated Oct. 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipAlternative Viewing Options\n\n\n\n\n\nIf the embedded presentation doesn’t load properly, you can:\n\nOpen in new tab: View presentation in Google Slides →\nFullscreen mode: Click the fullscreen button in the embedded player above\nDownload: Open the link above and look for download options if available",
    "crumbs": [
      "Presentations",
      "PowerPoints",
      "AWS and Grafana Monitoring Tutorial"
    ]
  },
  {
    "objectID": "Presentations/aws-grafana-tutorial.html#aws-and-grafana-monitoring-tutorial",
    "href": "Presentations/aws-grafana-tutorial.html#aws-and-grafana-monitoring-tutorial",
    "title": "AWS and Grafana Monitoring Tutorial",
    "section": "",
    "text": "This page provides comprehensive instructions and guidance for integrating Amazon Web Services (AWS) with Grafana for monitoring and observability.\n\n\n\n\n\n\nNoteAbout This Tutorial\n\n\n\nThis presentation covers AWS and Grafana monitoring setup, configuration, and best practices. The presentation is hosted on Google Slides and embedded here for easy access.\nControls: - Use arrow keys or click arrows to navigate slides - Click the “︙” menu for additional options - Press Esc to exit fullscreen mode\nVersion: V1.0, Updated Oct. 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipAlternative Viewing Options\n\n\n\n\n\nIf the embedded presentation doesn’t load properly, you can:\n\nOpen in new tab: View presentation in Google Slides →\nFullscreen mode: Click the fullscreen button in the embedded player above\nDownload: Open the link above and look for download options if available",
    "crumbs": [
      "Presentations",
      "PowerPoints",
      "AWS and Grafana Monitoring Tutorial"
    ]
  },
  {
    "objectID": "Presentations/aws-grafana-tutorial.html#what-youll-learn",
    "href": "Presentations/aws-grafana-tutorial.html#what-youll-learn",
    "title": "AWS and Grafana Monitoring Tutorial",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\nThis tutorial covers the following topics:\n\n🔧 Setup and Configuration\n\nAWS account setup and IAM configuration\nGrafana installation and initial setup\nCloudWatch data source configuration\nAuthentication and security best practices\n\n\n\n📊 Monitoring and Dashboards\n\nCreating custom Grafana dashboards\nCloudWatch metrics integration\nLog aggregation and analysis\nSetting up alerts and notifications\n\n\n\n🚀 Advanced Topics\n\nMulti-account AWS monitoring\nPerformance optimization\nCost management and optimization\nTroubleshooting common issues\n\n\n\n💡 Best Practices\n\nSecurity and compliance\nDashboard design principles\nScalability considerations\nOperational excellence",
    "crumbs": [
      "Presentations",
      "PowerPoints",
      "AWS and Grafana Monitoring Tutorial"
    ]
  },
  {
    "objectID": "Presentations/aws-grafana-tutorial.html#prerequisites",
    "href": "Presentations/aws-grafana-tutorial.html#prerequisites",
    "title": "AWS and Grafana Monitoring Tutorial",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting this tutorial, ensure you have:\n\nAWS Account with appropriate IAM permissions\nBasic understanding of AWS services (EC2, CloudWatch, IAM)\nGrafana installed or access to a Grafana instance\nNetwork access to AWS APIs and Grafana",
    "crumbs": [
      "Presentations",
      "PowerPoints",
      "AWS and Grafana Monitoring Tutorial"
    ]
  },
  {
    "objectID": "Presentations/aws-grafana-tutorial.html#technical-details",
    "href": "Presentations/aws-grafana-tutorial.html#technical-details",
    "title": "AWS and Grafana Monitoring Tutorial",
    "section": "Technical Details",
    "text": "Technical Details\n\nEmbedding Method\nThis presentation is embedded using Google Slides’ publish-to-web feature with auto-advance enabled:\n&lt;iframe src=\"https://docs.google.com/presentation/d/e/[ID]/pubembed?start=true&loop=false&delayms=3000\"\n        width=\"1280\"\n        height=\"749\"\n        allowfullscreen=\"true\"&gt;\n&lt;/iframe&gt;\n\n\nURL Parameters\n\n\n\nParameter\nValue\nDescription\n\n\n\n\nstart\ntrue\nAuto-start presentation when loaded\n\n\nloop\nfalse\nDon’t loop slides continuously\n\n\ndelayms\n3000\n3-second delay between auto-advanced slides\n\n\n\n\n\nEmbed Features\nThis embed uses: - Published URL (/pubembed) - Publicly accessible without authentication - Auto-advance - Presentation starts automatically - Large format - 1280x749 for better readability - Fullscreen support - Expandable for detailed viewing",
    "crumbs": [
      "Presentations",
      "PowerPoints",
      "AWS and Grafana Monitoring Tutorial"
    ]
  },
  {
    "objectID": "Presentations/aws-grafana-tutorial.html#related-resources",
    "href": "Presentations/aws-grafana-tutorial.html#related-resources",
    "title": "AWS and Grafana Monitoring Tutorial",
    "section": "Related Resources",
    "text": "Related Resources\n\nAWS Documentation\n\nAWS CloudWatch Documentation\nAWS IAM Best Practices\nAWS Well-Architected Framework\n\n\n\nGrafana Resources\n\nGrafana CloudWatch Data Source\nGrafana Dashboard Best Practices\nGrafana Alerting Guide\n\n\n\nAdditional Guides\n\nQuarto Guide for Beginners - Learn how to create and embed presentations\nNASA Disasters Logo and Template - PowerPoint template guidance",
    "crumbs": [
      "Presentations",
      "PowerPoints",
      "AWS and Grafana Monitoring Tutorial"
    ]
  },
  {
    "objectID": "Presentations/aws-grafana-tutorial.html#getting-started-checklist",
    "href": "Presentations/aws-grafana-tutorial.html#getting-started-checklist",
    "title": "AWS and Grafana Monitoring Tutorial",
    "section": "Getting Started Checklist",
    "text": "Getting Started Checklist\n\n\n\n\n\n\nImportantBefore You Begin\n\n\n\nComplete these steps to get the most out of this tutorial:\n\nHave AWS account credentials ready\nInstall AWS CLI (optional but recommended)\nSet up Grafana instance or have access to one\nReview IAM permissions requirements\nPrepare note-taking materials\nBookmark this page for future reference",
    "crumbs": [
      "Presentations",
      "PowerPoints",
      "AWS and Grafana Monitoring Tutorial"
    ]
  },
  {
    "objectID": "Presentations/aws-grafana-tutorial.html#support-and-questions",
    "href": "Presentations/aws-grafana-tutorial.html#support-and-questions",
    "title": "AWS and Grafana Monitoring Tutorial",
    "section": "Support and Questions",
    "text": "Support and Questions\n\nNeed Help?\nIf you encounter issues or have questions:\n\nReview the presentation - Most common questions are addressed in the slides\nCheck AWS documentation - Official docs have detailed troubleshooting\nGrafana community - Active community forums for Grafana-specific questions\nNASA Disasters team - Contact the infrastructure team for project-specific help\n\n\n\nFeedback\nWe welcome feedback on this tutorial! If you have suggestions for improvements or find any issues, please reach out to the infrastructure team.",
    "crumbs": [
      "Presentations",
      "PowerPoints",
      "AWS and Grafana Monitoring Tutorial"
    ]
  },
  {
    "objectID": "Presentations/aws-grafana-tutorial.html#conclusion",
    "href": "Presentations/aws-grafana-tutorial.html#conclusion",
    "title": "AWS and Grafana Monitoring Tutorial",
    "section": "Conclusion",
    "text": "Conclusion\nThis embedded tutorial provides comprehensive guidance for setting up and using AWS with Grafana for monitoring and observability. The combination of AWS’s robust cloud infrastructure and Grafana’s powerful visualization capabilities creates a strong foundation for modern monitoring solutions.\nKey Takeaways: - AWS and Grafana integration enables powerful monitoring - Proper configuration is essential for security and performance - Following best practices ensures scalable, maintainable solutions - Continuous learning and improvement drive operational excellence\nGood luck with your AWS and Grafana monitoring journey!",
    "crumbs": [
      "Presentations",
      "PowerPoints",
      "AWS and Grafana Monitoring Tutorial"
    ]
  },
  {
    "objectID": "Presentations/disasters-powerpoint-template.html",
    "href": "Presentations/disasters-powerpoint-template.html",
    "title": "NASA Disasters PowerPoint Template",
    "section": "",
    "text": "This page demonstrates embedding an external PowerPoint presentation hosted on Google Drive.\n\n\n\n\n\n\nNoteAbout This Presentation\n\n\n\nThis presentation is hosted on Google Drive and embedded here as a proof of concept. The presentation will load directly in the page below.\nControls: - Use arrow keys or click arrows to navigate slides - Click the “︙” menu for additional options\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipAlternative Viewing Options\n\n\n\n\n\nIf the embedded presentation doesn’t load properly, you can:\n\nOpen in new tab: View presentation in Google Slides →\nDownload: Click the link above, then go to File → Download → Microsoft PowerPoint (.pptx)\nPresent mode: Click the link above and press the “Present” button in Google Slides for full-screen mode",
    "crumbs": [
      "Presentations",
      "Disasters Logo and Template Overview"
    ]
  },
  {
    "objectID": "Presentations/disasters-powerpoint-template.html#embedded-aws-presentation",
    "href": "Presentations/disasters-powerpoint-template.html#embedded-aws-presentation",
    "title": "NASA Disasters PowerPoint Template",
    "section": "",
    "text": "This page demonstrates embedding an external PowerPoint presentation hosted on Google Drive.\n\n\n\n\n\n\nNoteAbout This Presentation\n\n\n\nThis presentation is hosted on Google Drive and embedded here as a proof of concept. The presentation will load directly in the page below.\nControls: - Use arrow keys or click arrows to navigate slides - Click the “︙” menu for additional options\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipAlternative Viewing Options\n\n\n\n\n\nIf the embedded presentation doesn’t load properly, you can:\n\nOpen in new tab: View presentation in Google Slides →\nDownload: Click the link above, then go to File → Download → Microsoft PowerPoint (.pptx)\nPresent mode: Click the link above and press the “Present” button in Google Slides for full-screen mode",
    "crumbs": [
      "Presentations",
      "Disasters Logo and Template Overview"
    ]
  },
  {
    "objectID": "Presentations/disasters-powerpoint-template.html#technical-details",
    "href": "Presentations/disasters-powerpoint-template.html#technical-details",
    "title": "NASA Disasters PowerPoint Template",
    "section": "Technical Details",
    "text": "Technical Details\n\nEmbedding Method\nThis presentation is embedded using an HTML &lt;iframe&gt; element that points to the Google Slides embed URL:\n&lt;iframe src=\"https://docs.google.com/presentation/d/[ID]/embed?start=false&loop=false&delayms=3000\"\n        width=\"960\"\n        height=\"569\"\n        allowfullscreen=\"true\"&gt;\n&lt;/iframe&gt;\n\n\nURL Parameters\n\n\n\nParameter\nValue\nDescription\n\n\n\n\nstart\nfalse\nDon’t auto-start presentation\n\n\nloop\nfalse\nDon’t loop slides\n\n\ndelayms\n3000\nDelay between slides in auto-play (ms)\n\n\n\n\n\nEmbedding Other Presentations\nTo embed your own Google Slides presentation:\n\nGet the presentation ID from your Google Slides URL:\nhttps://docs.google.com/presentation/d/[THIS-IS-THE-ID]/edit\nCreate the embed URL:\nhttps://docs.google.com/presentation/d/[YOUR-ID]/embed?start=false&loop=false&delayms=3000\nAdd to your .qmd file:\n&lt;iframe src=\"YOUR-EMBED-URL\"\n        width=\"960\"\n        height=\"569\"\n        allowfullscreen=\"true\"&gt;\n&lt;/iframe&gt;\nEnsure sharing is enabled: The presentation must be set to “Anyone with the link can view”\n\n\n\nResponsive Sizing\nFor responsive iframe sizing, you can use CSS:\n.responsive-iframe {\n  position: relative;\n  width: 100%;\n  padding-bottom: 56.25%; /* 16:9 aspect ratio */\n  height: 0;\n}\n\n.responsive-iframe iframe {\n  position: absolute;\n  top: 0;\n  left: 0;\n  width: 100%;\n  height: 100%;\n}\nThen wrap your iframe:\n&lt;div class=\"responsive-iframe\"&gt;\n  &lt;iframe src=\"...\"&gt;&lt;/iframe&gt;\n&lt;/div&gt;",
    "crumbs": [
      "Presentations",
      "Disasters Logo and Template Overview"
    ]
  },
  {
    "objectID": "Presentations/disasters-powerpoint-template.html#limitations-and-considerations",
    "href": "Presentations/disasters-powerpoint-template.html#limitations-and-considerations",
    "title": "NASA Disasters PowerPoint Template",
    "section": "Limitations and Considerations",
    "text": "Limitations and Considerations\n\n\n\n\n\n\nWarningImportant Limitations\n\n\n\n\nRequires internet connection - Embedded presentations load from Google servers\nSharing permissions - Presentation must be publicly accessible or shared with viewers\nGoogle account dependency - If the presentation owner deletes their account or changes permissions, the embed will break\nLimited offline support - Unlike locally hosted files, embedded presentations won’t work offline\nThird-party dependency - Relies on Google’s infrastructure and terms of service\n\n\n\n\nBest Practices\n✅ Do: - Use for external presentations you don’t control - Test embedding before sharing widely - Provide alternative download/view links - Document the source and owner\n❌ Don’t: - Embed sensitive or proprietary information - Rely solely on embedded content for critical documentation - Assume it will work indefinitely without maintenance - Embed presentations without permission",
    "crumbs": [
      "Presentations",
      "Disasters Logo and Template Overview"
    ]
  },
  {
    "objectID": "Presentations/disasters-powerpoint-template.html#alternative-embedding-methods",
    "href": "Presentations/disasters-powerpoint-template.html#alternative-embedding-methods",
    "title": "NASA Disasters PowerPoint Template",
    "section": "Alternative Embedding Methods",
    "text": "Alternative Embedding Methods\n\nOption 1: PDF Embed\nConvert PowerPoint to PDF and embed:\n&lt;iframe src=\"presentation.pdf\"\n        width=\"100%\"\n        height=\"600px\"&gt;\n&lt;/iframe&gt;\n\n\nOption 2: Quarto Native Presentation\nCreate a native Quarto presentation using .qmd files with presentation formats like PowerPoint or Reveal.js.\nAdvantages: - Full version control - Offline support - Customizable with Quarto features - No external dependencies\nSee the Quarto Guide for Beginners for more details on creating presentations.\n\n\nOption 3: Static Images\nExport slides as images and display them:\n![Slide 1](images/slide-1.png)\n\n![Slide 2](images/slide-2.png)",
    "crumbs": [
      "Presentations",
      "Disasters Logo and Template Overview"
    ]
  },
  {
    "objectID": "Presentations/disasters-powerpoint-template.html#conclusion",
    "href": "Presentations/disasters-powerpoint-template.html#conclusion",
    "title": "NASA Disasters PowerPoint Template",
    "section": "Conclusion",
    "text": "Conclusion\nThis page successfully demonstrates embedding a PowerPoint presentation from Google Drive into a Quarto document. This method works well for:\n\nSharing presentations created by others\nQuick embedding without conversion\nMaintaining a single source presentation\n\nFor production use, consider the tradeoffs between embedded content and native Quarto presentations based on your needs for control, offline access, and long-term maintenance.",
    "crumbs": [
      "Presentations",
      "Disasters Logo and Template Overview"
    ]
  },
  {
    "objectID": "Presentations/disasters-training-1.html",
    "href": "Presentations/disasters-training-1.html",
    "title": "AWS and Grafana Monitoring Tutorial",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Presentations",
      "Videos",
      "AWS and Grafana Monitoring Tutorial"
    ]
  }
]